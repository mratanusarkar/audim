{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Audim \u2728 <p> Audio Podcast Animation Engine   An animation and video rendering engine for audio-based and voice-based podcast videos. </p>"},{"location":"#demo","title":"\ud83d\ude80 Demo","text":"Your browser does not support the video element.    <p>A sample podcast video generated with Audim</p> <p>About the Demo</p> <p>For this example, we have transformed a conversation between Grant Sanderson (from 3Blue1Brown) and Sal Khan (from Khan Academy) from YouTube into a visually engaging podcast video using Audim.</p> <p>See docs/devblog/v0.0.7.md for more details on how this video was generated.</p>"},{"location":"#introduction","title":"\ud83c\udfaf Introduction","text":"<p>Audim is an engine for precise programmatic animation and rendering of podcast videos from audio-based and voice-based file recordings.</p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>\ud83d\udcbb Precise programmatic animations.</li> <li>\ud83c\udfac Rendering of videos with layout based scenes.</li> <li>\ud83d\udcdd Generate subtitles and transcripts from audio/video files.</li> <li>\ud83c\udfa4 From subtitle and scene elements to podcast video generation.</li> </ul>"},{"location":"#quick-links","title":"\ud83d\udd17 Quick Links","text":"<ol> <li>Getting Started<ul> <li>See Setup and ensure you have setup correctly before usage.</li> <li>For developers and contributors, see Development.</li> </ul> </li> <li>API Documentation<ul> <li>See API Docs for the <code>audim</code> API documentation.</li> </ul> </li> <li>Usage and Examples<ul> <li>See Usage for usage examples.</li> </ul> </li> <li>Dev Blog<ul> <li>See Dev Blog for the development blog of the project to gain more insights into the project.</li> <li>See Changelog for the changelog of the project.</li> </ul> </li> </ol>"},{"location":"#license-attribution","title":"\u2696\ufe0f License &amp; Attribution","text":"<p>Audim is licensed under Apache 2.0. You can use it freely for personal and commercial projects.</p> <p>Attribution is encouraged. If you use Audim, please:</p> <ul> <li>Keep the default watermark in videos, OR</li> <li>Add \"Made with Audim\" to video descriptions, OR  </li> <li>Link to this repo in your project documentation</li> </ul> <p>Additional Information</p> <ul> <li>See NOTICE file for complete attribution guidelines.</li> <li>See LICENSE file for the license of the project.</li> <li>For additional attribution examples, see Watermark documentation.</li> </ul>"},{"location":"#citation","title":"\ud83d\udcc4 Citation","text":"<p>If you use Audim in your project or research, please cite it as follows:</p> <pre><code>@software{audim,\n  title = {Audim: Audio Podcast Animation Engine},\n  author = {Sarkar, Atanu},\n  year = {2025},\n  url = {https://github.com/mratanusarkar/audim},\n  version = {0.0.7}\n}\n</code></pre> <p>You can also click the \"Cite this repository\" button on GitHub for other citation formats.</p>"},{"location":"#disclaimer","title":"\u26a0\ufe0f Disclaimer","text":"<p>Early Development Stage</p> <ul> <li>This project is actively under development and may contain bugs or limitations.</li> <li>While stable for basic use cases, the rendering engine requires further development and testing across diverse scenarios.</li> <li>The API is subject to change, so keep an eye at the documentation for the latest updates.</li> </ul> <p>We encourage you to:</p> <ul> <li>Try Audim for your projects and podcast videos.</li> <li>Report issues when encountered.  </li> <li>Feel free to raise a PR to contribute and improve the project.</li> </ul> <p>Your feedback and contributions help make Audim better for everyone!</p>"},{"location":"audim/","title":"Audim API Documentation","text":"<p>Audim is an engine for precise programmatic animation and rendering of podcast videos from audio-based and voice-based file recordings.</p>"},{"location":"audim/#modules","title":"Modules","text":"<ul> <li><code>aud2sub</code> - Audio to subtitle generation.</li> <li><code>sub2pod</code> - Subtitle to podcast video generation.</li> <li><code>utils</code> - Utility functions for audio and video processing.</li> <li><code>vid2sub</code> - Video to subtitle extraction.</li> </ul>"},{"location":"audim/#submodules","title":"Submodules","text":""},{"location":"audim/#aud2sub","title":"aud2sub","text":"<ul> <li>core - Core audio-to-subtitle generation pipeline.</li> <li>transcribers<ul> <li>base - Base transcriber interface.</li> <li>podcast - Transcriber implementation using WhisperX model.</li> </ul> </li> </ul>"},{"location":"audim/#sub2pod","title":"sub2pod","text":"<ul> <li>core - Core subtitle-to-podcast video generation and rendering pipeline.</li> <li>elements - video elements<ul> <li>header - Header and title elements.</li> <li>profile - Speaker profile and avatar components.</li> <li>text - Text styling and display components.</li> <li>watermark - Branding and watermark elements.</li> </ul> </li> <li>effects - effects on elements<ul> <li>highlights - Text and visual highlighting effects.</li> <li>transitions - Scene and element transition animations.</li> </ul> </li> <li>layouts - layouts for podcast videos<ul> <li>base - Base layout framework.</li> <li>podcast - Podcast-specific layouts.</li> </ul> </li> </ul>"},{"location":"audim/#utils","title":"utils","text":"<ul> <li>extract - Audio and video extraction utilities.</li> <li>playback - Media playback and control.</li> <li>subtitle - Subtitle parsing, formatting, and manipulation.</li> </ul>"},{"location":"audim/aud2sub/core/","title":"Core (Subtitle Generation Module)","text":"<p>The core module is the main Subtitle Generation Engine. It is responsible for the overall structure and flow of the subtitle generation.</p> <p>This module provides the high-level API for generating subtitles from audio files. It uses a transcriber object to define the format of the subtitle.</p> <p>The transcriber object internally uses an ASR model to transcribe the audio, aligns the transcription segment timestamps with the audio, and a diarization model to detect the speakers. Finally it uses a formatter to determine the format of the subtitle generation.</p> <p>Below is the API documentation for the core module:</p>"},{"location":"audim/aud2sub/core/#audim.aud2sub.core","title":"<code>audim.aud2sub.core</code>","text":""},{"location":"audim/aud2sub/core/#audim.aud2sub.core.SubtitleGenerator","title":"<code>SubtitleGenerator</code>","text":"<p>High-level generator for creating subtitles from audio.</p> <p>This class provides a simple interface for generating subtitles from audio files using a configured transcriber.</p> <p>Parameters:</p> Name Type Description Default <code>transcriber</code> <code>BaseTranscriber</code> <p>Configured transcriber to use for processing audio</p> required"},{"location":"audim/aud2sub/core/#audim.aud2sub.core.SubtitleGenerator.generate_from_mp3","title":"<code>generate_from_mp3(mp3_path)</code>","text":"<p>Generate subtitles from an MP3 file.</p> <p>This method processes the audio file and prepares subtitles for export.</p> <p>Parameters:</p> Name Type Description Default <code>mp3_path</code> <code>str</code> <p>Path to the MP3 file</p> required"},{"location":"audim/aud2sub/core/#audim.aud2sub.core.SubtitleGenerator.generate_from_wav","title":"<code>generate_from_wav(wav_path)</code>","text":"<p>Generate subtitles from a WAV file.</p> <p>This method processes the audio file and prepares subtitles for export.</p> <p>Parameters:</p> Name Type Description Default <code>wav_path</code> <code>str</code> <p>Path to the WAV file</p> required"},{"location":"audim/aud2sub/core/#audim.aud2sub.core.SubtitleGenerator.generate_from_audio","title":"<code>generate_from_audio(audio_path)</code>","text":"<p>Generate subtitles from any supported audio file.</p> <p>This method processes the audio file and prepares subtitles for export.</p> <p>Parameters:</p> Name Type Description Default <code>audio_path</code> <code>str</code> <p>Path to the audio file</p> required"},{"location":"audim/aud2sub/core/#audim.aud2sub.core.SubtitleGenerator.export_subtitle","title":"<code>export_subtitle(output_path)</code>","text":"<p>Export the generated subtitles to a file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to the output subtitle file</p> required"},{"location":"audim/aud2sub/transcribers/base/","title":"Base Transcriber","text":"<p>The base transcriber is the base class for all transcriber classes. It defines the interface for all transcriber classes.</p> <p>It must be overriden to create various transcriber classes with various ASR models, diarization models, alignment models, formatters and their implementations.</p> <p>It determines the subtitle generation pipeline and format of the subtitle generation.</p> <p>Below is the API documentation for the base transcriber:</p>"},{"location":"audim/aud2sub/transcribers/base/#audim.aud2sub.transcribers.base","title":"<code>audim.aud2sub.transcribers.base</code>","text":""},{"location":"audim/aud2sub/transcribers/base/#audim.aud2sub.transcribers.base.BaseTranscriber","title":"<code>BaseTranscriber</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all transcriber implementations.</p> <p>This abstract class defines the interface that all transcriber implementations must follow. Each implementation should handle transcription, diarization, and formatting internally.</p>"},{"location":"audim/aud2sub/transcribers/base/#audim.aud2sub.transcribers.base.BaseTranscriber.process_audio","title":"<code>process_audio(audio_path)</code>  <code>abstractmethod</code>","text":"<p>Process an audio file to generate transcription.</p> <p>Parameters:</p> Name Type Description Default <code>audio_path</code> <code>str</code> <p>Path to the audio file.</p> required"},{"location":"audim/aud2sub/transcribers/base/#audim.aud2sub.transcribers.base.BaseTranscriber.export_subtitle","title":"<code>export_subtitle(output_path)</code>  <code>abstractmethod</code>","text":"<p>Export the processed transcription to a subtitle file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to the output subtitle file.</p> required"},{"location":"audim/aud2sub/transcribers/podcast/","title":"Podcast Transcriber","text":"<p>The podcast transcriber is a transcriber implementation that uses the WhisperX under the hood. WhisperX provides Automatic Speech Recognition with Word-level Timestamps and Diarization.</p> <p>It uses <code>faster-whisper</code> as the ASR and Transcription model, it's own alignment logic and <code>pyannote-audio</code> for diarization.</p> <p>Warning</p> <p>WhisperX uses a local offline ASR model. So, all the models are downloaded and run locally. You must have a good system specification and NVIDIA GPU with 12GB VRAM to run this.</p> <p>In future, we will support to work with online model vendors like <code>OpenAI</code> and <code>HuggingFace</code>.</p> <p>Below is the API documentation for the podcast transcriber:</p>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast","title":"<code>audim.aud2sub.transcribers.podcast</code>","text":""},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber","title":"<code>PodcastTranscriber</code>","text":"<p>               Bases: <code>BaseTranscriber</code></p> <p>Podcast transcriber implementation using WhisperX.</p> <p>This class provides a complete implementation for podcast transcription, using WhisperX for ASR, diarization, and subtitle formatting.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>WhisperX model name (tiny, base, small, medium, large, large-v2, large-v3)</p> <code>'large-v2'</code> <code>language</code> <code>Optional[str]</code> <p>Language code (e.g., 'en', 'hi', 'bn') or None for auto-detection</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to run inference on (cpu, cuda, mps)</p> <code>None</code> <code>compute_type</code> <code>str</code> <p>Compute type (float16, float32, int8)</p> <code>'float16'</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>16</code> <code>min_speakers</code> <code>Optional[int]</code> <p>Minimum number of speakers</p> <code>1</code> <code>max_speakers</code> <code>Optional[int]</code> <p>Maximum number of speakers</p> <code>5</code> <code>hf_token</code> <code>Optional[str]</code> <p>HuggingFace token for accessing diarization models If not provided, will try to use HF_TOKEN environment variable or the token stored by huggingface-cli login</p> <code>None</code> <code>max_line_length</code> <code>int</code> <p>Maximum length of subtitle lines</p> <code>70</code> <code>min_char_length_splitter</code> <code>int</code> <p>Minimum characters before line splitting</p> <code>50</code> <code>show_speaker_names</code> <code>bool</code> <p>Whether to show speaker names in subtitles</p> <code>True</code> <code>speaker_name_pattern</code> <code>str</code> <p>Pattern for formatting speaker names</p> <code>'[{speaker}]'</code> <code>clear_gpu_memory</code> <code>bool</code> <p>Whether to clear GPU memory after completing major processing steps</p> <code>False</code>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.process_audio","title":"<code>process_audio(audio_path)</code>","text":"<p>Process audio file to generate transcription with diarization.</p> <p>Parameters:</p> Name Type Description Default <code>audio_path</code> <code>str</code> <p>Path to the audio file.</p> required"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.export_subtitle","title":"<code>export_subtitle(output_path)</code>","text":"<p>Export the processed transcription to an SRT subtitle file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to the output SRT file.</p> required"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.get_language","title":"<code>get_language()</code>","text":"<p>Get the detected language.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Detected language code</p>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_model","title":"<code>set_model(model_name)</code>","text":"<p>Set the Whisper model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Whisper model name</p> required"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_language","title":"<code>set_language(language)</code>","text":"<p>Set the language code.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>Language code (e.g., 'en', 'hi', 'bn')</p> required"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_device","title":"<code>set_device(device)</code>","text":"<p>Set the device for computation.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>Device to run on (cpu, cuda, mps)</p> required"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_speakers","title":"<code>set_speakers(min_speakers=None, max_speakers=None)</code>","text":"<p>Set the number of speakers for diarization.</p> <p>Parameters:</p> Name Type Description Default <code>min_speakers</code> <code>Optional[int]</code> <p>Minimum number of speakers</p> <code>None</code> <code>max_speakers</code> <code>Optional[int]</code> <p>Maximum number of speakers</p> <code>None</code>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_huggingface_token","title":"<code>set_huggingface_token(hf_token)</code>","text":"<p>Set the HuggingFace token for diarization.</p> <p>Note: It's recommended to use environment variables or the HuggingFace CLI login for better security rather than hardcoding tokens in your code.</p> <p>Parameters:</p> Name Type Description Default <code>hf_token</code> <code>str</code> <p>HuggingFace token</p> required"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_speaker_names_display","title":"<code>set_speaker_names_display(show_speaker_names, pattern=None)</code>","text":"<p>Configure how speaker names are displayed.</p> <p>Parameters:</p> Name Type Description Default <code>show_speaker_names</code> <code>bool</code> <p>Whether to show speaker names</p> required <code>pattern</code> <code>Optional[str]</code> <p>Pattern for formatting speaker names (e.g., \"[{speaker}]\")</p> <code>None</code>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_line_properties","title":"<code>set_line_properties(max_length=70, min_split_length=50)</code>","text":"<p>Configure subtitle line properties.</p> <p>Parameters:</p> Name Type Description Default <code>max_length</code> <code>int</code> <p>Maximum length of subtitle lines</p> <code>70</code> <code>min_split_length</code> <code>int</code> <p>Minimum characters before considering a line split</p> <code>50</code>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_memory_management","title":"<code>set_memory_management(clear_gpu_memory)</code>","text":"<p>Configure memory management.</p> <p>Parameters:</p> Name Type Description Default <code>clear_gpu_memory</code> <code>bool</code> <p>Whether to clear GPU memory after major processing steps</p> required"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.format_timestamp","title":"<code>format_timestamp(seconds)</code>","text":"<p>Convert seconds to SRT timestamp format (HH:MM:SS,mmm)</p>"},{"location":"audim/sub2pod/core/","title":"Core (Video Generation Module)","text":"<p>The core module is the main Video Generation Engine. It is responsible for the overall structure and flow of the podcast video.</p> <p>It uses a layout object to define the visual arrangement of the video, which internally uses a collection of elements and their effects to define the components of each frame in the video and their animations and transitions.</p> <p>Below is the API documentation for the core module:</p>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core","title":"<code>audim.sub2pod.core</code>","text":""},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator","title":"<code>VideoGenerator</code>","text":"<p>Core engine for generating videos from SRT files</p> <p>This class is responsible for generating video frames from an SRT or subtitle file. The subtitle file must follow our extended SRT format, which adds speaker identification:</p> <ul> <li>Standard SRT format with sequential numbering, timestamps, and text content</li> <li>Speaker identification in square brackets at the beginning of each subtitle text   Example: \"[Host] Welcome to our podcast!\"</li> </ul> <p>Example of expected SRT format: <pre><code>1\n00:00:00,000 --&gt; 00:00:04,500\n[Host] Welcome to our podcast!\n\n2\n00:00:04,600 --&gt; 00:00:08,200\n[Guest] Thank you! Glad to be here.\n</code></pre></p> <p>The speaker tag is used to visually distinguish different speakers in the generated video, and is mandatory for the core engine to work.</p> <p>It uses a layout object to define the visual arrangement of the video.</p>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator.__init__","title":"<code>__init__(layout, fps=30, batch_size=300)</code>","text":"<p>Initialize the video generator</p> <p>Parameters:</p> Name Type Description Default <code>layout</code> <p>Layout object that defines the visual arrangement</p> required <code>fps</code> <code>int</code> <p>Frames per second for the output video</p> <code>30</code> <code>batch_size</code> <code>int</code> <p>Number of frames to process in a batch               before writing to disk</p> <code>300</code>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator.generate_from_srt","title":"<code>generate_from_srt(srt_path, audio_path=None, logo_path=None, title=None, cpu_core_utilization='most')</code>","text":"<p>Generate video frames from an SRT file</p> <p>Parameters:</p> Name Type Description Default <code>srt_path</code> <code>str</code> <p>Path to the SRT file</p> required <code>audio_path</code> <code>str</code> <p>Path to the audio file</p> <code>None</code> <code>logo_path</code> <code>str</code> <p>Path to the logo image</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the video</p> <code>None</code> <code>cpu_core_utilization</code> <code>str</code> <p><code>'single'</code>, <code>'half'</code>, <code>'most'</code>, <code>'max'</code></p> <ul> <li><code>single</code>: Uses 1 CPU core</li> <li><code>half</code>: Uses half of available CPU cores</li> <li><code>most</code>: (default) Uses all available CPU cores except one</li> <li><code>max</code>: Uses all available CPU cores for maximum performance</li> </ul> <code>'most'</code>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator.export_video","title":"<code>export_video(output_path, encoder='auto', video_codec=None, audio_codec=None, video_bitrate='8M', audio_bitrate='192k', preset='medium', crf=23, threads=None, gpu_acceleration=True, extra_ffmpeg_args=None)</code>","text":"<p>Export the generated frames as a video</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path for the output video file</p> required <code>encoder</code> <code>str</code> <p>Encoding method to use: <code>'ffmpeg'</code>, <code>'moviepy'</code>, or <code>'auto'</code> (default)</p> <code>'auto'</code> <code>video_codec</code> <code>str</code> <p>Video codec to use (default: <code>'h264_nvenc'</code> for GPU, <code>'libx264'</code> for CPU)</p> <ul> <li>See FFmpeg H.264 Guide   for CPU options</li> <li>See NVIDIA FFmpeg Guide   for GPU options</li> </ul> <code>None</code> <code>audio_codec</code> <code>str</code> <p>Audio codec to use (default: <code>'aac'</code>)</p> <ul> <li>See FFmpeg AAC Guide   for audio codec options</li> </ul> <code>None</code> <code>video_bitrate</code> <code>str</code> <p>Video bitrate (default: <code>'8M'</code>)</p> <code>'8M'</code> <code>audio_bitrate</code> <code>str</code> <p>Audio bitrate (default: <code>'192k'</code>)</p> <code>'192k'</code> <code>preset</code> <code>str</code> <p>Encoding preset (default: <code>'medium'</code>)</p> <p>For CPU encoding (libx264):     Options: <code>'ultrafast'</code>, <code>'superfast'</code>, <code>'veryfast'</code>, <code>'faster'</code>,              <code>'fast'</code>, <code>'medium'</code>, <code>'slow'</code>, <code>'slower'</code>, <code>'veryslow'</code>     Slower presets give better compression/quality at the cost of     encoding time.</p> <ul> <li>See FFmpeg Preset Guide</li> </ul> <p>For GPU encoding (NVENC):     Will be automatically converted to NVENC presets:     <code>'slow'</code>/<code>'slower'</code>/<code>'veryslow'</code> \u2192 <code>'p1'</code> (highest quality)     <code>'medium'</code> \u2192 <code>'p3'</code> (balanced)     <code>'fast'</code>/<code>'faster'</code> \u2192 <code>'p5'</code> (faster encoding)     <code>'veryfast'</code>/<code>'superfast'</code>/<code>'ultrafast'</code> \u2192 <code>'p7'</code> (fastest encoding)</p> <ul> <li>See NVIDIA FFmpeg Integration</li> </ul> <code>'medium'</code> <code>crf</code> <code>int</code> <p>Constant Rate Factor for quality (default: <code>23</code>, lower is better quality)</p> <ul> <li>Range: <code>0-51</code>, where lower values mean better quality and larger   file size</li> <li>Recommended range: <code>18-28</code>.</li> <li>See CRF Guide</li> </ul> <code>23</code> <code>threads</code> <code>int</code> <p>Number of encoding threads (default: CPU count - 1)</p> <code>None</code> <code>gpu_acceleration</code> <code>bool</code> <p>Whether to use GPU acceleration if available (default: <code>True</code>)</p> <code>True</code> <code>extra_ffmpeg_args</code> <code>list</code> <p>Additional FFmpeg arguments as a list</p> <ul> <li>See FFmpeg Documentation for all   available options</li> </ul> <code>None</code>"},{"location":"audim/sub2pod/effects/highlights/","title":"Highlights","text":"<p>Highlight effects allow you to create highlight effects to elements within the video frames. It is useful when you want to create a highlight effect to an element, or a group of elements or sections of area within the frame.</p> <p>As of now, the following highlight effects are available:</p> <ul> <li><code>none</code>: No highlight (default)</li> <li><code>pulse</code>: Pulsing highlight</li> <li><code>glow</code>: Glowing highlight</li> <li><code>box</code>: Box highlight</li> <li><code>underline</code>: Underline highlight</li> </ul> <p>Below is the API documentation for the highlight effects:</p>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights","title":"<code>audim.sub2pod.effects.highlights</code>","text":"<p>Highlight effects for videos</p> <p>This module provides highlight effects that can be applied to text or other elements during video generation. Highlights are used to emphasize important parts of the content.</p>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.BaseHighlight","title":"<code>BaseHighlight</code>","text":"<p>Base class for all highlight effects (internal use only)</p>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.BaseHighlight.__init__","title":"<code>__init__(color=(255, 255, 0, 128), padding=5)</code>","text":"<p>Initialize the highlight effect</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>tuple</code> <p>RGBA color for the highlight (default: semi-transparent yellow)</p> <code>(255, 255, 0, 128)</code> <code>padding</code> <code>int</code> <p>Padding around the highlighted area in pixels</p> <code>5</code>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.BaseHighlight.apply","title":"<code>apply(frame, area, **kwargs)</code>","text":"<p>Apply the highlight effect to a specific area of a frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>The frame to apply the effect to (PIL Image or numpy array)</p> required <code>area</code> <code>tuple</code> <p>The area to highlight as (x1, y1, x2, y2)</p> required <code>**kwargs</code> <p>Additional arguments specific to the highlight</p> <code>{}</code> <p>Returns:</p> Type Description <p>The modified frame with the highlight effect applied</p>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.Highlight","title":"<code>Highlight</code>","text":"<p>Highlight effects for video elements</p> <p>This class provides various highlight effects that can be applied to text or other elements during video generation.</p> <p>Available effects: - \"pulse\": Pulsing highlight that grows and shrinks - \"glow\": Glowing highlight with blur effect - \"underline\": Simple underline highlight - \"box\": Box around the highlighted area - \"none\": No highlight effect</p>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.Highlight.__init__","title":"<code>__init__(effect_type='none', **kwargs)</code>","text":"<p>Initialize a highlight effect</p> <p>Parameters:</p> Name Type Description Default <code>effect_type</code> <code>str</code> <p>Type of highlight effect \"pulse\": Pulsing highlight \"glow\": Glowing highlight \"underline\": Underline highlight \"box\": Box highlight \"none\": No highlight (default)</p> <code>'none'</code> <code>**kwargs</code> <p>Additional parameters for the specific effect: color (tuple): RGBA color for the highlight padding (int): Padding around the highlighted area min_size (float): For pulse, minimum size factor (e.g., 0.8) max_size (float): For pulse, maximum size factor (e.g., 1.2) blur_radius (int): Blur radius for glow effect thickness (int): Line thickness for underline/box</p> <code>{}</code>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.Highlight.apply","title":"<code>apply(frame, area, progress=0.0, **kwargs)</code>","text":"<p>Apply the selected highlight effect to a specific area</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>The frame to apply the effect to (PIL Image or numpy array)</p> required <code>area</code> <code>tuple</code> <p>Area to highlight as (x1, y1, x2, y2)</p> required <code>progress</code> <code>float</code> <p>Animation progress from 0.0 to 1.0</p> <code>0.0</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>The modified frame with the highlight effect applied</p>"},{"location":"audim/sub2pod/effects/transitions/","title":"Transitions","text":"<p>Transition effects allow you to create transition effects to elements in the video frames or the frames themselves. It is useful when you want to create a smooth transition between two different scenes or frames, or individual elements.</p> <p>As of now, the following transition effects are available:</p> <ul> <li><code>none</code>: No transition (default)</li> <li><code>fade</code>: Fade-in transition</li> <li><code>slide</code>: Slide-in transition</li> </ul> <p>Below is the API documentation for the transition effects:</p>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions","title":"<code>audim.sub2pod.effects.transitions</code>","text":"<p>Transition effects for videos</p> <p>This module provides transition effects that can be applied to frames during video generation. Transitions are used for fade-in, fade-out, dissolve and other similar effects between frames.</p>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.BaseTransition","title":"<code>BaseTransition</code>","text":"<p>Base class for all transition effects (internal use only)</p>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.BaseTransition.__init__","title":"<code>__init__(frames=15)</code>","text":"<p>Initialize the transition effect</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>int</code> <p>Number of frames for the transition</p> <code>15</code>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.BaseTransition.apply","title":"<code>apply(frame, progress, **kwargs)</code>","text":"<p>Apply the transition effect to a frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>The frame to apply the effect to (PIL Image or numpy array)</p> required <code>progress</code> <code>float</code> <p>Progress of the transition, from 0.0 to 1.0</p> required <code>**kwargs</code> <p>Additional arguments specific to the transition</p> <code>{}</code> <p>Returns:</p> Type Description <p>The modified frame with the transition effect applied</p>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.Transition","title":"<code>Transition</code>","text":"<p>Transition effects for video frames</p> <p>This class provides various transition effects that can be applied to frames during video generation.</p> <p>Available effects: - \"fade\": Simple fade-in transition - \"slide\": Slide-in from the specified direction - \"none\": No transition effect</p>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.Transition.__init__","title":"<code>__init__(effect_type='none', **kwargs)</code>","text":"<p>Initialize a transition effect</p> <p>Parameters:</p> Name Type Description Default <code>effect_type</code> <code>str</code> <p>Type of transition effect \"fade\": Fade-in transition \"slide\": Slide-in transition \"none\": No transition (default)</p> <code>'none'</code> <code>**kwargs</code> <p>Additional parameters for the specific effect: frames (int): Number of frames for the transition direction (str): Direction for slide transition (\"left\", \"right\", \"up\", \"down\")</p> <code>{}</code>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.Transition.apply","title":"<code>apply(frame, progress, **kwargs)</code>","text":"<p>Apply the selected transition effect to a frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>The frame to apply the effect to (PIL Image or numpy array)</p> required <code>progress</code> <code>float</code> <p>Progress of the transition, from 0.0 to 1.0</p> required <code>**kwargs</code> <p>Additional arguments that may include: opacity_only (bool): If True, just return the opacity value (for fade effect)</p> <code>{}</code> <p>Returns:</p> Type Description <p>The modified frame with the transition effect applied</p>"},{"location":"audim/sub2pod/elements/header/","title":"Header","text":"<p>The header element is a component that is usually used at the topmost position in the layout. It is responsible for displaying the podcast title and the host's profile picture.</p> <p>Below is the API documentation for the header element:</p>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header","title":"<code>audim.sub2pod.elements.header</code>","text":""},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header","title":"<code>Header</code>","text":"<p>Header component for podcast layouts</p> <p>This component is responsible for displaying the header at the top of the podcast video frame. It may include various elements like logo, title, host profile, guest profile, etc.</p>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header.__init__","title":"<code>__init__(height=150, background_color=(30, 30, 30))</code>","text":"<p>Initialize the layout header</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Height of the header, defaults to 150</p> <code>150</code> <code>background_color</code> <code>tuple</code> <p>RGB background color, defaults to RGB (30, 30, 30)</p> <code>(30, 30, 30)</code> <code>text_renderer</code> <code>TextRenderer</code> <p>optional text renderer for the header,                           defaults to a new instance</p> required <code>logo</code> <code>Image</code> <p>optional logo image, defaults to None</p> required <code>logo_size</code> <code>tuple</code> <p>optional size of the logo, defaults to (100, 100)</p> required"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header.set_logo","title":"<code>set_logo(logo_path, size=(100, 100))</code>","text":"<p>Set the logo for the header</p> <p>Parameters:</p> Name Type Description Default <code>logo_path</code> <code>str</code> <p>Path to the logo image</p> required <code>size</code> <code>tuple</code> <p>Size of the logo, defaults to (100, 100)</p> <code>(100, 100)</code>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header.draw","title":"<code>draw(frame, draw, width, title='My Podcast', opacity=255)</code>","text":"<p>Draws the header on the frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>Image</code> <p>Frame to draw the header on</p> required <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>width</code> <code>int</code> <p>Width of the frame</p> required <code>title</code> <code>str</code> <p>Title of the podcast</p> <code>'My Podcast'</code> <code>opacity</code> <code>int</code> <p>Opacity of the header, defaults to 255</p> <code>255</code>"},{"location":"audim/sub2pod/elements/profile/","title":"Profile","text":"<p>The profile element is a component that is used to display the profile picture (DP) of the host.</p> <p>Below is the API documentation for the profile element:</p>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile","title":"<code>audim.sub2pod.elements.profile</code>","text":""},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture","title":"<code>ProfilePicture</code>","text":"<p>Handles user profile pictures or display picture with various shapes and effects</p> <p>This component is responsible for displaying the profile picture of the speaker. It may also include various shapes and effects like circle, square, highlight, etc.</p>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture.__init__","title":"<code>__init__(image_path, size=(120, 120), shape='circle')</code>","text":"<p>Initialize a profile picture</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the profile image</p> required <code>size</code> <code>tuple</code> <p>Width and height of the profile picture</p> <code>(120, 120)</code> <code>shape</code> <code>str</code> <p>Shape of the profile picture (\"circle\" or \"square\"),</p> <code>'circle'</code>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture.highlight","title":"<code>highlight(draw, position, color=(255, 200, 0), width=3, opacity=255)</code>","text":"<p>Add highlight around the profile picture</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>position</code> <code>tuple</code> <p>Position of the profile picture</p> required"},{"location":"audim/sub2pod/elements/text/","title":"Text","text":"<p>The text element is a component that is used to display the text, dialogue, or any other text content in the podcast.</p> <p>Below is the API documentation for the text element:</p>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text","title":"<code>audim.sub2pod.elements.text</code>","text":""},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer","title":"<code>TextRenderer</code>","text":"<p>Handles text rendering with various styles and wrapping</p> <p>This component is responsible for rendering text on the frame with various styles and wrapping. It can handle different fonts, sizes, colors, and anchor points.</p>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the text renderer with default fonts</p>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.get_font","title":"<code>get_font(size)</code>","text":"<p>Get or create a font of the specified size</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Size of the font</p> required"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.draw_text","title":"<code>draw_text(draw, text, position, font_size=40, color=(255, 255, 255, 255), anchor='mm')</code>","text":"<p>Draw text at the specified position</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>text</code> <code>str</code> <p>Text to draw</p> required <code>position</code> <code>tuple</code> <p>Position of the text</p> required <code>font_size</code> <code>int</code> <p>Size of the font, defaults to 40</p> <code>40</code> <code>color</code> <code>tuple</code> <p>Color of the text, defaults to RGB (255, 255, 255, 255)</p> <code>(255, 255, 255, 255)</code> <code>anchor</code> <code>str</code> <p>Anchor of the text (from PIL library), defaults to \"mm\".           See pillow docs: text anchors           for all possible options.</p> <code>'mm'</code>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.draw_wrapped_text","title":"<code>draw_wrapped_text(draw, text, position, max_width, font_size=40, color=(255, 255, 255, 255), anchor='lm')</code>","text":"<p>Draw text with word wrapping</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>text</code> <code>str</code> <p>Text to draw</p> required <code>position</code> <code>tuple</code> <p>Position of the text</p> required <code>max_width</code> <code>int</code> <p>Maximum width of the text before wrapping</p> required <code>font_size</code> <code>int</code> <p>Size of the font, defaults to 40</p> <code>40</code> <code>color</code> <code>tuple</code> <p>Color of the text, defaults to RGB (255, 255, 255, 255)</p> <code>(255, 255, 255, 255)</code> <code>anchor</code> <code>str</code> <p>Anchor of the text (from PIL library), defaults to \"lm\".           See pillow docs: text anchors           for all possible options.</p> <code>'lm'</code>"},{"location":"audim/sub2pod/elements/watermark/","title":"Watermark","text":"<p>The watermark element is a component that is used to display the watermark text at the bottom of the video frame.</p> <p>This is useful for branding and attribution. We can have the following texts as a watermark for Audim:</p> <ol> <li>\"made with \u2764\ufe0f by audim\" (default)</li> <li>\"made with \u2764\ufe0f and audim\"</li> <li>\"created with \u2764\ufe0f by audim\"</li> <li>\"made with audim\"</li> <li>\"created with audim\"</li> <li>\"generated with audim\"</li> <li>or any custom text attributing to \"Audim\"</li> </ol> <p>Below is the API documentation for the watermark element:</p>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark","title":"<code>audim.sub2pod.elements.watermark</code>","text":""},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark","title":"<code>Watermark</code>","text":"<p>Watermark component for video layouts</p> <p>This component is responsible for displaying a watermark text at the bottom of the video frame. It can be customized with different positions, colors, and opacity levels.</p>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.__init__","title":"<code>__init__(text='made with \u2764\ufe0f by audim', position='bottom-right', color=(255, 255, 255), opacity=150, font_size=20, margin=10)</code>","text":"<p>Initialize the watermark</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to display as watermark</p> <code>'made with \u2764\ufe0f by audim'</code> <code>position</code> <code>str</code> <p>Position of the watermark             'bottom-left', 'bottom-center', or 'bottom-right'</p> <code>'bottom-right'</code> <code>color</code> <code>tuple</code> <p>RGB color of the watermark text</p> <code>(255, 255, 255)</code> <code>opacity</code> <code>int</code> <p>Opacity of the watermark (0-255)</p> <code>150</code> <code>font_size</code> <code>int</code> <p>Font size of the watermark text</p> <code>20</code> <code>margin</code> <code>int</code> <p>Margin from the edges in pixels</p> <code>10</code>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.set_text","title":"<code>set_text(text)</code>","text":"<p>Set the watermark text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to display as watermark</p> required"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.set_position","title":"<code>set_position(position)</code>","text":"<p>Set the watermark position</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>str</code> <p>Position of the watermark             'bottom-left', 'bottom-center', or 'bottom-right'</p> required"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.set_color","title":"<code>set_color(color)</code>","text":"<p>Set the watermark color</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>tuple</code> <p>RGB color of the watermark text</p> required"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.set_opacity","title":"<code>set_opacity(opacity)</code>","text":"<p>Set the watermark opacity</p> <p>Parameters:</p> Name Type Description Default <code>opacity</code> <code>int</code> <p>Opacity of the watermark (0-255)</p> required"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.set_font_size","title":"<code>set_font_size(font_size)</code>","text":"<p>Set the watermark font size</p> <p>Parameters:</p> Name Type Description Default <code>font_size</code> <code>int</code> <p>Font size of the watermark text</p> required"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.draw","title":"<code>draw(frame, draw, width, height, frame_opacity=255)</code>","text":"<p>Draw the watermark on the frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>Frame to draw the watermark on</p> required <code>draw</code> <p>Draw object to draw on the frame</p> required <code>width</code> <code>int</code> <p>Width of the frame</p> required <code>height</code> <code>int</code> <p>Height of the frame</p> required <code>frame_opacity</code> <code>int</code> <p>Opacity of the entire frame (for transitions)</p> <code>255</code>"},{"location":"audim/sub2pod/layouts/base/","title":"Base Layout","text":"<p>The base layout sets the base layout for the podcast.</p> <p>It must be overriden to create various layout structures placing various elements in different places in the scenes. This would determine how the video frames would look like in the podcast.</p> <p>A Layout is defined by a collection of elements and their positions and placements in a video frame with their effects. It defines the components of each frame in the video and their animations and transitions.</p> <p>Below is the API documentation for the base layout:</p>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base","title":"<code>audim.sub2pod.layouts.base</code>","text":""},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout","title":"<code>BaseLayout</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all layouts</p> <p>This class defines the base structure for all layout classes. It provides a common interface for adding speakers and creating frames and scenes.</p>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.__init__","title":"<code>__init__(video_width=1920, video_height=1080, content_horizontal_offset=0)</code>","text":"<p>Initialize the base layout</p> <p>Parameters:</p> Name Type Description Default <code>video_width</code> <code>int</code> <p>Width of the video</p> <code>1920</code> <code>video_height</code> <code>int</code> <p>Height of the video</p> <code>1080</code> <code>content_horizontal_offset</code> <code>int</code> <p>Horizontal offset for the content (positive values move content right, negative values move content left). This allows shifting the main content (display pictures and subtitles) within the frame while keeping the header fixed.</p> <code>0</code>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_transition_effect","title":"<code>set_transition_effect(effect_type, **kwargs)</code>","text":"<p>Set the transition effect for this layout</p> <p>Parameters:</p> Name Type Description Default <code>effect_type</code> <code>str</code> <p>Type of transition effect \"fade\": Fade-in transition (default) \"slide\": Slide-in transition \"none\": No transition</p> required <code>**kwargs</code> <p>Additional parameters for the effect frames (int): Number of frames for the transition direction (str): Direction for slide transition (\"left\", \"right\", \"up\", \"down\")</p> <code>{}</code>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_highlight_effect","title":"<code>set_highlight_effect(effect_type, **kwargs)</code>","text":"<p>Set the highlight effect for this layout</p> <p>Parameters:</p> Name Type Description Default <code>effect_type</code> <code>str</code> <p>Type of highlight effect \"pulse\": Pulsing highlight \"glow\": Glowing highlight \"underline\": Underline highlight \"box\": Box highlight \"none\": No highlight</p> required <code>**kwargs</code> <p>Additional parameters for the effect color (tuple): RGBA color for the highlight padding (int): Padding around the highlighted area min_size (float): For pulse, minimum size factor (e.g., 0.8) max_size (float): For pulse, maximum size factor (e.g., 1.2) blur_radius (int): Blur radius for glow effect</p> <code>{}</code> <code>thickness</code> <code>int</code> <p>Line thickness for underline/box</p> required"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_content_offset","title":"<code>set_content_offset(offset)</code>","text":"<p>Set horizontal offset for the main content area</p> <p>This method allows shifting the main content (display pictures and subtitles) horizontally within the frame while keeping the header fixed. Useful for adjusting the layout based on subtitle length.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Horizontal offset in pixels.</p> required"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.enable_watermark","title":"<code>enable_watermark(show=True)</code>","text":"<p>Enable or disable the watermark</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>Whether to show the watermark</p> <code>True</code>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_watermark_text","title":"<code>set_watermark_text(text)</code>","text":"<p>Set the watermark text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to display as watermark</p> required"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_watermark_position","title":"<code>set_watermark_position(position)</code>","text":"<p>Set the watermark position</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>str</code> <p>Position of the watermark 'bottom-left', 'bottom-center', or 'bottom-right'</p> required"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_watermark_color","title":"<code>set_watermark_color(color)</code>","text":"<p>Set the watermark color</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>tuple</code> <p>RGB color of the watermark text</p> required"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_watermark_opacity","title":"<code>set_watermark_opacity(opacity)</code>","text":"<p>Set the watermark opacity</p> <p>Parameters:</p> Name Type Description Default <code>opacity</code> <code>int</code> <p>Opacity of the watermark (0-255)</p> required"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_watermark_properties","title":"<code>set_watermark_properties(**kwargs)</code>","text":"<p>Set multiple watermark properties at once</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for watermark properties: text (str): Watermark text position (str): Watermark position color (tuple): RGB color opacity (int): Opacity font_size (int): Font size margin (int): Margin from edges</p> <code>{}</code>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.add_speaker","title":"<code>add_speaker(name, image_path)</code>  <code>abstractmethod</code>","text":"<p>Add a speaker to the layout</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the speaker</p> required <code>image_path</code> <code>str</code> <p>Path to the speaker's image</p> required"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.create_frame","title":"<code>create_frame(current_sub=None, opacity=255)</code>  <code>abstractmethod</code>","text":"<p>Create a frame with the current subtitle</p> <p>Parameters:</p> Name Type Description Default <code>current_sub</code> <code>str</code> <p>Current subtitle</p> <code>None</code> <code>opacity</code> <code>int</code> <p>Opacity of the subtitle</p> <code>255</code>"},{"location":"audim/sub2pod/layouts/podcast/","title":"Podcast Layout","text":"<p>The podcast layout is designed to create dynamic scenes for the podcast.</p> <p>Typically, it features:</p> <ul> <li>profile pictures of the speakers (which are highlighted when they are speaking)</li> <li>text dialogues of the speakers (which is displayed in real-time as they speak)</li> </ul> <p>Below is the API documentation for the podcast layout:</p>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast","title":"<code>audim.sub2pod.layouts.podcast</code>","text":""},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout","title":"<code>PodcastLayout</code>","text":"<p>               Bases: <code>BaseLayout</code></p> <p>Standard podcast layout with profile pictures and subtitles</p> <p>This layout is designed for standard podcast videos with a header section, profile pictures, and subtitles. It provides a flexible structure for adding speakers and creating frames with customizable parameters.</p>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.__init__","title":"<code>__init__(video_width=1920, video_height=1080, header_height=150, dp_size=(120, 120), show_speaker_names=True, content_horizontal_offset=0, show_watermark=True)</code>","text":"<p>Initialize podcast layout</p> <p>Parameters:</p> Name Type Description Default <code>video_width</code> <code>int</code> <p>Width of the video</p> <code>1920</code> <code>video_height</code> <code>int</code> <p>Height of the video</p> <code>1080</code> <code>header_height</code> <code>int</code> <p>Height of the header section</p> <code>150</code> <code>dp_size</code> <code>tuple</code> <p>Size of profile pictures</p> <code>(120, 120)</code> <code>show_speaker_names</code> <code>bool</code> <p>Whether to show speaker names</p> <code>True</code> <code>content_horizontal_offset</code> <code>int</code> <p>Horizontal offset for the content (positive values move content right, negative values move content left)</p> <code>0</code> <code>show_watermark</code> <code>bool</code> <p>Whether to show the watermark</p> <code>True</code>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.set_content_offset","title":"<code>set_content_offset(offset)</code>","text":"<p>Set horizontal offset for the content (display pictures and subtitles)</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Horizontal offset in pixels. Positive values move content right,  negative values move content left.</p> required"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.add_speaker","title":"<code>add_speaker(name, image_path, shape='circle')</code>","text":"<p>Add a speaker to the layout</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the speaker</p> required <code>image_path</code> <code>str</code> <p>Path to the speaker's image</p> required <code>shape</code> <code>str</code> <p>Shape of the profile picture, defaults to \"circle\"</p> <code>'circle'</code>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.create_frame","title":"<code>create_frame(current_sub=None, opacity=255, background_color=(20, 20, 20), **kwargs)</code>","text":"<p>Create a frame with the podcast layout</p> <p>Parameters:</p> Name Type Description Default <code>current_sub</code> <code>str</code> <p>Current subtitle</p> <code>None</code> <code>opacity</code> <code>int</code> <p>Opacity of the subtitle</p> <code>255</code> <code>background_color</code> <code>tuple</code> <p>Background color in RGB format,                       defaults to (20, 20, 20)</p> <code>(20, 20, 20)</code> <code>**kwargs</code> <p>Additional keyword arguments: subtitle_position (float): Current position within subtitle in seconds subtitle_duration (float): Total duration of subtitle in seconds</p> <code>{}</code>"},{"location":"audim/utils/extract/","title":"Extract","text":"<p>The <code>Extract</code> is an utility class that is used to extract or convert media data from various types of media files. For example, it can be used to extract audio from video files.</p> <p>List of utilities provided by the <code>Extract</code> class:</p> <ul> <li><code>extract_audio</code>: Extract audio from a video file with no loss in quality.</li> <li>more to come...</li> </ul> <p>Below is the API documentation for the <code>Extract</code> utility:</p>"},{"location":"audim/utils/extract/#audim.utils.extract","title":"<code>audim.utils.extract</code>","text":""},{"location":"audim/utils/extract/#audim.utils.extract.Extract","title":"<code>Extract</code>","text":"<p>A class for extracting and converting various forms of media data from various types of media files</p>"},{"location":"audim/utils/extract/#audim.utils.extract.Extract.extract_audio","title":"<code>extract_audio(input_path, output_path, output_format='wav', bitrate='192k', sample_rate=44100)</code>","text":"<p>Extract audio from a video file with no loss in quality.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input video file</p> required <code>output_path</code> <code>str</code> <p>Path to save the output audio file</p> required <code>output_format</code> <code>str</code> <p>Format of the output audio file. e.g.: mp3, wav, flac (default: wav)</p> <code>'wav'</code> <code>bitrate</code> <code>str</code> <p>Bitrate for the output audio. e.g.: 128k, 192k, 320k (default: 192k)</p> <code>'192k'</code> <code>sample_rate</code> <code>int</code> <p>Sample rate for the output audio. e.g.: 44100, 48000, 96000 (default: 44100)</p> <code>44100</code> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: Path to the output audio file if extraction was successful, None otherwise</p>"},{"location":"audim/utils/playback/","title":"Playback","text":"<p>The <code>Playback</code> is an utility class that is used to playback various outputs and generations from Audim.</p> <p>It is also used to test and validate the results of the transcriber, subtitle generator and video generator (coming soon).</p> <p>List of utilities provided by the <code>Playback</code> class:</p> <ul> <li><code>play_audio_with_srt</code>: Play an audio file with synchronized subtitles in CLI</li> <li>more to come...</li> </ul> <p>Below is the API documentation for the <code>Playback</code> utility:</p>"},{"location":"audim/utils/playback/#audim.utils.playback","title":"<code>audim.utils.playback</code>","text":""},{"location":"audim/utils/playback/#audim.utils.playback.Playback","title":"<code>Playback</code>","text":"<p>Class for synchronized audio and video playback with subtitles</p> <p>Use it to playback various outputs and generations from Audim and testing and validating the results</p>"},{"location":"audim/utils/playback/#audim.utils.playback.Playback.play_audio_with_srt","title":"<code>play_audio_with_srt(audio_file, srt_file)</code>","text":"<p>Play audio file with synchronized subtitles in CLI</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>str</code> <p>Path to the audio file</p> required <code>srt_file</code> <code>str</code> <p>Path to the SRT subtitle file</p> required"},{"location":"audim/utils/subtitle/","title":"Subtitle","text":"<p>The <code>Subtitle</code> is an utility class that is used to handle and modify subtitle files.</p> <p>It is used to handle the output subtitle files from <code>aud2sub</code> before feeding them to <code>sub2pod</code> for video generation.</p> <p>List of utilities provided by the <code>Subtitle</code> class:</p> <ul> <li><code>replace_speakers</code>: Replace the audim speaker tags (names) in the subtitle file with new ones</li> <li><code>preview_replacement</code>: Preview the changes from <code>replace_speakers</code> in CLI without modifying the file</li> <li>more to come...</li> </ul> <p>Below is the API documentation for the <code>Subtitle</code> utility:</p>"},{"location":"audim/utils/subtitle/#audim.utils.subtitle","title":"<code>audim.utils.subtitle</code>","text":""},{"location":"audim/utils/subtitle/#audim.utils.subtitle.Subtitle","title":"<code>Subtitle</code>","text":"<p>Contains utility functions for SRT files</p>"},{"location":"audim/utils/subtitle/#audim.utils.subtitle.Subtitle.replace_speakers","title":"<code>replace_speakers(srt_file, speakers, in_place=True)</code>","text":"<p>Replace speaker placeholders with actual names in SRT file</p> <p>Example, allows replacing \"[Speaker 1]\", \"[Speaker 2]\", etc. with actual speaker names such as \"[Host]\", \"[Guest]\", etc.</p> <p>Parameters:</p> Name Type Description Default <code>srt_file</code> <code>str</code> <p>Path to the SRT file</p> required <code>speakers</code> <code>list or dict</code> <p>Either a list of speaker names in order or a dictionary mapping speaker numbers/names to actual names</p> required <code>in_place</code> <code>bool</code> <p>Whether to modify the file in place (default: True) If False, returns modified subs without saving</p> <code>True</code> <p>Returns:</p> Type Description <p>pysrt.SubRipFile: The modified subtitles object</p>"},{"location":"audim/utils/subtitle/#audim.utils.subtitle.Subtitle.preview_replacement","title":"<code>preview_replacement(srt_file, speakers, limit=5, pretty_print=True)</code>","text":"<p>Preview the speaker replacements without modifying the file</p> <p>Parameters:</p> Name Type Description Default <code>srt_file</code> <code>str</code> <p>Path to the SRT file</p> required <code>speakers</code> <code>list or dict</code> <p>Either a list of speaker names in order or a dictionary mapping speaker numbers/names to actual names</p> required <code>limit</code> <code>int</code> <p>Maximum number of subtitles to display in preview</p> <code>5</code> <code>pretty_print</code> <code>bool</code> <p>Whether to print a formatted preview to console</p> <code>True</code> <p>Returns:</p> Name Type Description <code>list</code> <p>List of tuples with (original_text, modified_text)</p>"},{"location":"devblog/","title":"Dev Blog","text":"<p>This is a blog space documenting the development of the Audim project.</p> <p>It contains details of version wise updates and progression of the project in a blog format. You can treat it as a changelog for the project, or a brain dump and thought process dump during the development of the project.</p>"},{"location":"devblog/#blog-contents","title":"Blog Contents","text":"<p>Each blog post might include (and not limited to) the following:</p> <ul> <li>changes made in each version</li> <li>rationale behind the changes</li> <li>design decisions</li> <li>features added</li> <li>end user usage and code snippets</li> </ul> Blog Post Date Description v0.0.1 Mar 05, 2025 generate basic video from SRT files v0.0.2 Mar 13, 2025 making professional podcast video from SRT files v0.0.3 Apr 16, 2025 cleaner design with progressive disclosure of complexity v0.0.4 NA no blog post v0.0.5 Apr 22, 2025 SRT transcription from audio + playback utility v0.0.6 May 15, 2025 Audio extraction from video files v0.0.7 May 18, 2025 A final look at the project for first release"},{"location":"devblog/#changelog","title":"Changelog","text":"<p>Since we are documenting the development of the project, why not have a changelog and PR trackers for the project?</p> Version Date Modules Affected Feature Changes PR Links v0.0.1 Mar 05, 2025 <code>sub2pod</code> subtitle text to podcast video #3 v0.0.2 Mar 13, 2025 <code>sub2pod</code> optimize &amp; parallize <code>sub2pod</code> #5 v0.0.3 Apr 17, 2025 <code>sub2pod</code> <code>effects</code> submodule + design changes #11 v0.0.4 Apr 22, 2025 <code>aud2sub</code>, <code>sub2pod</code> audio to srt + ts normalization + pos offset #15, #14, #13 v0.0.5 Apr 22, 2025 <code>util</code> playback audio with subs + replace speaker names #17 v0.0.6 May 15, 2025 <code>util</code> extract audio from video #24 v0.0.7* NA ??? final first release ??? <p>Note: In the above table, the versions marked with <code>*</code> are released.</p>"},{"location":"devblog/v0.0.1/","title":"Basic Podcast Video Generation","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: March 05, 2025</li> <li>Last Updated: March 05, 2025</li> <li>Compatible with: Audim v0.0.1</li> </ul> <p>This example demonstrates how to generate a simple podcast video from a subtitle file and audio file using Audim's Sub2Pod module.</p>"},{"location":"devblog/v0.0.1/#prerequisites","title":"Prerequisites","text":"<p>Before running this example, make sure you have:</p> <ol> <li>Installed Audim following the installation instructions</li> <li> <p>Created the required input files:</p> <ul> <li><code>input/podcast.srt</code> - Subtitle file with speaker tags</li> <li><code>input/podcast.mp3</code> - Audio file of the podcast</li> <li><code>input/host_dp.png</code> - Host profile picture</li> <li><code>input/guest_dp.png</code> - Guest profile picture</li> <li><code>input/logo.png</code> - Brand logo (optional)</li> </ul> </li> </ol>"},{"location":"devblog/v0.0.1/#example-srt-file","title":"Example SRT File","text":"<p>Your SRT file should follow the standard SubRip Subtitle format with added speaker tags as expected by Audim.</p> <p>Below is an example of an SRT file with speaker tags:</p> <pre><code>1\n00:00:00,000 --&gt; 00:00:04,500\n[Host] Welcome to our podcast!\n\n2\n00:00:04,600 --&gt; 00:00:08,200\n[Guest] Thank you! Glad to be here.\n</code></pre>"},{"location":"devblog/v0.0.1/#example-code-implementation","title":"Example Code Implementation","text":"<p>The following code demonstrates how to generate a podcast video from an SRT file and audio file using Audim's Sub2Pod module.</p> <pre><code>from datetime import datetime\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Add speakers\nprint(\"Adding speakers...\")\nlayout.add_speaker(\"Host\", \"input/host_dp.png\")\nlayout.add_speaker(\"Guest\", \"input/guest_dp.png\")\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=\"input/podcast.srt\",\n    audio_path=\"input/podcast.mp3\",\n    logo_path=\"input/logo.png\",\n    title=\"My Awesome Podcast\"\n)\n\n# Export the final video\nprint(\"Exporting video...\")\ndatetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\ngenerator.export_video(f\"output/podcast_{datetime}.mp4\")\n</code></pre>"},{"location":"devblog/v0.0.1/#output","title":"Output","text":"<p>The output podcast video will be saved in the <code>output</code> directory with a timestamp in the filename.</p>"},{"location":"devblog/v0.0.1/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with this example: - Verify you're using the compatible version of Audim - Check that all input files exist and are in the correct format - Ensure your SRT file has proper speaker tags in the format <code>[SpeakerName]</code></p>"},{"location":"devblog/v0.0.1/#see-also","title":"See Also","text":"<ul> <li>API Documentation for PodcastLayout</li> <li>API Documentation for VideoGenerator</li> </ul>"},{"location":"devblog/v0.0.2/","title":"Creating a Professional Podcast Video with Audim","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: March 13, 2025</li> <li>Last Updated: March 14, 2025</li> <li>Compatible with: Audim v0.0.2</li> </ul> <p>This example demonstrates how to create a professional-looking podcast video using Audim's <code>Sub2Pod</code> module, featuring real speakers with profile pictures, custom branding, and high-quality output.</p>"},{"location":"devblog/v0.0.2/#overview","title":"Overview","text":"<p>In this tutorial, we'll transform a conversation between Grant Sanderson (from 3Blue1Brown) and Sal Khan (from Khan Academy) into a visually engaging podcast video. We'll walk through:</p> <ol> <li>Preparing the input files</li> <li>Setting up the podcast layout</li> <li>Generating the video with Audim</li> <li>Reviewing the final output</li> </ol> <p>Note: The conversation between Grant and Sal is taken from this podcast.</p>"},{"location":"devblog/v0.0.2/#input-files","title":"Input Files","text":""},{"location":"devblog/v0.0.2/#1-podcast-audio-file","title":"1. Podcast Audio File","text":"<p>We need to have the audio recording of the podcast that we want to convert to a video.</p> <p>Below is a sample of the podcast audio we'll be using:</p>      Your browser does not support the audio element.    <p>Audio snippet from \"Sal Khan: Beyond Khan Academy | 3b1b Podcast #2\"</p>"},{"location":"devblog/v0.0.2/#2-podcast-subtitles-file-srt","title":"2. Podcast Subtitles File (.SRT)","text":"<p>The SRT file should contain the transcription with speaker tags for the package to understand the speaker for each text. The SRT file should follow the standard SubRip Subtitle format with added speaker tags as expected by Audim.</p> <p>Below is the SRT file used for this example:</p>"},{"location":"devblog/v0.0.2/#3-other-files","title":"3. Other Files","text":"<p>Along with the audio and subtitles files, we also need the following files:</p> <ul> <li>Profile Picture of Grant Sanderson</li> <li>Profile Picture of Sal Khan</li> <li>Brand Logo of 3Blue1Brown</li> </ul>"},{"location":"devblog/v0.0.2/#code-implementation","title":"Code Implementation","text":"<p>After gathering all the files, we can now generate the podcast video using Audim.</p> <p>Here's the complete code to generate our podcast video:</p> <pre><code>from datetime import datetime\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Add speakers\nprint(\"Adding speakers...\")\nlayout.add_speaker(\"Grant Sanderson\", \"input/grant.png\")\nlayout.add_speaker(\"Sal Khan\", \"input/sal.png\")\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=\"input/podcast.srt\",\n    audio_path=\"input/podcast.mp3\",\n    logo_path=\"input/logo.png\",\n    title=\"3b1b Podcast: Sal Khan: Beyond Khan Academy\",\n    cpu_core_utilization=\"max\"\n)\n\n# Export the final video\nprint(\"Exporting video...\")\ndatetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\ngenerator.export_video(f\"output/podcast_{datetime}.mp4\")\n</code></pre> <p>Here is the terminal logs upon running the code:</p> <pre><code>(audim) (base) atanu@atanu-LOQ-15APH8:~/Workspace/GitHub/audim$ python test.py \nCreating layout...\nAdding speakers...\nGenerating video...\n[2025-03-13 23:25:10] VideoGenerator (INFO) - Loading subtitles from input/podcast.srt\n[2025-03-13 23:25:10] VideoGenerator (INFO) - Using 16 CPU cores for parallel processing\n[2025-03-13 23:25:10] VideoGenerator (INFO) - Processing subtitle to generate frames in 23 batches\nProcessing batch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:37&lt;00:00,  1.64s/batch, frames processed=4727]\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Frame generation completed: Total 4727 frames created\nExporting video...\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Starting video generation process with 4727 frames\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Video duration: 156.06s (adjusted to match audio)\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Attempting video export with FFmpeg encoding\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Preparing frame list for FFmpeg\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Using NVIDIA GPU acceleration for video encoding\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Starting FFmpeg encoding process\nEncoding video:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 99/100 [01:09&lt;00:00,  1.42%/s]\n[2025-03-13 23:26:58] VideoGenerator (INFO) - Video successfully encoded to output/podcast_20250313232548.mp4\n[2025-03-13 23:26:58] VideoGenerator (INFO) - Cleaned up temporary files in /tmp/tmpoqv_t5x6\n[2025-03-13 23:26:58] VideoGenerator (INFO) - Video generation completed! Exported to: output/podcast_20250313232548.mp4\n</code></pre>"},{"location":"devblog/v0.0.2/#output-video","title":"Output Video","text":"<p>Here's how the generated video looks like upon completion of the rendering process:</p>      Your browser does not support the video element."},{"location":"devblog/v0.0.2/#code-breakdown","title":"Code Breakdown","text":"<ol> <li>Layout Creation: We create a <code>PodcastLayout</code> with Full HD resolution (1920\u00d71080) and enable speaker name display.</li> <li>Speaker Configuration: We add two speakers with their respective profile pictures.</li> <li>Video Generation: We initialize a <code>VideoGenerator</code> with our layout and set the frame rate to 30 FPS.</li> <li>Content Processing: The generator processes our SRT and audio files, incorporating the logo and title.</li> <li>Performance Optimization: We use <code>cpu_core_utilization=\"max\"</code> to leverage all available CPU cores for faster frame generation + native system FFmpeg with NVIDIA GPU acceleration for faster video encoding and rendering.</li> <li>Export: The final video is saved with a timestamp in the filename for easy versioning.</li> </ol>"},{"location":"devblog/v0.0.2/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ul> <li>Ensure your SRT file has proper speaker tags in the format <code>[SpeakerName]</code></li> <li>Verify that speaker names in the SRT match exactly with those added via <code>add_speaker()</code></li> <li>Check that all input files exist and are in the correct format</li> <li>For performance issues, try adjusting the <code>cpu_core_utilization</code> parameter</li> </ul>"},{"location":"devblog/v0.0.2/#see-also","title":"See Also","text":"<ul> <li>Basic Podcast Example</li> <li>API Documentation for VideoGenerator</li> <li>API Documentation for PodcastLayout</li> </ul>"},{"location":"devblog/v0.0.3/","title":"Advanced Video Effects with Progressive Disclosure of Complexity","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: April 16, 2025</li> <li>Last Updated: April 17, 2025</li> <li>Compatible with: Audim v0.0.3</li> </ul> <p>This example explores Audim's implementation of the \"progressive disclosure of complexity\" design principle through the <code>effects</code> module in the <code>sub2pod</code> package. We'll demonstrate how Audim provides a smooth learning curve for users of all experience levels.</p>"},{"location":"devblog/v0.0.3/#what-is-progressive-disclosure-of-complexity","title":"What is Progressive Disclosure of Complexity?","text":"<p>Progressive disclosure of complexity is a design principle that gradually reveals advanced functionality as users become more experienced with a system. This concept originated in UI design but has been adapted for API design in software libraries.</p> <p>As described by design experts:</p> <p>\"Progressive disclosure is an interaction design technique that sequences information and actions across several screens in order to reduce feelings of overwhelm for the user.\"</p> <p>\u2014 Interaction Design Foundation</p> <p>Specially in module or library design, this principle is realized by creating higher level APIs and lower level APIs. While the lower level APIs are granular, fundamental, small and rigid catering to specific functionalities of the module or library. The higher level APIs are complex and formed by combining the lower level APIs catering to specific end user requirements and use cases.</p> <p>This results in end users using the higher level APIs with ease, getting out of the box experience, while also being able to dig deeper into the lower level APIs for specific customizations when needed.</p> <p>The best way to put it is to quote the creator of Keras:</p> <p>\"A key design principle I follow in libraries (e.g. Keras) is 'progressive disclosure of complexity'. Make it easy to get started, yet make it possible to handle arbitrarily flexible use cases, only requiring incremental learning at each step.\"</p> <p>\u2014 Fran\u00e7ois Chollet</p> <p>This approach is used by many popular libraries like Keras, Hugging Face Transformers, Hugging Face Diffusers and many more allowing users to:</p> <ul> <li>Start simple with sensible defaults</li> <li>Incrementally discover more advanced features</li> <li>Access the full power of the library when needed</li> </ul>"},{"location":"devblog/v0.0.3/#how-audims-sub2pod-submodule-implements-progressive-disclosure","title":"How Audim's <code>sub2pod</code> submodule Implements Progressive Disclosure","text":"<p>Audim's <code>sub2pod</code> submodule implements progressive disclosure of complexity through a carefully designed hierarchy of abstractions:</p> <ol> <li> <p>High-Level API (<code>VideoGenerator</code> &amp; <code>PodcastLayout</code>)</p> <ul> <li><code>VideoGenerator</code> takes care of video generation and rendering</li> <li><code>PodcastLayout</code> takes care of how each frames in the video will look like</li> <li>Provides out-of-the-box functionality for podcast video generation</li> <li>Offers sensible defaults for all parameters</li> <li>Perfect for users who want to get started quickly</li> </ul> </li> <li> <p>Mid-Level API (<code>BaseLayout</code>)</p> <ul> <li>Allows customization of layouts, elements in the layout and it's effects</li> <li>Provides string-based configuration for common use cases</li> <li>Enables users to put together various elements and effects to create their own layouts</li> <li>It's like a lego or a puzzle piece where lower level elements and effects can be used to create new layouts</li> <li>Ideal for users who want to customize their videos</li> </ul> </li> <li> <p>Lower-Level API (<code>Header</code>, <code>Profile</code>, <code>Text</code> Elements and <code>Transition</code>, <code>Highlight</code> Effects)</p> <ul> <li>Offers complete control over each elements and effects</li> <li>These elements are the fundamental building blocks that Audim provides</li> <li>These effects are the fundamental animations that Audim provides</li> <li>These elements and effects can be combined to create new layouts and animations</li> </ul> </li> <li> <p>Lowest-Level API (<code>BaseElement</code> &amp; <code>BaseEffect</code>)</p> <ul> <li>Offers complete control over rendering and effects</li> <li>Allows creation of custom elements and effects</li> <li>Provides access to all parameters and methods</li> <li>Designed for power users and developers</li> </ul> </li> </ol> <p>So, by providing API abstractions from higher levels to the lower levels, Audim's <code>sub2pod</code> submodule allows users to:</p> <ul> <li>Start using and generation podcast videos out of the box with sensible defaults</li> <li>Incrementally discover more advanced features and customizations when needed</li> <li>Access the full power of the library when needed with fine-tuned control over the layouts, elements and effects</li> <li>Power users can also implement their own custom layouts, elements and effects by overriding <code>BaseLayout</code>, <code>BaseElement</code> and <code>BaseEffect</code> classes.</li> </ul> <p>This layered approach creates a natural progression path for users:</p> <pre><code># Level 1: High-Level API (Simple)\nlayout = PodcastLayout(...)\ngenerator = VideoGenerator(layout)\ngenerator.generate_from_srt(...)\n</code></pre> <pre><code># Level 2: Mid-Level API (Customization)\nlayout.set_transition_effect(\"fade\")\nlayout.set_highlight_effect(\"none\")\n</code></pre> <pre><code># Level 3: Low-Level API\nclass CustomEffect(BaseEffect):\n    # Override for full customization and control\n    def apply(self, frame, progress):\n        # Custom effect implementation\n        pass\n</code></pre> <p>So, the abstraction layers and inheritance hierarchy of Audim's <code>sub2pod</code> submodule can be visualized as follows:</p> <pre><code>graph TD\n    A[VideoGenerator] --&gt; B[PodcastLayout]\n    B --&gt; C[BaseLayout]\n    C --&gt; D[Elements]\n    C --&gt; E[Effects]\n    D --&gt; F[Header]\n    D --&gt; G[ProfilePicture]\n    D --&gt; H[TextRenderer]\n    E --&gt; I[Transition]\n    E --&gt; J[Highlight]\n    I --&gt; K[BaseTransition]\n    J --&gt; L[BaseHighlight]</code></pre>"},{"location":"devblog/v0.0.3/#why-this-is-powerful","title":"Why this is powerful?","text":""},{"location":"devblog/v0.0.3/#1-progressive-disclosure-of-complexity","title":"1. Progressive Disclosure of Complexity","text":"<p>This approach creates a natural hierarchy of complexity:</p> <ul> <li>Simple level: Users choose a pre-configured layout with default effects</li> <li>Intermediate level: Users customize effects for elements on existing layouts</li> <li>Advanced level: Users create custom layouts with custom elements and effects</li> </ul> <p>This matches how video editors typically work - first selecting templates, then adjusting effects, and finally creating custom compositions when needed.</p>"},{"location":"devblog/v0.0.3/#2-end-user-experience","title":"2. End User Experience","text":"<p>For video creators and editors, this model is intuitive because:</p> <ul> <li>It follows familiar mental models from tools like OBS, Premiere Pro, and After Effects</li> <li>Effects are naturally tied to how content appears (the layout)</li> <li>The separation keeps the API clean while maintaining flexibility</li> <li>Users can think in terms of \"scenes\" (layouts) that have both positioning and visual effects</li> </ul>"},{"location":"devblog/v0.0.3/#3-developer-experience","title":"3. Developer Experience","text":"<p>For developers and power users, this architecture provides:</p> <ul> <li>Clear extension points for adding new features</li> <li>Well-defined interfaces between components</li> <li>Easy testing and maintenance of individual components</li> <li>Ability to mix and match different levels of abstraction</li> </ul>"},{"location":"devblog/v0.0.3/#4-performance-and-maintainability","title":"4. Performance and Maintainability","text":"<p>The layered architecture also benefits the codebase itself:</p> <ul> <li>Each layer can be optimized independently</li> <li>Changes in one layer don't affect others</li> <li>Easier to add new features without breaking existing code</li> <li>Better separation of concerns</li> </ul>"},{"location":"devblog/v0.0.3/#how-the-new-effects-module-implements-progressive-disclosure","title":"How the new <code>effects</code> module Implements Progressive Disclosure","text":"<p>Audim's <code>effects</code> module provides transition and highlight effects for videos with three distinct levels of complexity:</p> <ol> <li>Level 1: Default Usage - No configuration needed</li> <li>Level 2: Simple Customization - Basic string-based configuration</li> <li>Level 3: Advanced Customization - Detailed parameter configuration</li> </ol> <p>This approach allows beginners to get started quickly while giving advanced users the power and flexibility they need.</p>"},{"location":"devblog/v0.0.3/#example-three-levels-of-complexity","title":"Example: Three Levels of Complexity","text":"<p>Let's explore how you can use Audim's effects at different complexity levels:</p>"},{"location":"devblog/v0.0.3/#level-1-default-usage-no-configuration","title":"Level 1: Default Usage (No Configuration)","text":"<p>The simplest way to use Audim is with default settings. The <code>PodcastLayout</code> automatically includes a default fade transition:</p> <pre><code>from audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout with default effects\nlayout = PodcastLayout(\n    video_width=1920, \n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Add speakers\nlayout.add_speaker(\"Host\", \"input/host.png\")\nlayout.add_speaker(\"Guest\", \"input/guest.png\")\n\n# Create generator with this layout\ngenerator = VideoGenerator(layout)\n\n# The layout will automatically use the default effects\n# No explicit configuration needed!\n</code></pre> <p>At this level, users don't need to know anything about effects - they just work.</p>"},{"location":"devblog/v0.0.3/#level-2-simple-customization","title":"Level 2: Simple Customization","text":"<p>As users become more comfortable, they can easily customize effects by simply specifying the effect type:</p> <pre><code># Create podcast layout\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Simple customization - just specify effect type\nlayout.set_transition_effect(\"fade\")\nlayout.set_highlight_effect(\"glow\")\n\n# Add speakers and generate video as before\n</code></pre> <p>This level introduces a clean, string-based API that's easy to understand and use.</p>"},{"location":"devblog/v0.0.3/#level-3-advanced-customization","title":"Level 3: Advanced Customization","text":"<p>Power users can access detailed customization options when they need fine-grained control:</p> <pre><code># Create podcast layout\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Advanced customization with detailed parameters\nlayout.set_transition_effect(\n    \"slide\", \n    frames=25,                  # Longer transition (default: 15 frames)\n    direction=\"left\"            # Slide in from left\n)\n\nlayout.set_highlight_effect(\n    \"pulse\",\n    color=(255, 215, 0, 100),   # Custom gold color, semi-transparent\n    min_size=0.9,               # Subtle pulse (90% to 110% size)\n    max_size=1.1,\n    blur_radius=8               # More blur for softer effect\n)\n\n# Add speakers and generate video as before\n</code></pre> <p>At this level, users have complete control over every aspect of the effects.</p>"},{"location":"devblog/v0.0.3/#full-implementation-example","title":"Full Implementation Example","text":"<p>Here's how you might use Audim's <code>sub2pod</code> submodule in a complete project:</p> <pre><code>from datetime import datetime\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Set custom effects for the layout\nlayout.set_transition_effect(\"fade\", frames=20)\nlayout.set_highlight_effect(\"none\")\n\n# Add speakers\nprint(\"Adding speakers...\")\nlayout.add_speaker(\"Grant Sanderson\", \"input/grant.png\")\nlayout.add_speaker(\"Sal Khan\", \"input/sal.png\")\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=\"input/podcast.srt\",\n    audio_path=\"input/podcast.mp3\",\n    logo_path=\"input/logo.png\",\n    title=\"3b1b Podcast: Sal Khan: Beyond Khan Academy\",\n    cpu_core_utilization=\"max\"\n)\n\n# Export the final video\nprint(\"Exporting video...\")\ndatetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\ngenerator.export_video(f\"output/podcast_underline_{datetime}.mp4\")\n</code></pre> <p>Here's how the generated video looks like upon completion of the rendering process:</p>      Your browser does not support the video element."},{"location":"devblog/v0.0.3/#available-effects","title":"Available Effects","text":""},{"location":"devblog/v0.0.3/#transition-effects","title":"Transition Effects","text":"<p>Transitions control how each frame fades in or slides into view:</p> Effect Type Description Parameters <code>\"none\"</code> No transition (default) None <code>\"fade\"</code> Smooth fade-in transition <code>frames</code>: Duration of fade <code>\"slide\"</code> Slide-in animation <code>frames</code>: Duration of slide, <code>direction</code>: \"left\", \"right\", \"up\", or \"down\""},{"location":"devblog/v0.0.3/#highlight-effects","title":"Highlight Effects","text":"<p>Highlights emphasize the active speaker's text:</p> Effect Type Description Parameters <code>\"none\"</code> No highlight None <code>\"pulse\"</code> Pulsing animation <code>color</code>, <code>min_size</code>, <code>max_size</code>, <code>blur_radius</code> <code>\"glow\"</code> Glowing background <code>color</code>, <code>blur_radius</code> <code>\"underline\"</code> Simple underline <code>color</code>, <code>thickness</code> <code>\"box\"</code> Box around text <code>color</code>, <code>padding</code>, <code>thickness</code>"},{"location":"devblog/v0.0.3/#benefits-of-progressive-disclosure","title":"Benefits of Progressive Disclosure","text":"<p>This design pattern offers several advantages:</p> <ol> <li>Reduced Learning Curve: New users can be productive immediately without being overwhelmed</li> <li>Smooth Progression: Users can gradually discover more advanced features as they need them</li> <li>Documentation Organization: Documentation can target different user groups based on expertise</li> <li>API Cleanliness: The API remains clean and intuitive at all levels</li> <li>Flexibility: Advanced users can access powerful features without sacrificing simplicity for beginners</li> </ol>"},{"location":"devblog/v0.0.3/#real-world-impact","title":"Real-World Impact","text":"<p>The progressive disclosure pattern in Audim helps different types of users:</p> <ul> <li>Content Creators: Can quickly generate basic podcast videos without technical knowledge</li> <li>Video Editors: Can customize effects to match their brand and style</li> <li>Developers: Can achieve precise control and integrate Audim into larger workflows</li> </ul>"},{"location":"devblog/v0.0.3/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with effects:</p> <ul> <li>Verify you're using Audim v0.0.3 or later</li> <li>Check that effect names are spelled correctly (e.g., \"fade\" not \"fading\")</li> <li>For slide transitions with text, ensure your text color includes an opacity value</li> <li>When using highlight effects, ensure the subtitle area is properly defined</li> </ul>"},{"location":"devblog/v0.0.3/#see-also","title":"See Also","text":"<ul> <li>Basic Podcast Example</li> <li>Professional Podcast Example</li> <li>API Documentation for Transitions</li> <li>API Documentation for Highlights</li> </ul>"},{"location":"devblog/v0.0.5/","title":"Automated Podcast Subtitling: From Audio to SRT","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: Apr 22, 2025</li> <li>Last Updated: May 18, 2025</li> <li>Compatible with: Audim v0.0.5</li> </ul> <p>This guide demonstrates how to use Audim's <code>aud2sub</code> module to automatically generate high-quality subtitles from podcast audio files. We'll also explore the <code>utils</code> module for replacing generic speaker labels and manually validating subtitles. The subtitles generated can finally be fed into <code>sub2pod</code> to generate podcast videos (as covered in previous examples).</p> <p>Together, these tools form a powerful foundation for podcast content creation.</p>"},{"location":"devblog/v0.0.5/#overview","title":"Overview","text":"<p>The audio-to-subtitle workflow involves these key steps:</p> <ol> <li>Transcription: Converting spoken audio to text</li> <li>Diarization: Identifying who spoke which parts</li> <li>Formatting: Optimizing subtitles for readability</li> <li>Validation: Checking, testing and validating the generated subtitles</li> </ol> <p>Audim handles all these steps with an easy-to-use interface, providing both simple defaults and advanced customization options.</p>"},{"location":"devblog/v0.0.5/#prerequisites","title":"Prerequisites","text":"<p>Before running this example, ensure you have:</p> <ol> <li>Installed Audim with its dependencies: see installation instructions</li> <li>An audio file to process (MP3, WAV, or other supported formats) with your podcast audio recording</li> <li>A HuggingFace token for speaker diarization (get one at huggingface.co/settings/tokens)</li> </ol> <p>Note: See HuggingFace Token Management for more details.</p>"},{"location":"devblog/v0.0.5/#basic-usage-high-level-api","title":"Basic Usage (High-level API)","text":"<p>The simplest way to generate subtitles from an audio file is:</p> <pre><code>from audim.aud2sub.transcribers.podcast import PodcastTranscriber\nfrom audim.aud2sub.core import SubtitleGenerator\n\n# Create a transcriber with default settings\ntranscriber = PodcastTranscriber(model_name=\"large-v2\")\n\n# Generate and export subtitles\ngenerator = SubtitleGenerator(transcriber)\ngenerator.generate_from_mp3(\"input/podcast.mp3\")\ngenerator.export_subtitle(\"output/podcast.srt\")\n</code></pre> <p>This basic approach works well for most of the cases, using these default settings: - WhisperX large-v2 model for transcription - Automatic language detection - 1-5 speakers for diarization - Speaker names displayed as \"[Speaker 1]\", \"[Speaker 2]\", etc. - Optimized line length for readability</p>"},{"location":"devblog/v0.0.5/#customizing-transcription-mid-level-api","title":"Customizing Transcription (Mid-level API)","text":"<p>For more custom use cases, you can configure the transcriber directly during initialization. You can also use the convenient setters for customizing various aspects of the transcription process:</p> <pre><code># Create a transcriber\ntranscriber = PodcastTranscriber(\n    hf_token=\"YOUR_HUGGINGFACE_TOKEN\",\n    model_name=\"large-v3\",              # Use newer/larger model for better accuracy\n    language=\"en\",                      # Force English language detection  \n    device=\"cuda\",                      # Use GPU for faster processing\n    compute_type=\"float16\",             # Use half-precision floating point for faster processing\n    batch_size=16,                      # Process audio in batches of 16 samples\n)\n\n# Customize speaker detection\ntranscriber.set_speakers(min_speakers=2, max_speakers=6)\n\n# Customize subtitle formatting\ntranscriber.set_speaker_names_display(True, pattern=\"[{speaker}]\")\ntranscriber.set_line_properties(max_length=70, min_split_length=50)\n\n# Enable GPU memory management for large files\ntranscriber.set_memory_management(clear_gpu_memory=True)\n</code></pre>"},{"location":"devblog/v0.0.5/#advanced-configuration-low-level-api","title":"Advanced Configuration (Low-level API)","text":"<p>For more advanced use cases, you can override the base transcriber class and implement your own transcriber:</p> <pre><code>from audim.aud2sub.transcribers.base import BaseTranscriber\nfrom audim.aud2sub.core import SubtitleGenerator\n\nclass MyTranscriber(BaseTranscriber):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Add any custom initialization logic here\n\n    def process_audio(self, audio_path: str) -&gt; None:\n        # Add any custom transcription pipeline and logic here\n\n    def export_subtitle(self, output_path: str) -&gt; None:\n        # Add any custom subtitle export format and logic here\n\ntranscriber = MyTranscriber()\ngenerator = SubtitleGenerator(transcriber)\ngenerator.generate_from_mp3(\"input/podcast.mp3\")\ngenerator.export_subtitle(\"output/podcast.srt\")\n</code></pre>"},{"location":"devblog/v0.0.5/#complete-subtitling-example","title":"Complete Subtitling Example","text":"<p>Here's a complete example demonstrating the entire process:</p> <pre><code>from audim.aud2sub.transcribers.podcast import PodcastTranscriber\nfrom audim.aud2sub.core import SubtitleGenerator\n\n# Create transcriber\nprint(\"Creating transcriber...\")\ntranscriber = PodcastTranscriber(\n    model_name=\"large-v2\",\n    hf_token=\"YOUR_HUGGINGFACE_TOKEN\"\n)\n\n# Set speaker detection and subtitle formatting parameters\ntranscriber.set_speakers(min_speakers=1, max_speakers=5)\ntranscriber.set_speaker_names_display(True, pattern=\"[{speaker}]\")\ntranscriber.set_line_properties(max_length=70, min_split_length=50)\n\n# Generate subtitles\nprint(\"Generating subtitles...\")\ngenerator = SubtitleGenerator(transcriber)\n\n# Process audio file\nprint(\"Processing audio...\")\ngenerator.generate_from_mp3(\"input/podcast.mp3\")\n\n# Export the final subtitle\nprint(\"Exporting subtitles...\")\ngenerator.export_subtitle(\"output/podcast.srt\")\nprint(\"Done! Check output/podcast.srt for results.\")\n</code></pre>"},{"location":"devblog/v0.0.5/#validating-and-enhancing-subtitles","title":"Validating and Enhancing Subtitles","text":"<p>After generating subtitles, Audim's <code>utils</code> module provides tools for validating and enhancing them:</p>"},{"location":"devblog/v0.0.5/#replacing-generic-speaker-labels","title":"Replacing Generic Speaker Labels","text":"<p>Generated subtitles use generic labels like \"[Speaker 1]\" and \"[Speaker 2]\". You can replace these with actual names:</p> <pre><code>from audim.utils.subtitle import Subtitle\n\nsubtitle = Subtitle()\n\n# Preview replacements before applying\nsubtitle.preview_replacement(\n    \"output/podcast.srt\",\n    speakers=[\"Host\", \"Guest\"],\n    pretty_print=True\n)\n\n# Apply replacements\nsubtitle.replace_speakers(\n    \"output/podcast.srt\",\n    speakers=[\"Host\", \"Guest\"],\n    in_place=True\n)\n</code></pre> <p>You can also use a dictionary for more complex mappings:</p> <pre><code>subtitle.replace_speakers(\n    \"output/podcast.srt\",\n    speakers={\n        1: \"Host\",              # Replace [Speaker 1]\n        3: \"Guest 1\",           # Replace [Speaker 3]\n        \"Speaker 2\": \"Guest 2\"  # Replace [Speaker 2]\n    },\n    in_place=True\n)\n</code></pre>"},{"location":"devblog/v0.0.5/#playing-audio-with-synchronized-subtitles","title":"Playing Audio with Synchronized Subtitles","text":"<p>The <code>playback</code> utility helps validate your subtitles by playing the audio with synchronized subtitles in the terminal:</p> <pre><code>from audim.utils.playback import Playback\n\n# Create a playback instance\nplayback = Playback()\n\n# Play audio with synchronized subtitles\nplayback.play_audio_with_srt(\"input/podcast.mp3\", \"output/podcast.srt\")\n</code></pre> <p>This is a script for checking if subtitles are correctly aligned with the audio and if speaker identification is accurate.</p> <p>Here's a sample output:</p>"},{"location":"devblog/v0.0.5/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>This complete example demonstrates the entire workflow from audio to validated subtitles:</p> <pre><code>from audim.aud2sub.transcribers.podcast import PodcastTranscriber\nfrom audim.aud2sub.core import SubtitleGenerator\nfrom audim.utils.subtitle import Subtitle\nfrom audim.utils.playback import Playback\n\n# 1. Generate subtitles\ntranscriber = PodcastTranscriber(hf_token=\"YOUR_HUGGINGFACE_TOKEN\")\ngenerator = SubtitleGenerator(transcriber)\ngenerator.generate_from_mp3(\"input/podcast.mp3\")\ngenerator.export_subtitle(\"output/podcast.srt\")\n\n# 2. Replace generic speaker labels with actual names\nsubtitle = Subtitle()\nsubtitle.replace_speakers(\n    \"output/podcast.srt\",\n    speakers=[\"Grant Sanderson\", \"Sal Khan\"],\n    in_place=True\n)\n\n# 3. Play audio with synchronized subtitles for validation\nplayback = Playback()\nplayback.play_audio_with_srt(\"input/podcast.mp3\", \"output/podcast.srt\")\n</code></pre>"},{"location":"devblog/v0.0.5/#input-and-output","title":"Input and Output","text":"<p>Here's the sample input and output on running the complete workflow example:</p> <p>Input audio file:</p>      Your browser does not support the audio element.    <p>Audio snippet from \"Sal Khan: Beyond Khan Academy | 3b1b Podcast #2\"</p> <p>Output subtitle file:</p>"},{"location":"devblog/v0.0.5/#advanced-memory-management","title":"Advanced Memory Management","text":"<p>When transcribing long audio files or using large models, you may encounter GPU memory limitations. Audim includes memory management features to help:</p> <pre><code>transcriber = PodcastTranscriber(\n    model_name=\"large-v3\",\n    hf_token=\"YOUR_HUGGINGFACE_TOKEN\",\n    clear_gpu_memory=True  # Enable automatic GPU memory cleanup\n)\n</code></pre> <p>With memory management enabled, Audim will: - Clear GPU memory after transcription - Clear GPU memory after alignment - Clear GPU memory after diarization - Perform a final cleanup at the end</p>"},{"location":"devblog/v0.0.5/#huggingface-token-management","title":"HuggingFace Token Management","text":"<p>Audim needs a HuggingFace token to access the diarization model. You can get one from huggingface.co/settings/tokens.</p> <p>Audim supports multiple ways to provide your HuggingFace token:</p> <ol> <li> <p>Directly passing the token as a parameter when creating a transcriber:</p> <pre><code>from audim.aud2sub.transcribers.podcast import PodcastTranscriber\n\ntranscriber = PodcastTranscriber(hf_token=\"your_huggingface_token\")\n</code></pre> </li> <li> <p>(Recommended) Set the <code>HF_TOKEN</code> environment variable before running your script:</p> <pre><code>export HF_TOKEN=\"your_huggingface_token\"\n</code></pre> </li> <li> <p>Login once using the HuggingFace CLI tool (you need to have <code>huggingface_hub</code> installed):</p> <p>Install:</p> <pre><code>&gt;&gt;&gt; pip install -U \"huggingface_hub[cli]\"\n</code></pre> <p>Login:</p> <pre><code>huggingface-cli login\n# Enter your token when prompted\n</code></pre> </li> </ol>"},{"location":"devblog/v0.0.5/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ul> <li>Memory Errors: Enable <code>clear_gpu_memory=True</code> or use a smaller model</li> <li>Missing Speakers: Check that you provided a valid HuggingFace token </li> <li>Poor Transcription: Try a larger model like \"large-v3\" or specify the language</li> <li>Speaker Confusion: Adjust min_speakers and max_speakers to match your podcast</li> </ul>"},{"location":"devblog/v0.0.5/#whats-next","title":"What's Next?","text":"<p>After generating and validating subtitles with <code>aud2sub</code> and <code>utils</code>, you can:</p> <ol> <li>Use the subtitles in <code>sub2pod</code> to generate podcast videos (see Podcast Video Generation)</li> <li>Add advanced effects to your videos (see Advanced Video Effects)</li> <li>Build a complete podcast production pipeline with Audim</li> </ol>"},{"location":"devblog/v0.0.5/#see-also","title":"See Also","text":"<ul> <li>API Documentation for aud2sub</li> <li>API Documentation for playback</li> <li>API Documentation for subtitle</li> <li>Basic Podcast Example</li> <li>Advanced Video Effects</li> </ul>"},{"location":"devblog/v0.0.6/","title":"Audio Extraction Utility: From Video to Audio","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: May 15, 2025</li> <li>Last Updated: May 15, 2025</li> <li>Compatible with: Audim v0.0.6</li> </ul> <p>This guide introduces Audim's new <code>Extract</code> utility for seamlessly extracting high-quality audio from video files. This simple yet powerful feature streamlines the podcast production workflow by eliminating the need for external tools, and can be used in various other use cases and scenarios in the podcast production workflow.</p>"},{"location":"devblog/v0.0.6/#overview","title":"Overview","text":"<p>The <code>Extract</code> utility provides a clean interface for:</p> <ol> <li>Video-to-Audio Conversion: Extract audio from podcast videos</li> <li>Format Flexibility: Support for WAV, MP3, FLAC, and other formats</li> <li>Quality Control: Configurable bitrate and sample rate</li> </ol>"},{"location":"devblog/v0.0.6/#basic-usage","title":"Basic Usage","text":"<p>Extracting audio from a video file is straightforward:</p> <pre><code>from audim.utils.extract import Extract\n\n# Set input and output file paths\ninput_file = \"./input/podcast.mp4\"\noutput_file = \"./output/podcast.wav\"\noutput_format = \"wav\"\n\n# Extract audio from video\nextractor = Extract()\nextractor.extract_audio(input_file, output_file, output_format)\n</code></pre>"},{"location":"devblog/v0.0.6/#advanced-options","title":"Advanced Options","text":"<p>For more control over the extraction process:</p> <pre><code>from audim.utils.extract import Extract\n\nextractor = Extract()\nextractor.extract_audio(\n    input_path=\"./input/podcast.mp4\",\n    output_path=\"./output/podcast.mp3\",\n    output_format=\"mp3\",\n    bitrate=\"320k\",\n    sample_rate=48000\n)\n</code></pre>"},{"location":"devblog/v0.0.6/#integration-with-audim-workflow","title":"Integration with Audim Workflow","text":"<p>This utility complements the existing Audim modules:</p> <ol> <li>Extract audio from your recorded video using <code>Extract.extract_audio()</code></li> <li>Generate subtitles with <code>aud2sub</code> from the extracted audio (see Podcast Subtitling)</li> <li>Produce podcasts with <code>sub2pod</code> from the extracted audio and generated subtitles (see Podcast Videos)</li> </ol>"},{"location":"devblog/v0.0.6/#see-also","title":"See Also","text":"<ul> <li>API Documentation for Extract</li> <li>Automated Podcast Subtitling</li> <li>Basic Podcast Example</li> </ul>"},{"location":"devblog/v0.0.7/","title":"Audim First Release","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: May 18, 2025</li> <li>Last Updated: May 18, 2025</li> <li>Compatible with: Audim v0.0.7</li> </ul> <p>Warning</p> <p>This blog is still a work in progress.</p> <p>Info</p> <p>This is the first release of Audim.</p> <p>Putting everything together, all the modules, small changes and everything we have developed so far into one place, let's take a look how Audim generates the podcast video.</p>"},{"location":"devblog/v0.0.7/#overview","title":"Overview","text":"<p>For this example, we'll transform a conversation between Grant Sanderson (from 3Blue1Brown) and Sal Khan (from Khan Academy) into a visually engaging podcast video. We'll walk through:</p> <ol> <li>Setup and Installation</li> <li>Preparing the input files</li> <li>Extracting the audio from the video</li> <li>Generating a transcript from the audio</li> <li>Setting up the podcast layout</li> <li>Generating the final output video with Audim</li> </ol>"},{"location":"devblog/v0.0.7/#step-00-setup","title":"Step 00: Setup","text":"<ul> <li>we have setup the project and installed the dependencies.</li> <li>see docs/setup/installation.md for more details on how to setup the project and install the dependencies.</li> <li>for demo purposes, we have decided to use Sal Khan: Beyond Khan Academy | 3b1b Podcast #2 as the input video. <p>Note: you will have your own recordings when you use audim for your own podcast video generation.</p> </li> </ul>"},{"location":"devblog/v0.0.7/#step-01-prepare-the-input-files","title":"Step 01: Prepare the input files","text":"<ul> <li>we have downloaded this video podcast from YouTube for demo purposes. <p>Note: you will have your own recordings when you use audim for your own podcast video generation.</p> </li> <li>since the video is too long for just a demo, we will only use the 19:39 - \"The next decades of education\" section of the video.</li> <li>other than the video, we need a podcast brand logo, and profile images for the speakers. I have used the following images from google:<ul> <li>3b1b Logo</li> <li>Grant Sanderson</li> <li>Sal Khan</li> </ul> </li> </ul>"},{"location":"devblog/v0.0.7/#step-02-extract-the-audio-from-the-video","title":"Step 02: Extract the audio from the video","text":"<ul> <li>we have extracted the audio from the video using Audim's <code>extract</code> module.</li> <li>see docs/audim/utils/extract.md API docs for more details.</li> <li>see blog v0.0.6 for more details on how to extract the audio from a video file.</li> </ul> <p>Note: Incase you had an audio recording instead of a video, you could have skipped step 02 and used the audio file directly in step 03.</p> <p>Here's the audio file we have extracted:</p>      Your browser does not support the audio element.    <p>extracted audio snippet from the downloaded youtube video</p>"},{"location":"devblog/v0.0.7/#step-03-generate-a-transcript-from-the-audio","title":"Step 03: Generate a transcript from the audio","text":"<ul> <li>we have generated a transcript from the audio using Audim's <code>aud2sub</code> module.</li> <li>see Podcast Transcriber API docs for more details.</li> <li>see blog v0.0.5 for more details on how to generate a transcript from an audio file.</li> </ul> <p>Note: Incase you had a transcript instead of an audio file, you could have skipped step 03 and used the transcript directly in step 04.</p> <p>Here's the transcript we have generated:</p> <p>transcript generated from the audio snippet</p>"},{"location":"devblog/v0.0.7/#step-04-set-up-the-podcast-layout","title":"Step 04: Set up the podcast layout","text":"<ul> <li>we have set up the podcast layout using Audim's <code>sub2pod</code> module.</li> <li>see Podcast Layout API docs for more details.</li> <li>see blog v0.0.2 for more details on how to set up the podcast layout.</li> <li>also, see blog v0.0.3 for the design philosophy behind the podcast layout, and some more variations on the podcast layout.</li> </ul> <p>Here is the final layout and generation code (mostly using the default settings):</p> <pre><code>from datetime import datetime\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout()\n\n# Add speakers and layout tweaks\nprint(\"Adding speakers...\")\nlayout.add_speaker(\"Grant Sanderson\", \"input/grant.png\")\nlayout.add_speaker(\"Sal Khan\", \"input/sal.png\")\nlayout.set_content_offset(200)\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=\"input/podcast.srt\",\n    audio_path=\"input/podcast.mp3\",\n    logo_path=\"input/logo.png\",\n    title=\"3b1b Podcast: Sal Khan: Beyond Khan Academy\",\n    cpu_core_utilization=\"max\"\n)\n\n# Export the final video\nprint(\"Exporting video...\")\ndatetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\ngenerator.export_video(f\"output/podcast_{datetime}.mp4\")\n</code></pre>"},{"location":"devblog/v0.0.7/#step-05-generate-the-video-and-export-final-output","title":"Step 05: Generate the video and export final output","text":"<ul> <li>we have generated the video using Audim's <code>sub2pod</code> module.</li> <li>see VideoGenerator API docs for more details.</li> <li>see blog v0.0.2 for more details on how to generate a video from a transcript.</li> </ul> <p>Here's the final output video we have generated:</p>      Your browser does not support the video element.    <p>final podcast video generated from the input content</p>"},{"location":"setup/development/","title":"Development","text":""},{"location":"setup/development/#setup","title":"Setup","text":""},{"location":"setup/development/#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone https://github.com/mratanusarkar/audim.git\ncd audim\n</code></pre>"},{"location":"setup/development/#2-install-ffmpeg-locally-recommended","title":"2. Install FFmpeg locally (recommended)","text":"<p>Using local FFmpeg is optional but recommended for speeding up the video encoding process.</p> <p>Install FFmpeg</p> UbuntuMacOSWindowsWindows (manual)Other platforms <pre><code>sudo apt install ffmpeg libx264-dev\n</code></pre> <pre><code>brew install ffmpeg\n</code></pre> <pre><code>choco install ffmpeg\n</code></pre> <p>Download and install FFmpeg from the official website:</p> <ul> <li>Download FFmpeg</li> <li>Ensure FFmpeg is in your system PATH</li> </ul> <p>Download and install FFmpeg from the official website:</p> <ul> <li>Download FFmpeg</li> <li>Follow the installation instructions for your platform</li> </ul>"},{"location":"setup/development/#3-setting-up-the-project-environment","title":"3. Setting up the project environment","text":"<p>We recommend using <code>uv</code> to manage your project environment since <code>audim</code> was developed using <code>uv</code>, and you can replicate the same environment by just running:</p> <pre><code>uv sync\n</code></pre> <p>But, feel free to use any python based environment and package manager of your choice.</p> <p>About uv</p> <p>uv is a fast, simple, and secure Python package manager. It is recommended to use <code>uv</code> to manage your project environment.</p>"},{"location":"setup/development/#31-installing-uv","title":"3.1 Installing <code>uv</code>","text":"<p>Note</p> <p>If you are using conda base environment as the default base environment for your python projects, run the below command to activate the base environment.</p> <p>If not, skip this step and continue with the next step.</p> <pre><code>conda activate base\n</code></pre> <p>If you don't have <code>uv</code> installed, you can install it by running:</p> LinuxWindows <pre><code># Install uv\npip install uv\n\n# Setup project environment\nuv venv\n\nsource .venv/bin/activate\n\nuv pip install -e \".[dev,docs]\"\n</code></pre> <pre><code># Install uv\npip install uv\n\n# Setup project environment\nuv venv\n\n.venv\\Scripts\\activate\n\nuv pip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"setup/development/#4-create-input-and-output-directories","title":"4. Create input and output directories","text":"<pre><code>mkdir ./input ./output\n</code></pre> <p>Create a <code>test.py</code> or <code>run.py</code> to test your python script using <code>audim</code>.</p> <pre><code>touch test.py\n</code></pre> <p>ideally, if done correctly, the setup should be like this:</p> <pre><code>audim/\n\u251c\u2500\u2500 audim/\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 input/\n\u251c\u2500\u2500 output/\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 test.py # or run.py\n</code></pre> <p>Note</p> <ul> <li>You would dump your input files in the <code>input</code> directory.</li> <li>The output files will be dumped in the <code>output</code> directory.</li> <li>See Usage for more details or see sample scripts to get some inspiration.</li> </ul>"},{"location":"setup/development/#code-quality","title":"Code Quality","text":"<p>Before committing, please ensure that the code is formatted and styled correctly. Run the following commands to check and fix code style issues:</p> <pre><code># Check and fix code style issues\nruff format .\nruff check --fix .\n</code></pre>"},{"location":"setup/development/#run-the-project","title":"Run the project","text":"<p>feel free to create a <code>run.py</code> or <code>test.py</code> file to test the project. They will be untracked by git.</p> <p>implement your usage logic in the <code>run.py</code> file.</p> <p>Run the script with:</p> <pre><code>python run.py\n</code></pre>"},{"location":"setup/development/#build-and-serve-the-documentation","title":"Build and serve the documentation","text":"<p>You can build and serve the documentation by running:</p> <pre><code>uv pip install -e .[docs]\nmkdocs serve\n</code></pre>"},{"location":"setup/installation/","title":"Installation","text":"<p>Note</p> <p>This guide is for end users using Audim for creating podcast videos. For developers and contributors, see Development.</p>"},{"location":"setup/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>\ud83d\udc0d Python \u2265 3.10</li> <li>\ud83d\udda5\ufe0f Conda or venv</li> <li>\ud83c\udfa5 FFmpeg (recommended, for faster video encoding)</li> </ul>"},{"location":"setup/installation/#setup","title":"Setup","text":""},{"location":"setup/installation/#1-install-audim","title":"1. Install Audim","text":"<p>It is recommended to install <code>audim</code> in a virtual environment from PyPI or Conda in a Python=3.10 environment.</p> Install using PyPIInstall using CondaInstall from source <p>Activate your virtual environment (recommended):</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>Install <code>audim</code> using <code>pip</code>:</p> <pre><code>pip install audim\n</code></pre> <p>Create a new environment using Conda:</p> <pre><code>conda create -n my-project python=3.10\n</code></pre> <p>Activate your virtual environment:</p> <pre><code>conda activate my-project\n</code></pre> <p>Install <code>pip</code> and <code>audim</code> using <code>conda</code>:</p> <pre><code>conda install pip\npip install audim\n</code></pre> <p>Note</p> <p>By installing <code>audim</code> from source, you can explore the latest features and enhancements that have not yet been officially released.</p> <p>Note</p> <p>Please note that the latest changes may be still in development and may not be stable and may contain bugs.</p> <p>Install from source</p> <pre><code>pip install git+https://github.com/mratanusarkar/audim.git\n</code></pre>"},{"location":"setup/installation/#2-install-ffmpeg-locally-recommended","title":"2. Install FFmpeg locally (recommended)","text":"<p>Using local FFmpeg is optional but recommended for speeding up the video encoding process.</p> <p>Install FFmpeg</p> UbuntuMacOSWindowsWindows (manual)Other platforms <pre><code>sudo apt install ffmpeg libx264-dev\n</code></pre> <pre><code>brew install ffmpeg\n</code></pre> <pre><code>choco install ffmpeg\n</code></pre> <p>Download and install FFmpeg from the official website:</p> <ul> <li>Download FFmpeg</li> <li>Ensure FFmpeg is in your system PATH</li> </ul> <p>Download and install FFmpeg from the official website:</p> <ul> <li>Download FFmpeg</li> <li>Follow the installation instructions for your platform</li> </ul>"},{"location":"setup/installation/#start-your-project","title":"Start your project","text":"<p>Create input and output directories</p> <pre><code>mkdir ./input ./output\n</code></pre> <p>Create a <code>run.py</code> or <code>test.py</code> to write your python script using <code>audim</code> and start creating your podcast videos.</p> <pre><code>touch run.py\n</code></pre> <p>Ideally, if done correctly, the setup should look like this:</p> <pre><code>your-project/\n\u251c\u2500\u2500 .venv/\n\u251c\u2500\u2500 input/\n\u251c\u2500\u2500 output/\n\u2514\u2500\u2500 run.py # or test.py\n</code></pre> <p>Now, you are all set!</p> <ul> <li>Go ahead and create your python script in <code>run.py</code> to start creating your podcast videos.</li> <li>See Usage for more details or see sample scripts to get some inspiration.</li> <li>Once done, you would dump your input files in the <code>input</code> directory.</li> <li>The output files will be generated in the <code>output</code> directory on running the script.</li> </ul> <p>Run the script using:</p> <pre><code>python run.py\n</code></pre> <p>Feel free to share your generations and tag us!</p>"},{"location":"usage/","title":"Usage","text":"<p>Here are some example usage covering various use cases, using the <code>audim</code> library.</p> <p>Have a quick pick for your script based on your requirements:</p> From To Use Script Comment Audio Subs generate script_01 audio transcription with speaker diarization Audio + Subs Podcast generate script_02 generate podcast from audio and original subtitle Audio Podcast generate script_03 \ud83d\udea7, generate podcast from audio recording Video Audio extract script_04 extract audio from video Video Subs generate script_05 transcribe and generate subtitles from video Video + Subs Podcast generate script_06 generate podcast from video and original subtitle Video Podcast generate script_07 \ud83d\udea7, generate podcast from video recording Audio - playback script_08 play audio with subtitles in sync, in terminal Video - playback NYI NYI Subs Subs modify script_10 replace speaker names in subtitle file <p>Key</p> <ul> <li>\ud83d\udea7: work in progress</li> <li>NYI: not yet implemented (not supported by <code>audim</code> yet)</li> <li>Audio: any audio file (.mp3, .m4a, .wav, etc.)</li> <li>Video: any video file (.mp4, .mkv, .avi, etc.)</li> <li>Subs: subtitle file (.srt)</li> <li>Podcast: podcast generated by <code>audim</code></li> </ul>"},{"location":"usage/script_01/","title":"Audio to Subtitle Generation","text":""},{"location":"usage/script_01/#use-case","title":"Use Case","text":"<p>You have an audio file and you want to generate subtitles and transcriptions with speaker diarization.</p>"},{"location":"usage/script_01/#script","title":"Script","text":"<pre><code>from datetime import datetime\nfrom audim.aud2sub.transcribers.podcast import PodcastTranscriber\nfrom audim.aud2sub.core import SubtitleGenerator\n\n# Set input and output files\ntimestamp = datetime.now().strftime('%Y%m%d%H%M%S')\ninput_audio_file = \"input/podcast.mp3\"\noutput_subtitle_file = f\"output/podcast_{timestamp}.srt\"\n\n# Create transcriber object\nprint(\"Creating transcriber...\")\ntranscriber = PodcastTranscriber(model_name=\"large-v2\")\n\n# Set speaker detection and subtitle formatting parameters\ntranscriber.set_speakers(min_speakers=1, max_speakers=5)\ntranscriber.set_speaker_names_display(True, pattern=\"[{speaker}]\")\ntranscriber.set_line_properties(max_length=70, min_split_length=50)\n\n# Create subtitle generator object\nprint(\"Generating subtitles...\")\ngenerator = SubtitleGenerator(transcriber)\n\n# Process audio file\nprint(\"Processing audio...\")\ngenerator.generate_from_mp3(input_audio_file)\n\n# Export the final subtitle\nprint(\"Exporting subtitles...\")\ngenerator.export_subtitle(output_subtitle_file)\nprint(f\"Done! Check {output_subtitle_file} for results.\")\n</code></pre>"},{"location":"usage/script_02/","title":"Audio + Subtitle to Podcast Generation","text":""},{"location":"usage/script_02/#use-case","title":"Use Case","text":"<p>You have an audio file or a recording and an original subtitle file and you want to generate a podcast video.</p> <p>Note</p> <p>Use this script when you have the original subtitle file and you don't want to use <code>audim</code> to generate the subtitle file.</p> <p>Assumption</p> <p>This script assumes that the original subtitle file is in the same language as the audio file. If the original subtitle file is in a different language, you need to translate it to the same language as the audio file. You can use a tool like OpenAI's Whisper to translate the subtitle file.</p> <p>Caution</p> <p>The subtitle file must be as per the format expected by <code>audim</code>, which adds speaker identification.</p> <p>Example of expected subtitle file format:</p> <pre><code>1\n00:00:00,000 --&gt; 00:00:04,500\n[Host] Welcome to our podcast!\n\n2\n00:00:04,600 --&gt; 00:00:08,200\n[Guest] Thank you! Glad to be here.\n</code></pre> <p>see sub2pod/core for more details.</p>"},{"location":"usage/script_02/#script","title":"Script","text":"<pre><code>from datetime import datetime\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Set input and output files\ntimestamp = datetime.now().strftime('%Y%m%d%H%M%S')\ninput_audio_file = \"input/podcast.mp3\"\ninput_subtitle_file = \"input/podcast.srt\"\noutput_video_file = f\"output/podcast_{timestamp}.mp4\"\nspeakers = [\n    {\n        \"name\": \"Grant Sanderson\",\n        \"dp\": \"input/grant.png\"\n    },\n    {\n        \"name\": \"Sal Khan\",\n        \"dp\": \"input/sal.png\"\n    }\n]\npodcast_logo = \"input/logo.png\"\npodcast_title = \"3b1b Podcast: Sal Khan: Beyond Khan Academy\"\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Set custom effects for the layout\nlayout.set_transition_effect(\"fade\", frames=20)\nlayout.set_highlight_effect(\"none\")\n\n# Add speakers\nprint(\"Adding speakers...\")\nfor speaker in speakers:\n    layout.add_speaker(speaker[\"name\"], speaker[\"dp\"])\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=input_subtitle_file,\n    audio_path=input_audio_file,\n    logo_path=podcast_logo,\n    title=podcast_title,\n    cpu_core_utilization=\"max\"\n)\n\n# Export the final video\nprint(\"Exporting video...\")\ngenerator.export_video(output_video_file)\nprint(f\"Done! Check {output_video_file} for results.\")\n</code></pre>"},{"location":"usage/script_03/","title":"Audio to Podcast Generation","text":""},{"location":"usage/script_03/#use-case","title":"Use Case","text":"<p>You have an audio file or a recording and you want to generate a podcast video from just the recording using <code>audim</code>.</p>"},{"location":"usage/script_03/#script","title":"Script","text":"<p>\ud83d\udea7</p> <p>We don't have a single standalone script for this, and it won't be possible until we have an automatic speaker identification from <code>audim</code>.</p> <p>You can use the following series of actions to achieve this:</p> <ol> <li>Use script 01 to generate a subtitle file</li> <li>Manually identify the speakers with help from script 08</li> <li>Replace the speaker names with the names you identified using script 10</li> <li>Use script 02 to generate a podcast video</li> </ol>"},{"location":"usage/script_04/","title":"Video to Audio Extraction","text":""},{"location":"usage/script_04/#use-case","title":"Use Case","text":"<p>You have a video file and you want to extract the audio from it.</p>"},{"location":"usage/script_04/#script","title":"Script","text":"<pre><code>from audim.utils.extract import Extract\n\n\n# Set input and output file paths\ninput_file = \"input/podcast.mp4\"\noutput_file = \"output/podcast.wav\"\noutput_format = \"wav\"\n\n# Extract audio from video\nextractor = Extract()\nextractor.extract_audio(input_file, output_file, output_format)\n</code></pre>"},{"location":"usage/script_05/","title":"Video to Subtitle Generation","text":""},{"location":"usage/script_05/#use-case","title":"Use Case","text":"<p>You have a video file and you want to generate a subtitle file from it.</p>"},{"location":"usage/script_05/#script","title":"Script","text":"<pre><code>import os\nfrom audim.utils.extract import Extract\nfrom audim.aud2sub.transcribers.podcast import PodcastTranscriber\nfrom audim.aud2sub.core import SubtitleGenerator\n\n# Set input and output file paths\ninput_file = \"input/podcast.mp4\"\noutput_subtitle_file = f\"output/podcast.srt\"\n\n# Extract audio from video\ntemp_audio_file = \"temp/podcast.wav\"\ntemp_audio_format = \"wav\"\nextractor = Extract()\nextractor.extract_audio(input_file, temp_audio_file, temp_audio_format)\n\n# Create transcriber object\nprint(\"Creating transcriber...\")\ntranscriber = PodcastTranscriber(model_name=\"large-v2\")\n\n# Set speaker detection and subtitle formatting parameters\ntranscriber.set_speakers(min_speakers=1, max_speakers=10)\ntranscriber.set_speaker_names_display(True, pattern=\"[{speaker}]\")\ntranscriber.set_line_properties(max_length=70, min_split_length=50)\n\n# Create subtitle generator object\nprint(\"Generating subtitles...\")\ngenerator = SubtitleGenerator(transcriber)\n\n# Process audio file\nprint(\"Processing extracted audio from video...\")\ngenerator.generate_from_mp3(temp_audio_file)\n\n# Delete the intermediate extracted audio file\nos.remove(temp_audio_file)\n\n# Export the final subtitle\nprint(\"Exporting subtitles...\")\ngenerator.export_subtitle(output_subtitle_file)\nprint(f\"Done! Check {output_subtitle_file} for results.\")\n</code></pre>"},{"location":"usage/script_06/","title":"Video + Subtitle to Podcast Generation","text":""},{"location":"usage/script_06/#use-case","title":"Use Case","text":"<p>You have a video file and an original subtitle file and you want to generate a podcast video using <code>audim</code>.</p> <p>Note</p> <p>Use this script when you have the original subtitle file and you don't want to use <code>audim</code> to generate the subtitle file.</p> <p>Assumption</p> <p>This script assumes that the original subtitle file is in the same language as the video file. If the original subtitle file is in a different language, you need to translate it to the same language as the video file. You can use a tool like OpenAI's Whisper to translate the subtitle file.</p> <p>Caution</p> <p>The subtitle file must be as per the format expected by <code>audim</code>, which adds speaker identification.</p> <p>Example of expected subtitle file format:</p> <pre><code>1\n00:00:00,000 --&gt; 00:00:04,500\n[Host] Welcome to our podcast!\n\n2\n00:00:04,600 --&gt; 00:00:08,200\n[Guest] Thank you! Glad to be here.\n</code></pre> <p>see sub2pod/core for more details.</p>"},{"location":"usage/script_06/#script","title":"Script","text":"<pre><code>import os\nfrom datetime import datetime\nfrom audim.utils.extract import Extract\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Set input and output files\ntimestamp = datetime.now().strftime('%Y%m%d%H%M%S')\ninput_video_file = \"input/podcast.mp4\"\ninput_subtitle_file = \"input/podcast.srt\"\noutput_video_file = f\"output/podcast_{timestamp}.mp4\"\nspeakers = [\n    {\n        \"name\": \"Grant Sanderson\",\n        \"dp\": \"input/grant.png\"\n    },\n    {\n        \"name\": \"Sal Khan\",\n        \"dp\": \"input/sal.png\"\n    }\n]\npodcast_logo = \"input/logo.png\"\npodcast_title = \"3b1b Podcast: Sal Khan: Beyond Khan Academy\"\n\n# Extract audio from video\ntemp_audio_file = \"temp/podcast.wav\"\ntemp_audio_format = \"wav\"\nextractor = Extract()\nextractor.extract_audio(input_video_file, temp_audio_file, temp_audio_format)\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Set custom effects for the layout\nlayout.set_transition_effect(\"fade\", frames=20)\nlayout.set_highlight_effect(\"none\")\n\n# Add speakers\nprint(\"Adding speakers...\")\nfor speaker in speakers:\n    layout.add_speaker(speaker[\"name\"], speaker[\"dp\"])\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=input_subtitle_file,\n    audio_path=temp_audio_file,\n    logo_path=podcast_logo,\n    title=podcast_title,\n    cpu_core_utilization=\"max\"\n)\n\n# Delete the intermediate extracted audio file\nos.remove(temp_audio_file)\n\n# Export the final video\nprint(\"Exporting video...\")\ngenerator.export_video(output_video_file)\nprint(f\"Done! Check {output_video_file} for results.\")\n</code></pre>"},{"location":"usage/script_07/","title":"Video to Podcast Generation","text":""},{"location":"usage/script_07/#use-case","title":"Use Case","text":"<p>You have a video recording and you want to generate a podcast video from just the video using <code>audim</code>.</p>"},{"location":"usage/script_07/#script","title":"Script","text":"<p>\ud83d\udea7</p> <p>We don't have a single standalone script for this, and it won't be possible until we have an automatic speaker identification from <code>audim</code>.</p> <p>You can use the following series of actions to achieve this:</p> <ol> <li>Use script 04 to extract the audio from the video</li> <li>Use script 01 to generate a subtitle file</li> <li>Manually identify the speakers with help from script 08</li> <li>Replace the speaker names with the names you identified using script 10</li> <li>Use script 02 to generate a podcast video</li> </ol>"},{"location":"usage/script_08/","title":"Audio Playback with Subtitles","text":""},{"location":"usage/script_08/#use-case","title":"Use Case","text":"<p>You have an audio file and a subtitle file and you want to play the audio with the subtitles.</p> <p>This might come in handy when:</p> <ul> <li>you have generated a subtitle with <code>audim</code> and wish to validate and preview it in sync with the audio.</li> <li>you have a subtitle file and original audio file, and wish to listen to it in sync to figure out the speaker names.</li> <li>idk, just like to listen to the audio with the subtitles the terminal cauz it's cool!</li> </ul>"},{"location":"usage/script_08/#script","title":"Script","text":"<pre><code>from audim.utils.playback import Playback\n\n\n# Create a playback instance\nplayback = Playback()\n\n# Play an audio file with synchronized subtitles\nplayback.play_audio_with_srt(\"input/podcast.mp3\", \"output/podcast.srt\")\n</code></pre> <p>Tips</p> <p>You might want to generate subtitles with <code>audim</code> first, and then use this script to play the audio with the subtitles.</p> <p>see: script 01 to generate subtitles.</p> <p>Tips</p> <p>You might want to replace the speaker names with the actual names, and then use this script again to play the audio with the subtitles.</p> <p>see: script 10 to replace the speaker names.</p>"},{"location":"usage/script_10/","title":"Replace Speaker Names in Subtitle","text":""},{"location":"usage/script_10/#use-case","title":"Use Case","text":"<p>You have generated a subtitle file with <code>audim</code> or you have a original subtitle file (with manual speaker diarization). Now, you want to replace the speaker names with the actual names or you wish to add speaker names to the subtitle.</p> <p>Disclaimer</p> <p><code>Audim &gt; Aud2Sub</code> can perform speaker diarization, but it generates a subtitle file with speaker placeholders like <code>[Speaker 1]</code>, <code>[Speaker 2]</code>, etc.</p> <p>Currently, you have to manually identify the speakers and replace the placeholders with the actual names.</p> <p>\ud83d\udea7 We will soon have a <code>util</code> tool to aid you with in this manual process.</p> <p>It is theoritically impossible to identify the speaker names, as it could be anyone's voice and a code has no way to determine that.</p>"},{"location":"usage/script_10/#script","title":"Script","text":"<pre><code>from audim.utils.subtitle import Subtitle\n\nsubtitle = Subtitle()\n\n# Replace speaker placeholders with actual names\nsubtitle.preview_replacement(\n    \"output/podcast.srt\",\n    speakers=[\"Grant Sanderson\", \"Sal Khan\"],\n    pretty_print=True\n)\n\nsubtitle.replace_speakers(\n    \"output/podcast.srt\",\n    speakers=[\"Grant Sanderson\", \"Sal Khan\"],\n    in_place=True\n)\n</code></pre> <p>Tips</p> <p>You might want to generate subtitles with <code>audim</code> first, and then use this script to replace the speaker names.</p> <p>see: script 01 to generate subtitles.</p> <p>Tips</p> <p>You might want to verify the new subtitle file after replacement, Or, you might want some playback help to identify the speakers first.</p> <p>see: script 08 to play the audio with the subtitles.</p>"}]}