{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Audim","text":"<p>Audio Podcast Animation Engine</p> <p>An animation and video rendering engine for audio-based and voice-based podcast videos.</p> <p>Audim is an engine for precise programmatic animation and rendering of podcast videos from audio-based and voice-based file recordings.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Precise programmatic animations.</li> <li>Rendering of videos.</li> <li>Layout based scenes.</li> <li>Support for audio to subtitle generation.</li> <li>Support for video to subtitle and scene elements generation.</li> <li>Support for subtitle and scene elements to video generation.</li> </ul>"},{"location":"#example-usage","title":"Example Usage","text":"<ul> <li>See \"Examples\" section in the documentation for usage examples.</li> <li>Ensure you have setup correctly before usage. See \"Setup\" section in the documentation for more details.</li> </ul>"},{"location":"audim/sub2pod/core/","title":"Core (Video Generation Module)","text":"<p>The core module is the main Video Generation Engine. It is responsible for the overall structure and flow of the podcast video.</p> <p>It uses a layout object to define the visual arrangement of the video, which internally uses a collection of elements and effects to define the components of each frame in the video and their animations and transitions.</p> <p>Below is the API documentation for the core module:</p>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator","title":"<code>VideoGenerator</code>","text":"<p>Core engine for generating videos from SRT files</p> <p>This class is responsible for generating video frames from an SRT or subtitle file. The subtitle file must follow our extended SRT format, which adds speaker identification:</p> <ul> <li>Standard SRT format with sequential numbering, timestamps, and text content</li> <li>Speaker identification in square brackets at the beginning of each subtitle text   Example: \"[Host] Welcome to our podcast!\"</li> </ul> <p>Example of expected SRT format: <pre><code>1\n00:00:00,000 --&gt; 00:00:04,500\n[Host] Welcome to our podcast!\n\n2\n00:00:04,600 --&gt; 00:00:08,200\n[Guest] Thank you! Glad to be here.\n</code></pre></p> <p>The speaker tag is used to visually distinguish different speakers in the generated video, and is mandatory for the core engine to work.</p> <p>It uses a layout object to define the visual arrangement of the video.</p> Source code in <code>audim/sub2pod/core.py</code> <pre><code>class VideoGenerator:\n    \"\"\"\n    Core engine for generating videos from SRT files\n\n    This class is responsible for generating video frames from an SRT or subtitle file.\n    The subtitle file must follow our extended SRT format,\n    which adds speaker identification:\n\n    - Standard SRT format with sequential numbering, timestamps, and text content\n    - Speaker identification in square brackets at the beginning of each subtitle text\n      Example: \"[Host] Welcome to our podcast!\"\n\n    Example of expected SRT format:\n    ```srt\n    1\n    00:00:00,000 --&gt; 00:00:04,500\n    [Host] Welcome to our podcast!\n\n    2\n    00:00:04,600 --&gt; 00:00:08,200\n    [Guest] Thank you! Glad to be here.\n    ```\n\n    The speaker tag is used to visually distinguish different speakers in the\n    generated video, and is mandatory for the core engine to work.\n\n    It uses a layout object to define the visual arrangement of the video.\n    \"\"\"\n\n    def __init__(self, layout, fps=30, batch_size=300):\n        \"\"\"\n        Initialize the video generator\n\n        Args:\n            layout: Layout object that defines the visual arrangement\n            fps (int): Frames per second for the output video\n            batch_size (int): Number of frames to process in a batch\n                              before writing to disk\n        \"\"\"\n\n        self.layout = layout\n        self.fps = fps\n        self.batch_size = batch_size\n        self.audio_path = None\n        self.logo_path = None\n        self.title = None\n        self.temp_dir = None\n        self.frame_files = []\n        self.total_frames = 0\n\n    def generate_from_srt(\n        self,\n        srt_path,\n        audio_path=None,\n        logo_path=None,\n        title=None,\n        cpu_core_utilization=\"most\",\n    ):\n        \"\"\"\n        Generate video frames from an SRT file\n\n        Args:\n            srt_path (str): Path to the SRT file\n            audio_path (str, optional): Path to the audio file\n            logo_path (str, optional): Path to the logo image\n            title (str, optional): Title for the video\n            cpu_core_utilization (str, optional): `'single'`, `'half'`, `'most'`, `'max'`\n\n                - `single`: Uses 1 CPU core\n                - `half`: Uses half of available CPU cores\n                - `most`: (default) Uses all available CPU cores except one\n                - `max`: Uses all available CPU cores for maximum performance\n        \"\"\"\n\n        # Store paths for later use\n        self.audio_path = audio_path\n        self.logo_path = logo_path\n        self.title = title\n\n        # Update layout with logo and title\n        if hasattr(self.layout, \"logo_path\"):\n            self.layout.logo_path = logo_path\n        if hasattr(self.layout, \"title\"):\n            self.layout.title = title\n\n        # Load SRT file\n        logger.info(f\"Loading subtitles from {srt_path}\")\n        subs = pysrt.open(srt_path)\n\n        # Create temporary directory for frame storage\n        self.temp_dir = tempfile.mkdtemp()\n        self.frame_files = []\n        self.total_frames = 0\n\n        # Determine optimal number of workers\n        if cpu_core_utilization == \"single\":\n            num_workers = 1\n        elif cpu_core_utilization == \"half\":\n            num_workers = max(1, multiprocessing.cpu_count() // 2)\n        elif cpu_core_utilization == \"most\":\n            num_workers = max(1, multiprocessing.cpu_count() - 1)\n        elif cpu_core_utilization == \"max\":\n            num_workers = max(1, multiprocessing.cpu_count())\n        else:\n            raise ValueError(f\"Invalid CPU core utilities: {cpu_core_utilization}\")\n\n        logger.info(f\"Using {num_workers} CPU cores for parallel processing\")\n\n        # Process subtitles in parallel batches\n        with concurrent.futures.ProcessPoolExecutor(\n            max_workers=num_workers\n        ) as executor:\n            # Prepare subtitle batches for parallel processing\n            sub_batches = []\n            current_batch = []\n            current_batch_frames = 0\n\n            for sub in subs:\n                start_frame = sub.start.ordinal // (1000 // self.fps)\n                end_frame = sub.end.ordinal // (1000 // self.fps)\n                num_frames = (end_frame - start_frame) + min(\n                    15, end_frame - start_frame\n                )  # Including fade frames\n\n                if (\n                    current_batch_frames + num_frames &gt; self.batch_size\n                    and current_batch\n                ):\n                    sub_batches.append(current_batch)\n                    current_batch = []\n                    current_batch_frames = 0\n\n                current_batch.append(sub)\n                current_batch_frames += num_frames\n\n            # Add the last batch if not empty\n            if current_batch:\n                sub_batches.append(current_batch)\n\n            logger.info(f\"Processing subtitle to generate frames in {len(sub_batches)} batches\")\n\n            # Process each batch in parallel\n            batch_results = []\n            for batch_idx, batch in enumerate(sub_batches):\n                batch_results.append(\n                    executor.submit(\n                        self._process_subtitle_batch,\n                        batch,\n                        batch_idx,\n                        self.layout,\n                        self.fps,\n                        self.temp_dir,\n                    )\n                )\n\n            # Collect results with progress bar\n            with tqdm(total=len(batch_results), desc=\"Processing batch\", unit=\"batch\") as pbar:\n                for future in concurrent.futures.as_completed(batch_results):\n                    batch_frame_files, batch_frame_count = future.result()\n                    self.frame_files.extend(batch_frame_files)\n                    self.total_frames += batch_frame_count\n                    pbar.update(1)\n                    pbar.set_postfix({\"frames processed\": self.total_frames})\n\n        # Sort frame files by frame number to ensure correct sequence\n        self.frame_files.sort(\n            key=lambda x: int(os.path.basename(x).split(\"_\")[1].split(\".\")[0])\n        )\n\n        logger.info(f\"Frame generation completed: Total {self.total_frames} frames created\")\n        return self\n\n    def _process_subtitle_batch(self, subs_batch, batch_index, layout, fps, temp_dir):\n        \"\"\"\n        Process a batch of subtitles in parallel\n\n        Args:\n            subs_batch (list): List of subtitles to process\n            batch_index (int): Index of the current batch\n            layout: Layout object to use for frame creation\n            fps (int): Frames per second\n            temp_dir (str): Directory to store temporary files\n\n        Returns:\n            tuple: (list of frame files, number of frames processed)\n        \"\"\"\n\n        # Create a batch directory\n        batch_dir = os.path.join(temp_dir, f\"batch_{batch_index}\")\n        os.makedirs(batch_dir, exist_ok=True)\n\n        frame_files = []\n        frame_count = 0\n\n        # Process each subtitle in the batch\n        for sub in subs_batch:\n            start_frame = sub.start.ordinal // (1000 // fps)\n            end_frame = sub.end.ordinal // (1000 // fps)\n\n            # Add fade-in effect\n            fade_frames = min(15, end_frame - start_frame)\n            for i in range(fade_frames):\n                opacity = int((i / fade_frames) * 255)\n                frame = layout.create_frame(current_sub=sub, opacity=opacity)\n\n                frame_path = os.path.join(batch_dir, f\"frame_{start_frame + i:08d}.png\")\n\n                # Convert numpy array to PIL Image and save\n                if isinstance(frame, np.ndarray):\n                    Image.fromarray(frame).save(frame_path)\n                else:\n                    frame.save(frame_path)\n\n                frame_files.append(frame_path)\n                frame_count += 1\n\n            # Add main frames\n            for frame_idx in range(start_frame + fade_frames, end_frame):\n                frame = layout.create_frame(current_sub=sub)\n\n                frame_path = os.path.join(batch_dir, f\"frame_{frame_idx:08d}.png\")\n\n                # Convert numpy array to PIL Image and save\n                if isinstance(frame, np.ndarray):\n                    Image.fromarray(frame).save(frame_path)\n                else:\n                    frame.save(frame_path)\n\n                frame_files.append(frame_path)\n                frame_count += 1\n\n        return frame_files, frame_count\n\n    def export_video(\n        self,\n        output_path,\n        encoder=\"auto\",\n        video_codec=None,\n        audio_codec=None,\n        video_bitrate=\"8M\",\n        audio_bitrate=\"192k\",\n        preset=\"medium\",\n        crf=23,\n        threads=None,\n        gpu_acceleration=True,\n        extra_ffmpeg_args=None,\n    ):\n        \"\"\"\n        Export the generated frames as a video\n\n        Args:\n            output_path (str): Path for the output video file\n            encoder (str): Encoding method to use: `'ffmpeg'`, `'moviepy'`, or `'auto'` (default)\n            video_codec (str, optional): Video codec to use (default: `'h264_nvenc'` for GPU, `'libx264'` for CPU)\n                See [FFmpeg H.264 Guide](https://trac.ffmpeg.org/wiki/Encode/H.264) for CPU options\n                See [NVIDIA FFmpeg Guide](https://developer.nvidia.com/blog/nvidia-ffmpeg-transcoding-guide/) for GPU options\n            audio_codec (str, optional): Audio codec to use (default: `'aac'`)\n                See [FFmpeg AAC Guide](https://trac.ffmpeg.org/wiki/Encode/AAC) for audio codec options\n            video_bitrate (str, optional): Video bitrate (default: `'8M'`)\n            audio_bitrate (str, optional): Audio bitrate (default: `'192k'`)\n            preset (str, optional): Encoding preset (default: `'medium'`)\n                For CPU encoding (libx264):\n                    Options: `'ultrafast'`, `'superfast'`, `'veryfast'`, `'faster'`, `'fast'`,\n                             `'medium'`, `'slow'`, `'slower'`, `'veryslow'`\n                    Slower presets give better compression/quality at the cost of encoding time.\n                    See [FFmpeg Preset Guide](https://trac.ffmpeg.org/wiki/Encode/H.264#a2.Chooseapresetandtune)\n                For GPU encoding (NVENC):\n                    Will be automatically converted to NVENC presets:\n                    `'slow'`/`'slower'`/`'veryslow'` \u2192 `'p1'` (highest quality)\n                    `'medium'` \u2192 `'p3'` (balanced)\n                    `'fast'`/`'faster'` \u2192 `'p5'` (faster encoding)\n                    `'veryfast'`/`'superfast'`/`'ultrafast'` \u2192 `'p7'` (fastest encoding)\n                    See [NVIDIA FFmpeg Integration](https://docs.nvidia.com/video-technologies/video-codec-sdk/12.0/ffmpeg-with-nvidia-gpu/index.html)\n            crf (int, optional): Constant Rate Factor for quality (default: `23`, lower is better quality)\n                Range: `0-51`, where lower values mean better quality and larger file size\n                Recommended range: `18-28`. See [CRF Guide](https://trac.ffmpeg.org/wiki/Encode/H.264#crf)\n            threads (int, optional): Number of encoding threads (default: CPU count - 1)\n            gpu_acceleration (bool, optional): Whether to use GPU acceleration if available (default: `True`)\n            extra_ffmpeg_args (list, optional): Additional FFmpeg arguments as a list\n                See [FFmpeg Documentation](https://ffmpeg.org/ffmpeg.html) for all available options\n        \"\"\"\n\n        logger.info(f\"Starting video generation process with {self.total_frames} frames\")\n\n        # Calculate video duration\n        video_duration = self.total_frames / self.fps\n\n        # Determine audio duration if provided\n        audio_duration = None\n        if self.audio_path:\n            try:\n                # Import locally since this is a heavier dependency\n                from moviepy.editor import AudioFileClip\n\n                audio = AudioFileClip(self.audio_path)\n                audio_duration = audio.duration\n                audio.close()\n            except Exception as e:\n                logger.warning(f\"Could not determine audio duration: {e}\")\n\n        # Use the shorter duration to ensure sync\n        final_duration = video_duration\n        if audio_duration:\n            final_duration = min(video_duration, audio_duration)\n            logger.info(f\"Video duration: {final_duration:.2f}s (adjusted to match audio)\")\n        else:\n            logger.info(f\"Video duration: {final_duration:.2f}s\")\n\n        # Sort frames by number to ensure correct sequence\n        self.frame_files.sort(\n            key=lambda x: int(os.path.basename(x).split(\"_\")[1].split(\".\")[0])\n        )\n\n        # Set default threads if not specified\n        if threads is None:\n            threads = max(4, os.cpu_count() - 1)\n\n        # Determine which encoder to use\n        if encoder == \"auto\":\n            try:\n                logger.info(\"Attempting video export with FFmpeg encoding\")\n                self._export_video_with_ffmpeg(\n                    output_path,\n                    final_duration,\n                    video_codec,\n                    audio_codec,\n                    video_bitrate,\n                    audio_bitrate,\n                    preset,\n                    crf,\n                    threads,\n                    gpu_acceleration,\n                    extra_ffmpeg_args,\n                )\n            except Exception as e:\n                logger.warning(f\"FFmpeg export failed: {e}\")\n                logger.info(\"Falling back to MoviePy for video encoding\")\n                self._export_video_with_moviepy(\n                    output_path,\n                    final_duration,\n                    video_codec,\n                    audio_codec,\n                    video_bitrate,\n                    audio_bitrate,\n                    preset,\n                    threads,\n                )\n        elif encoder == \"ffmpeg\":\n            logger.info(\"Starting video export using native FFmpeg\")\n            self._export_video_with_ffmpeg(\n                output_path,\n                final_duration,\n                video_codec,\n                audio_codec,\n                video_bitrate,\n                audio_bitrate,\n                preset,\n                crf,\n                threads,\n                gpu_acceleration,\n                extra_ffmpeg_args,\n            )\n        elif encoder == \"moviepy\":\n            logger.info(\"Starting video export using module MoviePy\")\n            self._export_video_with_moviepy(\n                output_path,\n                final_duration,\n                video_codec,\n                audio_codec,\n                video_bitrate,\n                audio_bitrate,\n                preset,\n                threads,\n            )\n        else:\n            logger.error(f\"Invalid encoder: {encoder}. Choose 'ffmpeg', 'moviepy', or 'auto'\")\n            raise ValueError(\n                f\"Invalid encoder: {encoder}. Choose 'ffmpeg', 'moviepy', or 'auto'\"\n            )\n\n        # Clean up temporary files\n        try:\n            shutil.rmtree(self.temp_dir)\n            logger.info(f\"Cleaned up temporary files in {self.temp_dir}\")\n        except Exception as e:\n            logger.warning(f\"Could not clean up temporary files: {e}\")\n\n        logger.info(f\"Video generation completed! Exported to: {output_path}\")\n        return output_path\n\n    def _export_video_with_ffmpeg(\n        self,\n        output_path,\n        duration,\n        video_codec=None,\n        audio_codec=None,\n        video_bitrate=\"8M\",\n        audio_bitrate=\"192k\",\n        preset=\"medium\",\n        crf=23,\n        threads=None,\n        gpu_acceleration=True,\n        extra_args=None,\n    ):\n        \"\"\"\n        Export video using FFmpeg directly with potential GPU acceleration\n\n        Args:\n            output_path (str): Path for the output video file\n            duration (float): Duration of the video in seconds\n            video_codec (str, optional): Video codec to use\n            audio_codec (str, optional): Audio codec to use\n            video_bitrate (str, optional): Video bitrate\n            audio_bitrate (str, optional): Audio bitrate\n            preset (str, optional): Encoding preset\n            crf (int, optional): Constant Rate Factor for quality\n            threads (int, optional): Number of encoding threads\n            gpu_acceleration (bool): Whether to use GPU acceleration\n            extra_args (list, optional): Additional FFmpeg arguments\n        \"\"\"\n\n        # Prepare output directory\n        output_dir = os.path.dirname(output_path)\n        if output_dir and not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        # Create a temporary file listing all frames with precise timing\n        frames_list_file = os.path.join(self.temp_dir, \"frames_list.txt\")\n        frame_duration = 1.0 / self.fps\n\n        logger.info(\"Preparing frame list for FFmpeg\")\n        with open(frames_list_file, \"w\") as f:\n            for i, frame_file in enumerate(self.frame_files):\n                f.write(f\"file '{frame_file}'\\n\")\n                # Use exact frame duration to prevent drift\n                f.write(f\"duration {frame_duration}\\n\")\n\n        # Check for NVIDIA GPU with NVENC support if GPU acceleration is requested\n        has_nvidia = False\n        if gpu_acceleration and (video_codec is None or video_codec == \"h264_nvenc\"):\n            try:\n                nvidia_check = subprocess.run(\n                    [\"nvidia-smi\"],\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                    text=True,\n                )\n                has_nvidia = nvidia_check.returncode == 0\n            except FileNotFoundError:\n                pass\n\n        # Set default threads if not specified\n        if threads is None:\n            threads = max(4, os.cpu_count() - 1)\n\n        # Base FFmpeg command with improved sync options\n        ffmpeg_cmd = [\n            \"ffmpeg\",\n            \"-y\",\n            \"-f\",\n            \"concat\",\n            \"-safe\",\n            \"0\",\n            \"-i\",\n            frames_list_file,\n            \"-vsync\",\n            \"cfr\",  # Constant frame rate for better sync\n            \"-t\",\n            str(duration),\n        ]\n\n        # Add audio if provided\n        if self.audio_path:\n            ffmpeg_cmd.extend(\n                [\n                    \"-i\",\n                    self.audio_path,\n                    \"-t\",\n                    str(duration),\n                    \"-map\",\n                    \"0:v\",\n                    \"-map\",\n                    \"1:a\",\n                    \"-async\",\n                    \"1\",  # Better audio sync\n                ]\n            )\n\n        # Determine if we should use GPU encoding\n        use_gpu = (\n            has_nvidia\n            and gpu_acceleration\n            and (video_codec is None or video_codec == \"h264_nvenc\")\n        )\n\n        # Determine video codec and encoding settings\n        if use_gpu:\n            logger.info(\"Using NVIDIA GPU acceleration for video encoding\")\n            # Set default video codec for GPU\n            video_codec = \"h264_nvenc\"\n\n            # Convert x264 preset to NVENC preset\n            nvenc_preset = \"p3\"  # Default balanced preset\n            if preset in [\"veryslow\", \"slower\", \"slow\"]:\n                nvenc_preset = \"p1\"  # Highest quality\n            elif preset == \"medium\":\n                nvenc_preset = \"p3\"  # Balanced\n            elif preset in [\"fast\", \"faster\"]:\n                nvenc_preset = \"p5\"  # Faster encoding\n            elif preset in [\"veryfast\", \"superfast\", \"ultrafast\"]:\n                nvenc_preset = \"p7\"  # Fastest encoding\n\n            ffmpeg_cmd.extend(\n                [\n                    \"-c:v\",\n                    video_codec,\n                    \"-preset\",\n                    nvenc_preset,\n                    \"-tune\",\n                    \"hq\",\n                    \"-rc\",\n                    \"vbr\",\n                    \"-b:v\",\n                    video_bitrate,\n                    \"-maxrate\",\n                    str(float(video_bitrate.rstrip(\"M\")) * 1.25) + \"M\",\n                ]\n            )\n        else:\n            logger.info(f\"Using CPU encoding with {threads} threads\")\n            # Set default video codec for CPU if not specified\n            if video_codec is None:\n                video_codec = \"libx264\"\n\n            # For CPU encoding, use the x264 preset directly\n            ffmpeg_cmd.extend(\n                [\n                    \"-c:v\",\n                    video_codec,\n                    \"-preset\",\n                    preset,\n                    \"-crf\",\n                    str(crf),\n                    \"-threads\",\n                    str(threads),\n                ]\n            )\n\n            # Add tune parameter only for libx264\n            if video_codec == \"libx264\":\n                ffmpeg_cmd.extend([\"-tune\", \"film\"])\n\n        # Add audio encoding settings if audio is provided\n        if self.audio_path:\n            # Set default audio codec if not specified\n            if audio_codec is None:\n                audio_codec = \"aac\"\n\n            ffmpeg_cmd.extend([\"-c:a\", audio_codec, \"-b:a\", audio_bitrate])\n\n        # Add output format settings with improved sync options\n        ffmpeg_cmd.extend([\"-pix_fmt\", \"yuv420p\", \"-movflags\", \"+faststart\"])\n\n        # Add any extra arguments\n        if extra_args:\n            ffmpeg_cmd.extend(extra_args)\n\n        # Add output path\n        ffmpeg_cmd.append(output_path)\n\n        # Run FFmpeg\n        logger.info(\"Starting FFmpeg encoding process\")\n        logger.debug(f\"FFmpeg command: {' '.join(ffmpeg_cmd)}\")\n\n        # Run FFmpeg with progress indication\n        process = subprocess.Popen(\n            ffmpeg_cmd, \n            stdout=subprocess.PIPE, \n            stderr=subprocess.STDOUT, \n            universal_newlines=True\n        )\n\n        # Simple progress indicator since FFmpeg output is complex\n        with tqdm(total=100, desc=\"Encoding video\", unit=\"%\") as pbar:\n            last_progress = 0\n            for line in process.stdout:\n                # Try to extract progress information from FFmpeg output\n                if \"time=\" in line:\n                    try:\n                        time_str = line.split(\"time=\")[1].split()[0]\n                        h, m, s = time_str.split(':')\n                        current_time = float(h) * 3600 + float(m) * 60 + float(s)\n                        progress = min(int(current_time / duration * 100), 100)\n                        if progress &gt; last_progress:\n                            pbar.update(progress - last_progress)\n                            last_progress = progress\n                    except:\n                        pass\n\n        process.wait()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, ffmpeg_cmd)\n\n        logger.info(f\"Video successfully encoded to {output_path}\")\n\n    def _export_video_with_moviepy(\n        self,\n        output_path,\n        duration,\n        video_codec=None,\n        audio_codec=None,\n        video_bitrate=\"8M\",\n        audio_bitrate=\"192k\",\n        preset=\"medium\",\n        threads=None,\n    ):\n        \"\"\"\n        Fallback method to export video using MoviePy\n\n        Args:\n            output_path (str): Path for the output video file\n            duration (float): Duration of the video in seconds\n            video_codec (str, optional): Video codec to use\n            audio_codec (str, optional): Audio codec to use\n            video_bitrate (str, optional): Video bitrate\n            audio_bitrate (str, optional): Audio bitrate\n            preset (str, optional): Encoding preset (x264 preset names)\n            threads (int, optional): Number of encoding threads\n        \"\"\"\n\n        # Import locally since this is a heavier dependency\n        from moviepy.editor import ImageSequenceClip\n\n        # Set default codecs if not specified\n        if video_codec is None:\n            video_codec = \"libx264\"\n        if audio_codec is None:\n            audio_codec = \"aac\"\n\n        # Set default threads if not specified\n        if threads is None:\n            threads = max(4, os.cpu_count() - 1)\n\n        logger.info(\"Loading frames for MoviePy\")\n        # Convert frames to video using the saved frame files\n        video = ImageSequenceClip(self.frame_files, fps=self.fps)\n\n        # Trim video to match duration\n        video = video.subclip(0, duration)\n\n        # Add audio if provided\n        if self.audio_path:\n            # Import locally since this is a heavier dependency\n            from moviepy.editor import AudioFileClip\n\n            logger.info(f\"Adding audio from {self.audio_path}\")\n            audio = AudioFileClip(self.audio_path)\n            audio = audio.subclip(0, duration)\n            video = video.set_audio(audio)\n\n        # Prepare ffmpeg parameters for MoviePy\n        ffmpeg_params = [\"-preset\", preset]\n\n        # Add bitrate parameter if specified\n        if video_bitrate:\n            ffmpeg_params.extend([\"-b:v\", video_bitrate])\n\n        # Export video\n        logger.info(f\"Starting MoviePy encoding with {video_codec} codec\")\n        video.write_videofile(\n            output_path,\n            codec=video_codec,\n            fps=self.fps,\n            threads=threads,\n            audio_codec=audio_codec,\n            bitrate=video_bitrate,\n            ffmpeg_params=ffmpeg_params,\n            logger=\"bar\",\n        )\n</code></pre>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator.__init__","title":"<code>__init__(layout, fps=30, batch_size=300)</code>","text":"<p>Initialize the video generator</p> <p>Parameters:</p> Name Type Description Default <code>layout</code> <p>Layout object that defines the visual arrangement</p> required <code>fps</code> <code>int</code> <p>Frames per second for the output video</p> <code>30</code> <code>batch_size</code> <code>int</code> <p>Number of frames to process in a batch               before writing to disk</p> <code>300</code> Source code in <code>audim/sub2pod/core.py</code> <pre><code>def __init__(self, layout, fps=30, batch_size=300):\n    \"\"\"\n    Initialize the video generator\n\n    Args:\n        layout: Layout object that defines the visual arrangement\n        fps (int): Frames per second for the output video\n        batch_size (int): Number of frames to process in a batch\n                          before writing to disk\n    \"\"\"\n\n    self.layout = layout\n    self.fps = fps\n    self.batch_size = batch_size\n    self.audio_path = None\n    self.logo_path = None\n    self.title = None\n    self.temp_dir = None\n    self.frame_files = []\n    self.total_frames = 0\n</code></pre>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator._export_video_with_ffmpeg","title":"<code>_export_video_with_ffmpeg(output_path, duration, video_codec=None, audio_codec=None, video_bitrate='8M', audio_bitrate='192k', preset='medium', crf=23, threads=None, gpu_acceleration=True, extra_args=None)</code>","text":"<p>Export video using FFmpeg directly with potential GPU acceleration</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path for the output video file</p> required <code>duration</code> <code>float</code> <p>Duration of the video in seconds</p> required <code>video_codec</code> <code>str</code> <p>Video codec to use</p> <code>None</code> <code>audio_codec</code> <code>str</code> <p>Audio codec to use</p> <code>None</code> <code>video_bitrate</code> <code>str</code> <p>Video bitrate</p> <code>'8M'</code> <code>audio_bitrate</code> <code>str</code> <p>Audio bitrate</p> <code>'192k'</code> <code>preset</code> <code>str</code> <p>Encoding preset</p> <code>'medium'</code> <code>crf</code> <code>int</code> <p>Constant Rate Factor for quality</p> <code>23</code> <code>threads</code> <code>int</code> <p>Number of encoding threads</p> <code>None</code> <code>gpu_acceleration</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <code>extra_args</code> <code>list</code> <p>Additional FFmpeg arguments</p> <code>None</code> Source code in <code>audim/sub2pod/core.py</code> <pre><code>def _export_video_with_ffmpeg(\n    self,\n    output_path,\n    duration,\n    video_codec=None,\n    audio_codec=None,\n    video_bitrate=\"8M\",\n    audio_bitrate=\"192k\",\n    preset=\"medium\",\n    crf=23,\n    threads=None,\n    gpu_acceleration=True,\n    extra_args=None,\n):\n    \"\"\"\n    Export video using FFmpeg directly with potential GPU acceleration\n\n    Args:\n        output_path (str): Path for the output video file\n        duration (float): Duration of the video in seconds\n        video_codec (str, optional): Video codec to use\n        audio_codec (str, optional): Audio codec to use\n        video_bitrate (str, optional): Video bitrate\n        audio_bitrate (str, optional): Audio bitrate\n        preset (str, optional): Encoding preset\n        crf (int, optional): Constant Rate Factor for quality\n        threads (int, optional): Number of encoding threads\n        gpu_acceleration (bool): Whether to use GPU acceleration\n        extra_args (list, optional): Additional FFmpeg arguments\n    \"\"\"\n\n    # Prepare output directory\n    output_dir = os.path.dirname(output_path)\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create a temporary file listing all frames with precise timing\n    frames_list_file = os.path.join(self.temp_dir, \"frames_list.txt\")\n    frame_duration = 1.0 / self.fps\n\n    logger.info(\"Preparing frame list for FFmpeg\")\n    with open(frames_list_file, \"w\") as f:\n        for i, frame_file in enumerate(self.frame_files):\n            f.write(f\"file '{frame_file}'\\n\")\n            # Use exact frame duration to prevent drift\n            f.write(f\"duration {frame_duration}\\n\")\n\n    # Check for NVIDIA GPU with NVENC support if GPU acceleration is requested\n    has_nvidia = False\n    if gpu_acceleration and (video_codec is None or video_codec == \"h264_nvenc\"):\n        try:\n            nvidia_check = subprocess.run(\n                [\"nvidia-smi\"],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n            )\n            has_nvidia = nvidia_check.returncode == 0\n        except FileNotFoundError:\n            pass\n\n    # Set default threads if not specified\n    if threads is None:\n        threads = max(4, os.cpu_count() - 1)\n\n    # Base FFmpeg command with improved sync options\n    ffmpeg_cmd = [\n        \"ffmpeg\",\n        \"-y\",\n        \"-f\",\n        \"concat\",\n        \"-safe\",\n        \"0\",\n        \"-i\",\n        frames_list_file,\n        \"-vsync\",\n        \"cfr\",  # Constant frame rate for better sync\n        \"-t\",\n        str(duration),\n    ]\n\n    # Add audio if provided\n    if self.audio_path:\n        ffmpeg_cmd.extend(\n            [\n                \"-i\",\n                self.audio_path,\n                \"-t\",\n                str(duration),\n                \"-map\",\n                \"0:v\",\n                \"-map\",\n                \"1:a\",\n                \"-async\",\n                \"1\",  # Better audio sync\n            ]\n        )\n\n    # Determine if we should use GPU encoding\n    use_gpu = (\n        has_nvidia\n        and gpu_acceleration\n        and (video_codec is None or video_codec == \"h264_nvenc\")\n    )\n\n    # Determine video codec and encoding settings\n    if use_gpu:\n        logger.info(\"Using NVIDIA GPU acceleration for video encoding\")\n        # Set default video codec for GPU\n        video_codec = \"h264_nvenc\"\n\n        # Convert x264 preset to NVENC preset\n        nvenc_preset = \"p3\"  # Default balanced preset\n        if preset in [\"veryslow\", \"slower\", \"slow\"]:\n            nvenc_preset = \"p1\"  # Highest quality\n        elif preset == \"medium\":\n            nvenc_preset = \"p3\"  # Balanced\n        elif preset in [\"fast\", \"faster\"]:\n            nvenc_preset = \"p5\"  # Faster encoding\n        elif preset in [\"veryfast\", \"superfast\", \"ultrafast\"]:\n            nvenc_preset = \"p7\"  # Fastest encoding\n\n        ffmpeg_cmd.extend(\n            [\n                \"-c:v\",\n                video_codec,\n                \"-preset\",\n                nvenc_preset,\n                \"-tune\",\n                \"hq\",\n                \"-rc\",\n                \"vbr\",\n                \"-b:v\",\n                video_bitrate,\n                \"-maxrate\",\n                str(float(video_bitrate.rstrip(\"M\")) * 1.25) + \"M\",\n            ]\n        )\n    else:\n        logger.info(f\"Using CPU encoding with {threads} threads\")\n        # Set default video codec for CPU if not specified\n        if video_codec is None:\n            video_codec = \"libx264\"\n\n        # For CPU encoding, use the x264 preset directly\n        ffmpeg_cmd.extend(\n            [\n                \"-c:v\",\n                video_codec,\n                \"-preset\",\n                preset,\n                \"-crf\",\n                str(crf),\n                \"-threads\",\n                str(threads),\n            ]\n        )\n\n        # Add tune parameter only for libx264\n        if video_codec == \"libx264\":\n            ffmpeg_cmd.extend([\"-tune\", \"film\"])\n\n    # Add audio encoding settings if audio is provided\n    if self.audio_path:\n        # Set default audio codec if not specified\n        if audio_codec is None:\n            audio_codec = \"aac\"\n\n        ffmpeg_cmd.extend([\"-c:a\", audio_codec, \"-b:a\", audio_bitrate])\n\n    # Add output format settings with improved sync options\n    ffmpeg_cmd.extend([\"-pix_fmt\", \"yuv420p\", \"-movflags\", \"+faststart\"])\n\n    # Add any extra arguments\n    if extra_args:\n        ffmpeg_cmd.extend(extra_args)\n\n    # Add output path\n    ffmpeg_cmd.append(output_path)\n\n    # Run FFmpeg\n    logger.info(\"Starting FFmpeg encoding process\")\n    logger.debug(f\"FFmpeg command: {' '.join(ffmpeg_cmd)}\")\n\n    # Run FFmpeg with progress indication\n    process = subprocess.Popen(\n        ffmpeg_cmd, \n        stdout=subprocess.PIPE, \n        stderr=subprocess.STDOUT, \n        universal_newlines=True\n    )\n\n    # Simple progress indicator since FFmpeg output is complex\n    with tqdm(total=100, desc=\"Encoding video\", unit=\"%\") as pbar:\n        last_progress = 0\n        for line in process.stdout:\n            # Try to extract progress information from FFmpeg output\n            if \"time=\" in line:\n                try:\n                    time_str = line.split(\"time=\")[1].split()[0]\n                    h, m, s = time_str.split(':')\n                    current_time = float(h) * 3600 + float(m) * 60 + float(s)\n                    progress = min(int(current_time / duration * 100), 100)\n                    if progress &gt; last_progress:\n                        pbar.update(progress - last_progress)\n                        last_progress = progress\n                except:\n                    pass\n\n    process.wait()\n    if process.returncode != 0:\n        raise subprocess.CalledProcessError(process.returncode, ffmpeg_cmd)\n\n    logger.info(f\"Video successfully encoded to {output_path}\")\n</code></pre>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator._export_video_with_moviepy","title":"<code>_export_video_with_moviepy(output_path, duration, video_codec=None, audio_codec=None, video_bitrate='8M', audio_bitrate='192k', preset='medium', threads=None)</code>","text":"<p>Fallback method to export video using MoviePy</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path for the output video file</p> required <code>duration</code> <code>float</code> <p>Duration of the video in seconds</p> required <code>video_codec</code> <code>str</code> <p>Video codec to use</p> <code>None</code> <code>audio_codec</code> <code>str</code> <p>Audio codec to use</p> <code>None</code> <code>video_bitrate</code> <code>str</code> <p>Video bitrate</p> <code>'8M'</code> <code>audio_bitrate</code> <code>str</code> <p>Audio bitrate</p> <code>'192k'</code> <code>preset</code> <code>str</code> <p>Encoding preset (x264 preset names)</p> <code>'medium'</code> <code>threads</code> <code>int</code> <p>Number of encoding threads</p> <code>None</code> Source code in <code>audim/sub2pod/core.py</code> <pre><code>def _export_video_with_moviepy(\n    self,\n    output_path,\n    duration,\n    video_codec=None,\n    audio_codec=None,\n    video_bitrate=\"8M\",\n    audio_bitrate=\"192k\",\n    preset=\"medium\",\n    threads=None,\n):\n    \"\"\"\n    Fallback method to export video using MoviePy\n\n    Args:\n        output_path (str): Path for the output video file\n        duration (float): Duration of the video in seconds\n        video_codec (str, optional): Video codec to use\n        audio_codec (str, optional): Audio codec to use\n        video_bitrate (str, optional): Video bitrate\n        audio_bitrate (str, optional): Audio bitrate\n        preset (str, optional): Encoding preset (x264 preset names)\n        threads (int, optional): Number of encoding threads\n    \"\"\"\n\n    # Import locally since this is a heavier dependency\n    from moviepy.editor import ImageSequenceClip\n\n    # Set default codecs if not specified\n    if video_codec is None:\n        video_codec = \"libx264\"\n    if audio_codec is None:\n        audio_codec = \"aac\"\n\n    # Set default threads if not specified\n    if threads is None:\n        threads = max(4, os.cpu_count() - 1)\n\n    logger.info(\"Loading frames for MoviePy\")\n    # Convert frames to video using the saved frame files\n    video = ImageSequenceClip(self.frame_files, fps=self.fps)\n\n    # Trim video to match duration\n    video = video.subclip(0, duration)\n\n    # Add audio if provided\n    if self.audio_path:\n        # Import locally since this is a heavier dependency\n        from moviepy.editor import AudioFileClip\n\n        logger.info(f\"Adding audio from {self.audio_path}\")\n        audio = AudioFileClip(self.audio_path)\n        audio = audio.subclip(0, duration)\n        video = video.set_audio(audio)\n\n    # Prepare ffmpeg parameters for MoviePy\n    ffmpeg_params = [\"-preset\", preset]\n\n    # Add bitrate parameter if specified\n    if video_bitrate:\n        ffmpeg_params.extend([\"-b:v\", video_bitrate])\n\n    # Export video\n    logger.info(f\"Starting MoviePy encoding with {video_codec} codec\")\n    video.write_videofile(\n        output_path,\n        codec=video_codec,\n        fps=self.fps,\n        threads=threads,\n        audio_codec=audio_codec,\n        bitrate=video_bitrate,\n        ffmpeg_params=ffmpeg_params,\n        logger=\"bar\",\n    )\n</code></pre>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator._process_subtitle_batch","title":"<code>_process_subtitle_batch(subs_batch, batch_index, layout, fps, temp_dir)</code>","text":"<p>Process a batch of subtitles in parallel</p> <p>Parameters:</p> Name Type Description Default <code>subs_batch</code> <code>list</code> <p>List of subtitles to process</p> required <code>batch_index</code> <code>int</code> <p>Index of the current batch</p> required <code>layout</code> <p>Layout object to use for frame creation</p> required <code>fps</code> <code>int</code> <p>Frames per second</p> required <code>temp_dir</code> <code>str</code> <p>Directory to store temporary files</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(list of frame files, number of frames processed)</p> Source code in <code>audim/sub2pod/core.py</code> <pre><code>def _process_subtitle_batch(self, subs_batch, batch_index, layout, fps, temp_dir):\n    \"\"\"\n    Process a batch of subtitles in parallel\n\n    Args:\n        subs_batch (list): List of subtitles to process\n        batch_index (int): Index of the current batch\n        layout: Layout object to use for frame creation\n        fps (int): Frames per second\n        temp_dir (str): Directory to store temporary files\n\n    Returns:\n        tuple: (list of frame files, number of frames processed)\n    \"\"\"\n\n    # Create a batch directory\n    batch_dir = os.path.join(temp_dir, f\"batch_{batch_index}\")\n    os.makedirs(batch_dir, exist_ok=True)\n\n    frame_files = []\n    frame_count = 0\n\n    # Process each subtitle in the batch\n    for sub in subs_batch:\n        start_frame = sub.start.ordinal // (1000 // fps)\n        end_frame = sub.end.ordinal // (1000 // fps)\n\n        # Add fade-in effect\n        fade_frames = min(15, end_frame - start_frame)\n        for i in range(fade_frames):\n            opacity = int((i / fade_frames) * 255)\n            frame = layout.create_frame(current_sub=sub, opacity=opacity)\n\n            frame_path = os.path.join(batch_dir, f\"frame_{start_frame + i:08d}.png\")\n\n            # Convert numpy array to PIL Image and save\n            if isinstance(frame, np.ndarray):\n                Image.fromarray(frame).save(frame_path)\n            else:\n                frame.save(frame_path)\n\n            frame_files.append(frame_path)\n            frame_count += 1\n\n        # Add main frames\n        for frame_idx in range(start_frame + fade_frames, end_frame):\n            frame = layout.create_frame(current_sub=sub)\n\n            frame_path = os.path.join(batch_dir, f\"frame_{frame_idx:08d}.png\")\n\n            # Convert numpy array to PIL Image and save\n            if isinstance(frame, np.ndarray):\n                Image.fromarray(frame).save(frame_path)\n            else:\n                frame.save(frame_path)\n\n            frame_files.append(frame_path)\n            frame_count += 1\n\n    return frame_files, frame_count\n</code></pre>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator.export_video","title":"<code>export_video(output_path, encoder='auto', video_codec=None, audio_codec=None, video_bitrate='8M', audio_bitrate='192k', preset='medium', crf=23, threads=None, gpu_acceleration=True, extra_ffmpeg_args=None)</code>","text":"<p>Export the generated frames as a video</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path for the output video file</p> required <code>encoder</code> <code>str</code> <p>Encoding method to use: <code>'ffmpeg'</code>, <code>'moviepy'</code>, or <code>'auto'</code> (default)</p> <code>'auto'</code> <code>video_codec</code> <code>str</code> <p>Video codec to use (default: <code>'h264_nvenc'</code> for GPU, <code>'libx264'</code> for CPU) See FFmpeg H.264 Guide for CPU options See NVIDIA FFmpeg Guide for GPU options</p> <code>None</code> <code>audio_codec</code> <code>str</code> <p>Audio codec to use (default: <code>'aac'</code>) See FFmpeg AAC Guide for audio codec options</p> <code>None</code> <code>video_bitrate</code> <code>str</code> <p>Video bitrate (default: <code>'8M'</code>)</p> <code>'8M'</code> <code>audio_bitrate</code> <code>str</code> <p>Audio bitrate (default: <code>'192k'</code>)</p> <code>'192k'</code> <code>preset</code> <code>str</code> <p>Encoding preset (default: <code>'medium'</code>) For CPU encoding (libx264):     Options: <code>'ultrafast'</code>, <code>'superfast'</code>, <code>'veryfast'</code>, <code>'faster'</code>, <code>'fast'</code>,              <code>'medium'</code>, <code>'slow'</code>, <code>'slower'</code>, <code>'veryslow'</code>     Slower presets give better compression/quality at the cost of encoding time.     See FFmpeg Preset Guide For GPU encoding (NVENC):     Will be automatically converted to NVENC presets:     <code>'slow'</code>/<code>'slower'</code>/<code>'veryslow'</code> \u2192 <code>'p1'</code> (highest quality)     <code>'medium'</code> \u2192 <code>'p3'</code> (balanced)     <code>'fast'</code>/<code>'faster'</code> \u2192 <code>'p5'</code> (faster encoding)     <code>'veryfast'</code>/<code>'superfast'</code>/<code>'ultrafast'</code> \u2192 <code>'p7'</code> (fastest encoding)     See NVIDIA FFmpeg Integration</p> <code>'medium'</code> <code>crf</code> <code>int</code> <p>Constant Rate Factor for quality (default: <code>23</code>, lower is better quality) Range: <code>0-51</code>, where lower values mean better quality and larger file size Recommended range: <code>18-28</code>. See CRF Guide</p> <code>23</code> <code>threads</code> <code>int</code> <p>Number of encoding threads (default: CPU count - 1)</p> <code>None</code> <code>gpu_acceleration</code> <code>bool</code> <p>Whether to use GPU acceleration if available (default: <code>True</code>)</p> <code>True</code> <code>extra_ffmpeg_args</code> <code>list</code> <p>Additional FFmpeg arguments as a list See FFmpeg Documentation for all available options</p> <code>None</code> Source code in <code>audim/sub2pod/core.py</code> <pre><code>def export_video(\n    self,\n    output_path,\n    encoder=\"auto\",\n    video_codec=None,\n    audio_codec=None,\n    video_bitrate=\"8M\",\n    audio_bitrate=\"192k\",\n    preset=\"medium\",\n    crf=23,\n    threads=None,\n    gpu_acceleration=True,\n    extra_ffmpeg_args=None,\n):\n    \"\"\"\n    Export the generated frames as a video\n\n    Args:\n        output_path (str): Path for the output video file\n        encoder (str): Encoding method to use: `'ffmpeg'`, `'moviepy'`, or `'auto'` (default)\n        video_codec (str, optional): Video codec to use (default: `'h264_nvenc'` for GPU, `'libx264'` for CPU)\n            See [FFmpeg H.264 Guide](https://trac.ffmpeg.org/wiki/Encode/H.264) for CPU options\n            See [NVIDIA FFmpeg Guide](https://developer.nvidia.com/blog/nvidia-ffmpeg-transcoding-guide/) for GPU options\n        audio_codec (str, optional): Audio codec to use (default: `'aac'`)\n            See [FFmpeg AAC Guide](https://trac.ffmpeg.org/wiki/Encode/AAC) for audio codec options\n        video_bitrate (str, optional): Video bitrate (default: `'8M'`)\n        audio_bitrate (str, optional): Audio bitrate (default: `'192k'`)\n        preset (str, optional): Encoding preset (default: `'medium'`)\n            For CPU encoding (libx264):\n                Options: `'ultrafast'`, `'superfast'`, `'veryfast'`, `'faster'`, `'fast'`,\n                         `'medium'`, `'slow'`, `'slower'`, `'veryslow'`\n                Slower presets give better compression/quality at the cost of encoding time.\n                See [FFmpeg Preset Guide](https://trac.ffmpeg.org/wiki/Encode/H.264#a2.Chooseapresetandtune)\n            For GPU encoding (NVENC):\n                Will be automatically converted to NVENC presets:\n                `'slow'`/`'slower'`/`'veryslow'` \u2192 `'p1'` (highest quality)\n                `'medium'` \u2192 `'p3'` (balanced)\n                `'fast'`/`'faster'` \u2192 `'p5'` (faster encoding)\n                `'veryfast'`/`'superfast'`/`'ultrafast'` \u2192 `'p7'` (fastest encoding)\n                See [NVIDIA FFmpeg Integration](https://docs.nvidia.com/video-technologies/video-codec-sdk/12.0/ffmpeg-with-nvidia-gpu/index.html)\n        crf (int, optional): Constant Rate Factor for quality (default: `23`, lower is better quality)\n            Range: `0-51`, where lower values mean better quality and larger file size\n            Recommended range: `18-28`. See [CRF Guide](https://trac.ffmpeg.org/wiki/Encode/H.264#crf)\n        threads (int, optional): Number of encoding threads (default: CPU count - 1)\n        gpu_acceleration (bool, optional): Whether to use GPU acceleration if available (default: `True`)\n        extra_ffmpeg_args (list, optional): Additional FFmpeg arguments as a list\n            See [FFmpeg Documentation](https://ffmpeg.org/ffmpeg.html) for all available options\n    \"\"\"\n\n    logger.info(f\"Starting video generation process with {self.total_frames} frames\")\n\n    # Calculate video duration\n    video_duration = self.total_frames / self.fps\n\n    # Determine audio duration if provided\n    audio_duration = None\n    if self.audio_path:\n        try:\n            # Import locally since this is a heavier dependency\n            from moviepy.editor import AudioFileClip\n\n            audio = AudioFileClip(self.audio_path)\n            audio_duration = audio.duration\n            audio.close()\n        except Exception as e:\n            logger.warning(f\"Could not determine audio duration: {e}\")\n\n    # Use the shorter duration to ensure sync\n    final_duration = video_duration\n    if audio_duration:\n        final_duration = min(video_duration, audio_duration)\n        logger.info(f\"Video duration: {final_duration:.2f}s (adjusted to match audio)\")\n    else:\n        logger.info(f\"Video duration: {final_duration:.2f}s\")\n\n    # Sort frames by number to ensure correct sequence\n    self.frame_files.sort(\n        key=lambda x: int(os.path.basename(x).split(\"_\")[1].split(\".\")[0])\n    )\n\n    # Set default threads if not specified\n    if threads is None:\n        threads = max(4, os.cpu_count() - 1)\n\n    # Determine which encoder to use\n    if encoder == \"auto\":\n        try:\n            logger.info(\"Attempting video export with FFmpeg encoding\")\n            self._export_video_with_ffmpeg(\n                output_path,\n                final_duration,\n                video_codec,\n                audio_codec,\n                video_bitrate,\n                audio_bitrate,\n                preset,\n                crf,\n                threads,\n                gpu_acceleration,\n                extra_ffmpeg_args,\n            )\n        except Exception as e:\n            logger.warning(f\"FFmpeg export failed: {e}\")\n            logger.info(\"Falling back to MoviePy for video encoding\")\n            self._export_video_with_moviepy(\n                output_path,\n                final_duration,\n                video_codec,\n                audio_codec,\n                video_bitrate,\n                audio_bitrate,\n                preset,\n                threads,\n            )\n    elif encoder == \"ffmpeg\":\n        logger.info(\"Starting video export using native FFmpeg\")\n        self._export_video_with_ffmpeg(\n            output_path,\n            final_duration,\n            video_codec,\n            audio_codec,\n            video_bitrate,\n            audio_bitrate,\n            preset,\n            crf,\n            threads,\n            gpu_acceleration,\n            extra_ffmpeg_args,\n        )\n    elif encoder == \"moviepy\":\n        logger.info(\"Starting video export using module MoviePy\")\n        self._export_video_with_moviepy(\n            output_path,\n            final_duration,\n            video_codec,\n            audio_codec,\n            video_bitrate,\n            audio_bitrate,\n            preset,\n            threads,\n        )\n    else:\n        logger.error(f\"Invalid encoder: {encoder}. Choose 'ffmpeg', 'moviepy', or 'auto'\")\n        raise ValueError(\n            f\"Invalid encoder: {encoder}. Choose 'ffmpeg', 'moviepy', or 'auto'\"\n        )\n\n    # Clean up temporary files\n    try:\n        shutil.rmtree(self.temp_dir)\n        logger.info(f\"Cleaned up temporary files in {self.temp_dir}\")\n    except Exception as e:\n        logger.warning(f\"Could not clean up temporary files: {e}\")\n\n    logger.info(f\"Video generation completed! Exported to: {output_path}\")\n    return output_path\n</code></pre>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator.generate_from_srt","title":"<code>generate_from_srt(srt_path, audio_path=None, logo_path=None, title=None, cpu_core_utilization='most')</code>","text":"<p>Generate video frames from an SRT file</p> <p>Parameters:</p> Name Type Description Default <code>srt_path</code> <code>str</code> <p>Path to the SRT file</p> required <code>audio_path</code> <code>str</code> <p>Path to the audio file</p> <code>None</code> <code>logo_path</code> <code>str</code> <p>Path to the logo image</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the video</p> <code>None</code> <code>cpu_core_utilization</code> <code>str</code> <p><code>'single'</code>, <code>'half'</code>, <code>'most'</code>, <code>'max'</code></p> <ul> <li><code>single</code>: Uses 1 CPU core</li> <li><code>half</code>: Uses half of available CPU cores</li> <li><code>most</code>: (default) Uses all available CPU cores except one</li> <li><code>max</code>: Uses all available CPU cores for maximum performance</li> </ul> <code>'most'</code> Source code in <code>audim/sub2pod/core.py</code> <pre><code>def generate_from_srt(\n    self,\n    srt_path,\n    audio_path=None,\n    logo_path=None,\n    title=None,\n    cpu_core_utilization=\"most\",\n):\n    \"\"\"\n    Generate video frames from an SRT file\n\n    Args:\n        srt_path (str): Path to the SRT file\n        audio_path (str, optional): Path to the audio file\n        logo_path (str, optional): Path to the logo image\n        title (str, optional): Title for the video\n        cpu_core_utilization (str, optional): `'single'`, `'half'`, `'most'`, `'max'`\n\n            - `single`: Uses 1 CPU core\n            - `half`: Uses half of available CPU cores\n            - `most`: (default) Uses all available CPU cores except one\n            - `max`: Uses all available CPU cores for maximum performance\n    \"\"\"\n\n    # Store paths for later use\n    self.audio_path = audio_path\n    self.logo_path = logo_path\n    self.title = title\n\n    # Update layout with logo and title\n    if hasattr(self.layout, \"logo_path\"):\n        self.layout.logo_path = logo_path\n    if hasattr(self.layout, \"title\"):\n        self.layout.title = title\n\n    # Load SRT file\n    logger.info(f\"Loading subtitles from {srt_path}\")\n    subs = pysrt.open(srt_path)\n\n    # Create temporary directory for frame storage\n    self.temp_dir = tempfile.mkdtemp()\n    self.frame_files = []\n    self.total_frames = 0\n\n    # Determine optimal number of workers\n    if cpu_core_utilization == \"single\":\n        num_workers = 1\n    elif cpu_core_utilization == \"half\":\n        num_workers = max(1, multiprocessing.cpu_count() // 2)\n    elif cpu_core_utilization == \"most\":\n        num_workers = max(1, multiprocessing.cpu_count() - 1)\n    elif cpu_core_utilization == \"max\":\n        num_workers = max(1, multiprocessing.cpu_count())\n    else:\n        raise ValueError(f\"Invalid CPU core utilities: {cpu_core_utilization}\")\n\n    logger.info(f\"Using {num_workers} CPU cores for parallel processing\")\n\n    # Process subtitles in parallel batches\n    with concurrent.futures.ProcessPoolExecutor(\n        max_workers=num_workers\n    ) as executor:\n        # Prepare subtitle batches for parallel processing\n        sub_batches = []\n        current_batch = []\n        current_batch_frames = 0\n\n        for sub in subs:\n            start_frame = sub.start.ordinal // (1000 // self.fps)\n            end_frame = sub.end.ordinal // (1000 // self.fps)\n            num_frames = (end_frame - start_frame) + min(\n                15, end_frame - start_frame\n            )  # Including fade frames\n\n            if (\n                current_batch_frames + num_frames &gt; self.batch_size\n                and current_batch\n            ):\n                sub_batches.append(current_batch)\n                current_batch = []\n                current_batch_frames = 0\n\n            current_batch.append(sub)\n            current_batch_frames += num_frames\n\n        # Add the last batch if not empty\n        if current_batch:\n            sub_batches.append(current_batch)\n\n        logger.info(f\"Processing subtitle to generate frames in {len(sub_batches)} batches\")\n\n        # Process each batch in parallel\n        batch_results = []\n        for batch_idx, batch in enumerate(sub_batches):\n            batch_results.append(\n                executor.submit(\n                    self._process_subtitle_batch,\n                    batch,\n                    batch_idx,\n                    self.layout,\n                    self.fps,\n                    self.temp_dir,\n                )\n            )\n\n        # Collect results with progress bar\n        with tqdm(total=len(batch_results), desc=\"Processing batch\", unit=\"batch\") as pbar:\n            for future in concurrent.futures.as_completed(batch_results):\n                batch_frame_files, batch_frame_count = future.result()\n                self.frame_files.extend(batch_frame_files)\n                self.total_frames += batch_frame_count\n                pbar.update(1)\n                pbar.set_postfix({\"frames processed\": self.total_frames})\n\n    # Sort frame files by frame number to ensure correct sequence\n    self.frame_files.sort(\n        key=lambda x: int(os.path.basename(x).split(\"_\")[1].split(\".\")[0])\n    )\n\n    logger.info(f\"Frame generation completed: Total {self.total_frames} frames created\")\n    return self\n</code></pre>"},{"location":"audim/sub2pod/elements/header/","title":"Header","text":"<p>The header element is a component that is usually used at the topmost position in the layout. It is responsible for displaying the podcast title and the host's profile picture.</p> <p>Below is the API documentation for the header element:</p>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header","title":"<code>Header</code>","text":"<p>Header component for podcast layouts</p> <p>This component is responsible for displaying the header at the top of the podcast video frame. It may include various elements like logo, title, host profile, guest profile, etc.</p> Source code in <code>audim/sub2pod/elements/header.py</code> <pre><code>class Header:\n    \"\"\"\n    Header component for podcast layouts\n\n    This component is responsible for displaying the header at the top of the\n    podcast video frame. It may include various elements like logo, title,\n    host profile, guest profile, etc.\n    \"\"\"\n\n    def __init__(self, height=150, background_color=(30, 30, 30)):\n        \"\"\"\n        Initialize the layout header\n\n        Args:\n            height (int): Height of the header, defaults to 150\n            background_color (tuple): RGB background color, defaults to RGB (30, 30, 30)\n            text_renderer (TextRenderer): optional text renderer for the header,\n                                          defaults to a new instance\n            logo (Image): optional logo image, defaults to None\n            logo_size (tuple): optional size of the logo, defaults to (100, 100)\n        \"\"\"\n\n        self.height = height\n        self.background_color = background_color\n        self.text_renderer = TextRenderer()\n        self.logo = None\n        self.logo_size = (100, 100)\n\n    def set_logo(self, logo_path, size=(100, 100)):\n        \"\"\"\n        Set the logo for the header\n\n        Args:\n            logo_path (str): Path to the logo image\n            size (tuple): Size of the logo, defaults to (100, 100)\n        \"\"\"\n\n        if logo_path:\n            self.logo = Image.open(logo_path).convert(\"RGBA\")\n            self.logo_size = size\n            self.logo = self.logo.resize(self.logo_size)\n        return self\n\n    def draw(self, frame, draw, width, title=\"My Podcast\", opacity=255):\n        \"\"\"\n        Draws the header on the frame\n\n        Args:\n            frame (Image): Frame to draw the header on\n            draw (ImageDraw): Draw object to draw on the frame\n            width (int): Width of the frame\n            title (str): Title of the podcast\n            opacity (int): Opacity of the header, defaults to 255\n        \"\"\"\n\n        # Draw header background\n        draw.rectangle([0, 0, width, self.height], fill=self.background_color + (255,))\n\n        # Add logo if available\n        if self.logo:\n            frame.paste(\n                self.logo,\n                (\n                    width - self.logo_size[0] - 50,\n                    (self.height - self.logo_size[1]) // 2,\n                ),\n                self.logo,\n            )\n\n        # Add title\n        self.text_renderer.draw_text(\n            draw,\n            title,\n            (width // 2, self.height // 2),\n            font_size=60,\n            color=(255, 255, 255, opacity),\n            anchor=\"mm\",\n        )\n</code></pre>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header.__init__","title":"<code>__init__(height=150, background_color=(30, 30, 30))</code>","text":"<p>Initialize the layout header</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Height of the header, defaults to 150</p> <code>150</code> <code>background_color</code> <code>tuple</code> <p>RGB background color, defaults to RGB (30, 30, 30)</p> <code>(30, 30, 30)</code> <code>text_renderer</code> <code>TextRenderer</code> <p>optional text renderer for the header,                           defaults to a new instance</p> required <code>logo</code> <code>Image</code> <p>optional logo image, defaults to None</p> required <code>logo_size</code> <code>tuple</code> <p>optional size of the logo, defaults to (100, 100)</p> required Source code in <code>audim/sub2pod/elements/header.py</code> <pre><code>def __init__(self, height=150, background_color=(30, 30, 30)):\n    \"\"\"\n    Initialize the layout header\n\n    Args:\n        height (int): Height of the header, defaults to 150\n        background_color (tuple): RGB background color, defaults to RGB (30, 30, 30)\n        text_renderer (TextRenderer): optional text renderer for the header,\n                                      defaults to a new instance\n        logo (Image): optional logo image, defaults to None\n        logo_size (tuple): optional size of the logo, defaults to (100, 100)\n    \"\"\"\n\n    self.height = height\n    self.background_color = background_color\n    self.text_renderer = TextRenderer()\n    self.logo = None\n    self.logo_size = (100, 100)\n</code></pre>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header.draw","title":"<code>draw(frame, draw, width, title='My Podcast', opacity=255)</code>","text":"<p>Draws the header on the frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>Image</code> <p>Frame to draw the header on</p> required <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>width</code> <code>int</code> <p>Width of the frame</p> required <code>title</code> <code>str</code> <p>Title of the podcast</p> <code>'My Podcast'</code> <code>opacity</code> <code>int</code> <p>Opacity of the header, defaults to 255</p> <code>255</code> Source code in <code>audim/sub2pod/elements/header.py</code> <pre><code>def draw(self, frame, draw, width, title=\"My Podcast\", opacity=255):\n    \"\"\"\n    Draws the header on the frame\n\n    Args:\n        frame (Image): Frame to draw the header on\n        draw (ImageDraw): Draw object to draw on the frame\n        width (int): Width of the frame\n        title (str): Title of the podcast\n        opacity (int): Opacity of the header, defaults to 255\n    \"\"\"\n\n    # Draw header background\n    draw.rectangle([0, 0, width, self.height], fill=self.background_color + (255,))\n\n    # Add logo if available\n    if self.logo:\n        frame.paste(\n            self.logo,\n            (\n                width - self.logo_size[0] - 50,\n                (self.height - self.logo_size[1]) // 2,\n            ),\n            self.logo,\n        )\n\n    # Add title\n    self.text_renderer.draw_text(\n        draw,\n        title,\n        (width // 2, self.height // 2),\n        font_size=60,\n        color=(255, 255, 255, opacity),\n        anchor=\"mm\",\n    )\n</code></pre>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header.set_logo","title":"<code>set_logo(logo_path, size=(100, 100))</code>","text":"<p>Set the logo for the header</p> <p>Parameters:</p> Name Type Description Default <code>logo_path</code> <code>str</code> <p>Path to the logo image</p> required <code>size</code> <code>tuple</code> <p>Size of the logo, defaults to (100, 100)</p> <code>(100, 100)</code> Source code in <code>audim/sub2pod/elements/header.py</code> <pre><code>def set_logo(self, logo_path, size=(100, 100)):\n    \"\"\"\n    Set the logo for the header\n\n    Args:\n        logo_path (str): Path to the logo image\n        size (tuple): Size of the logo, defaults to (100, 100)\n    \"\"\"\n\n    if logo_path:\n        self.logo = Image.open(logo_path).convert(\"RGBA\")\n        self.logo_size = size\n        self.logo = self.logo.resize(self.logo_size)\n    return self\n</code></pre>"},{"location":"audim/sub2pod/elements/profile/","title":"Profile","text":"<p>The profile element is a component that is used to display the profile picture (DP) of the host.</p> <p>Below is the API documentation for the profile element:</p>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture","title":"<code>ProfilePicture</code>","text":"<p>Handles user profile pictures or display picture with various shapes and effects</p> <p>This component is responsible for displaying the profile picture of the speaker. It may also include various shapes and effects like circle, square, highlight, etc.</p> Source code in <code>audim/sub2pod/elements/profile.py</code> <pre><code>class ProfilePicture:\n    \"\"\"\n    Handles user profile pictures or display picture with various shapes and effects\n\n    This component is responsible for displaying the profile picture of the speaker.\n    It may also include various shapes and effects like circle, square, highlight, etc.\n    \"\"\"\n\n    def __init__(self, image_path, size=(120, 120), shape=\"circle\"):\n        \"\"\"\n        Initialize a profile picture\n\n        Args:\n            image_path (str): Path to the profile image\n            size (tuple): Width and height of the profile picture\n            shape (str): Shape of the profile picture (\"circle\" or \"square\"),\n            defaults to \"circle\"\n        \"\"\"\n\n        self.image_path = image_path\n        self.size = size\n        self.shape = shape\n        self.image = self._load_and_process_image()\n\n    def _load_and_process_image(self):\n        \"\"\"\n        Load and process the profile image based on shape\n        (mostly for internal use)\n\n        Returns:\n            Image: Processed profile image\n        \"\"\"\n\n        img = Image.open(self.image_path).convert(\"RGBA\")\n        img = img.resize(self.size)\n\n        if self.shape == \"circle\":\n            mask = self._create_circular_mask()\n            img.putalpha(mask)\n        elif self.shape == \"square\":\n            mask = self._create_square_mask()\n            img.putalpha(mask)\n\n        return img\n\n    def _create_circular_mask(self):\n        \"\"\"\n        Create a circular mask for profile pictures\n        (mostly for internal use)\n\n        Returns:\n            Image: Circular mask\n        \"\"\"\n\n        mask = Image.new(\"L\", self.size, 0)\n        draw = ImageDraw.Draw(mask)\n        draw.ellipse((0, 0) + self.size, fill=255)\n        return mask\n\n    def _create_square_mask(self):\n        \"\"\"\n        Create a square mask for profile pictures\n        (mostly for internal use)\n\n        Returns:\n            Image: Square mask\n        \"\"\"\n\n        mask = Image.new(\"L\", self.size, 0)\n        draw = ImageDraw.Draw(mask)\n        draw.rectangle((0, 0) + self.size, fill=255)\n        return mask\n\n    def highlight(self, draw, position, color=(255, 200, 0), width=3, opacity=255):\n        \"\"\"\n        Add highlight around the profile picture\n\n        Args:\n            draw (ImageDraw): Draw object to draw on the frame\n            position (tuple): Position of the profile picture\n        \"\"\"\n\n        highlight_color = color + (opacity,)\n\n        if self.shape == \"circle\":\n            draw.ellipse(\n                [\n                    position[0] - width,\n                    position[1] - width,\n                    position[0] + self.size[0] + width,\n                    position[1] + self.size[1] + width,\n                ],\n                outline=highlight_color,\n                width=width,\n            )\n        else:\n            draw.rectangle(\n                [\n                    position[0] - width,\n                    position[1] - width,\n                    position[0] + self.size[0] + width,\n                    position[1] + self.size[1] + width,\n                ],\n                outline=highlight_color,\n                width=width,\n            )\n</code></pre>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture.__init__","title":"<code>__init__(image_path, size=(120, 120), shape='circle')</code>","text":"<p>Initialize a profile picture</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the profile image</p> required <code>size</code> <code>tuple</code> <p>Width and height of the profile picture</p> <code>(120, 120)</code> <code>shape</code> <code>str</code> <p>Shape of the profile picture (\"circle\" or \"square\"),</p> <code>'circle'</code> Source code in <code>audim/sub2pod/elements/profile.py</code> <pre><code>def __init__(self, image_path, size=(120, 120), shape=\"circle\"):\n    \"\"\"\n    Initialize a profile picture\n\n    Args:\n        image_path (str): Path to the profile image\n        size (tuple): Width and height of the profile picture\n        shape (str): Shape of the profile picture (\"circle\" or \"square\"),\n        defaults to \"circle\"\n    \"\"\"\n\n    self.image_path = image_path\n    self.size = size\n    self.shape = shape\n    self.image = self._load_and_process_image()\n</code></pre>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture._create_circular_mask","title":"<code>_create_circular_mask()</code>","text":"<p>Create a circular mask for profile pictures (mostly for internal use)</p> <p>Returns:</p> Name Type Description <code>Image</code> <p>Circular mask</p> Source code in <code>audim/sub2pod/elements/profile.py</code> <pre><code>def _create_circular_mask(self):\n    \"\"\"\n    Create a circular mask for profile pictures\n    (mostly for internal use)\n\n    Returns:\n        Image: Circular mask\n    \"\"\"\n\n    mask = Image.new(\"L\", self.size, 0)\n    draw = ImageDraw.Draw(mask)\n    draw.ellipse((0, 0) + self.size, fill=255)\n    return mask\n</code></pre>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture._create_square_mask","title":"<code>_create_square_mask()</code>","text":"<p>Create a square mask for profile pictures (mostly for internal use)</p> <p>Returns:</p> Name Type Description <code>Image</code> <p>Square mask</p> Source code in <code>audim/sub2pod/elements/profile.py</code> <pre><code>def _create_square_mask(self):\n    \"\"\"\n    Create a square mask for profile pictures\n    (mostly for internal use)\n\n    Returns:\n        Image: Square mask\n    \"\"\"\n\n    mask = Image.new(\"L\", self.size, 0)\n    draw = ImageDraw.Draw(mask)\n    draw.rectangle((0, 0) + self.size, fill=255)\n    return mask\n</code></pre>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture._load_and_process_image","title":"<code>_load_and_process_image()</code>","text":"<p>Load and process the profile image based on shape (mostly for internal use)</p> <p>Returns:</p> Name Type Description <code>Image</code> <p>Processed profile image</p> Source code in <code>audim/sub2pod/elements/profile.py</code> <pre><code>def _load_and_process_image(self):\n    \"\"\"\n    Load and process the profile image based on shape\n    (mostly for internal use)\n\n    Returns:\n        Image: Processed profile image\n    \"\"\"\n\n    img = Image.open(self.image_path).convert(\"RGBA\")\n    img = img.resize(self.size)\n\n    if self.shape == \"circle\":\n        mask = self._create_circular_mask()\n        img.putalpha(mask)\n    elif self.shape == \"square\":\n        mask = self._create_square_mask()\n        img.putalpha(mask)\n\n    return img\n</code></pre>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture.highlight","title":"<code>highlight(draw, position, color=(255, 200, 0), width=3, opacity=255)</code>","text":"<p>Add highlight around the profile picture</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>position</code> <code>tuple</code> <p>Position of the profile picture</p> required Source code in <code>audim/sub2pod/elements/profile.py</code> <pre><code>def highlight(self, draw, position, color=(255, 200, 0), width=3, opacity=255):\n    \"\"\"\n    Add highlight around the profile picture\n\n    Args:\n        draw (ImageDraw): Draw object to draw on the frame\n        position (tuple): Position of the profile picture\n    \"\"\"\n\n    highlight_color = color + (opacity,)\n\n    if self.shape == \"circle\":\n        draw.ellipse(\n            [\n                position[0] - width,\n                position[1] - width,\n                position[0] + self.size[0] + width,\n                position[1] + self.size[1] + width,\n            ],\n            outline=highlight_color,\n            width=width,\n        )\n    else:\n        draw.rectangle(\n            [\n                position[0] - width,\n                position[1] - width,\n                position[0] + self.size[0] + width,\n                position[1] + self.size[1] + width,\n            ],\n            outline=highlight_color,\n            width=width,\n        )\n</code></pre>"},{"location":"audim/sub2pod/elements/text/","title":"Text","text":"<p>The text element is a component that is used to display the text, dialogue, or any other text content in the podcast.</p> <p>Below is the API documentation for the text element:</p>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer","title":"<code>TextRenderer</code>","text":"<p>Handles text rendering with various styles and wrapping</p> <p>This component is responsible for rendering text on the frame with various styles and wrapping. It can handle different fonts, sizes, colors, and anchor points.</p> Source code in <code>audim/sub2pod/elements/text.py</code> <pre><code>class TextRenderer:\n    \"\"\"\n    Handles text rendering with various styles and wrapping\n\n    This component is responsible for rendering text on the frame with various styles\n    and wrapping. It can handle different fonts, sizes, colors, and anchor points.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the text renderer with default fonts\n        \"\"\"\n\n        self.font_path = font_manager.findfont(\n            font_manager.FontProperties(family=[\"sans\"])\n        )\n        self.fonts = {}\n\n    def get_font(self, size):\n        \"\"\"\n        Get or create a font of the specified size\n\n        Args:\n            size (int): Size of the font\n        \"\"\"\n\n        if size not in self.fonts:\n            self.fonts[size] = ImageFont.truetype(self.font_path, size)\n        return self.fonts[size]\n\n    def draw_text(\n        self,\n        draw,\n        text,\n        position,\n        font_size=40,\n        color=(255, 255, 255, 255),\n        anchor=\"mm\",\n    ):\n        \"\"\"\n        Draw text at the specified position\n\n        Args:\n            draw (ImageDraw): Draw object to draw on the frame\n            text (str): Text to draw\n            position (tuple): Position of the text\n            font_size (int): Size of the font, defaults to 40\n            color (tuple): Color of the text, defaults to RGB (255, 255, 255, 255)\n            anchor (str): Anchor of the text (from PIL library), defaults to \"mm\".\n                          See [pillow docs: text anchors](https://pillow.readthedocs.io/en/latest/handbook/text-anchors.html)\n                          for all possible options.\n        \"\"\"\n\n        font = self.get_font(font_size)\n        draw.text(position, text, fill=color, font=font, anchor=anchor)\n\n    def draw_wrapped_text(\n        self,\n        draw,\n        text,\n        position,\n        max_width,\n        font_size=40,\n        color=(255, 255, 255, 255),\n        anchor=\"lm\",\n    ):\n        \"\"\"\n        Draw text with word wrapping\n\n        Args:\n            draw (ImageDraw): Draw object to draw on the frame\n            text (str): Text to draw\n            position (tuple): Position of the text\n            max_width (int): Maximum width of the text before wrapping\n            font_size (int): Size of the font, defaults to 40\n            color (tuple): Color of the text, defaults to RGB (255, 255, 255, 255)\n            anchor (str): Anchor of the text (from PIL library), defaults to \"lm\".\n                          See [pillow docs: text anchors](https://pillow.readthedocs.io/en/latest/handbook/text-anchors.html)\n                          for all possible options.\n        \"\"\"\n\n        font = self.get_font(font_size)\n\n        # Get font metrics for dynamic calculations\n        font_ascent, font_descent = font.getmetrics()\n        line_height = font_ascent + font_descent\n        line_spacing = line_height * 0.5  # 50% of line height for spacing\n        total_line_height = line_height + line_spacing\n\n        # Word wrap\n        words = text.split()\n        lines = []\n        current_line = []\n\n        for word in words:\n            current_line.append(word)\n            w = draw.textlength(\" \".join(current_line), font=font)\n            if w &gt; max_width:\n                current_line.pop()\n                lines.append(\" \".join(current_line))\n                current_line = [word]\n        lines.append(\" \".join(current_line))\n\n        # Calculate vertical offset for multiple lines to maintain center alignment\n        text_x, text_y = position\n        total_text_height = len(lines) * total_line_height\n\n        if anchor == \"lm\":\n            text_start_y = text_y - (total_text_height // 2) + (line_height // 2)\n        else:\n            text_start_y = text_y\n\n        # Draw each line\n        for i, line in enumerate(lines):\n            line_y = text_start_y + (i * total_line_height)\n            draw.text((text_x, line_y), line, fill=color, font=font, anchor=anchor)\n</code></pre>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the text renderer with default fonts</p> Source code in <code>audim/sub2pod/elements/text.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the text renderer with default fonts\n    \"\"\"\n\n    self.font_path = font_manager.findfont(\n        font_manager.FontProperties(family=[\"sans\"])\n    )\n    self.fonts = {}\n</code></pre>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.draw_text","title":"<code>draw_text(draw, text, position, font_size=40, color=(255, 255, 255, 255), anchor='mm')</code>","text":"<p>Draw text at the specified position</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>text</code> <code>str</code> <p>Text to draw</p> required <code>position</code> <code>tuple</code> <p>Position of the text</p> required <code>font_size</code> <code>int</code> <p>Size of the font, defaults to 40</p> <code>40</code> <code>color</code> <code>tuple</code> <p>Color of the text, defaults to RGB (255, 255, 255, 255)</p> <code>(255, 255, 255, 255)</code> <code>anchor</code> <code>str</code> <p>Anchor of the text (from PIL library), defaults to \"mm\".           See pillow docs: text anchors           for all possible options.</p> <code>'mm'</code> Source code in <code>audim/sub2pod/elements/text.py</code> <pre><code>def draw_text(\n    self,\n    draw,\n    text,\n    position,\n    font_size=40,\n    color=(255, 255, 255, 255),\n    anchor=\"mm\",\n):\n    \"\"\"\n    Draw text at the specified position\n\n    Args:\n        draw (ImageDraw): Draw object to draw on the frame\n        text (str): Text to draw\n        position (tuple): Position of the text\n        font_size (int): Size of the font, defaults to 40\n        color (tuple): Color of the text, defaults to RGB (255, 255, 255, 255)\n        anchor (str): Anchor of the text (from PIL library), defaults to \"mm\".\n                      See [pillow docs: text anchors](https://pillow.readthedocs.io/en/latest/handbook/text-anchors.html)\n                      for all possible options.\n    \"\"\"\n\n    font = self.get_font(font_size)\n    draw.text(position, text, fill=color, font=font, anchor=anchor)\n</code></pre>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.draw_wrapped_text","title":"<code>draw_wrapped_text(draw, text, position, max_width, font_size=40, color=(255, 255, 255, 255), anchor='lm')</code>","text":"<p>Draw text with word wrapping</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>text</code> <code>str</code> <p>Text to draw</p> required <code>position</code> <code>tuple</code> <p>Position of the text</p> required <code>max_width</code> <code>int</code> <p>Maximum width of the text before wrapping</p> required <code>font_size</code> <code>int</code> <p>Size of the font, defaults to 40</p> <code>40</code> <code>color</code> <code>tuple</code> <p>Color of the text, defaults to RGB (255, 255, 255, 255)</p> <code>(255, 255, 255, 255)</code> <code>anchor</code> <code>str</code> <p>Anchor of the text (from PIL library), defaults to \"lm\".           See pillow docs: text anchors           for all possible options.</p> <code>'lm'</code> Source code in <code>audim/sub2pod/elements/text.py</code> <pre><code>def draw_wrapped_text(\n    self,\n    draw,\n    text,\n    position,\n    max_width,\n    font_size=40,\n    color=(255, 255, 255, 255),\n    anchor=\"lm\",\n):\n    \"\"\"\n    Draw text with word wrapping\n\n    Args:\n        draw (ImageDraw): Draw object to draw on the frame\n        text (str): Text to draw\n        position (tuple): Position of the text\n        max_width (int): Maximum width of the text before wrapping\n        font_size (int): Size of the font, defaults to 40\n        color (tuple): Color of the text, defaults to RGB (255, 255, 255, 255)\n        anchor (str): Anchor of the text (from PIL library), defaults to \"lm\".\n                      See [pillow docs: text anchors](https://pillow.readthedocs.io/en/latest/handbook/text-anchors.html)\n                      for all possible options.\n    \"\"\"\n\n    font = self.get_font(font_size)\n\n    # Get font metrics for dynamic calculations\n    font_ascent, font_descent = font.getmetrics()\n    line_height = font_ascent + font_descent\n    line_spacing = line_height * 0.5  # 50% of line height for spacing\n    total_line_height = line_height + line_spacing\n\n    # Word wrap\n    words = text.split()\n    lines = []\n    current_line = []\n\n    for word in words:\n        current_line.append(word)\n        w = draw.textlength(\" \".join(current_line), font=font)\n        if w &gt; max_width:\n            current_line.pop()\n            lines.append(\" \".join(current_line))\n            current_line = [word]\n    lines.append(\" \".join(current_line))\n\n    # Calculate vertical offset for multiple lines to maintain center alignment\n    text_x, text_y = position\n    total_text_height = len(lines) * total_line_height\n\n    if anchor == \"lm\":\n        text_start_y = text_y - (total_text_height // 2) + (line_height // 2)\n    else:\n        text_start_y = text_y\n\n    # Draw each line\n    for i, line in enumerate(lines):\n        line_y = text_start_y + (i * total_line_height)\n        draw.text((text_x, line_y), line, fill=color, font=font, anchor=anchor)\n</code></pre>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.get_font","title":"<code>get_font(size)</code>","text":"<p>Get or create a font of the specified size</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Size of the font</p> required Source code in <code>audim/sub2pod/elements/text.py</code> <pre><code>def get_font(self, size):\n    \"\"\"\n    Get or create a font of the specified size\n\n    Args:\n        size (int): Size of the font\n    \"\"\"\n\n    if size not in self.fonts:\n        self.fonts[size] = ImageFont.truetype(self.font_path, size)\n    return self.fonts[size]\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/","title":"Base Layout","text":"<p>The base layout sets the base layout for the podcast.</p> <p>It must be overriden to create various layout structures placing various elements in different places in the scenes. This would determine how the video frames would look like in the podcast.</p> <p>Below is the API documentation for the base layout:</p>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout","title":"<code>BaseLayout</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all layouts</p> <p>This class defines the base structure for all layout classes. It provides a common interface for adding speakers and creating frames and scenes.</p> Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>class BaseLayout(ABC):\n    \"\"\"\n    Base class for all layouts\n\n    This class defines the base structure for all layout classes.\n    It provides a common interface for adding speakers and creating frames and scenes.\n    \"\"\"\n\n    def __init__(self, video_width=1920, video_height=1080):\n        \"\"\"\n        Initialize the base layout\n\n        Args:\n            video_width (int): Width of the video\n            video_height (int): Height of the video\n        \"\"\"\n\n        self.video_width = video_width\n        self.video_height = video_height\n\n    @abstractmethod\n    def add_speaker(self, name, image_path):\n        \"\"\"\n        Add a speaker to the layout\n\n        Args:\n            name (str): Name of the speaker\n            image_path (str): Path to the speaker's image\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def create_frame(self, current_sub=None, opacity=255):\n        \"\"\"\n        Create a frame with the current subtitle\n\n        Args:\n            current_sub (str): Current subtitle\n            opacity (int): Opacity of the subtitle\n        \"\"\"\n\n        pass\n\n    def _create_base_frame(self, background_color=(20, 20, 20)):\n        \"\"\"\n        Create a base frame with the specified background color\n        (mostly for internal use)\n\n        Args:\n            background_color (tuple): Background color in RGB format\n        \"\"\"\n\n        frame = Image.new(\n            \"RGBA\", (self.video_width, self.video_height), background_color + (255,)\n        )\n        draw = ImageDraw.Draw(frame)\n        return frame, draw\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.__init__","title":"<code>__init__(video_width=1920, video_height=1080)</code>","text":"<p>Initialize the base layout</p> <p>Parameters:</p> Name Type Description Default <code>video_width</code> <code>int</code> <p>Width of the video</p> <code>1920</code> <code>video_height</code> <code>int</code> <p>Height of the video</p> <code>1080</code> Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def __init__(self, video_width=1920, video_height=1080):\n    \"\"\"\n    Initialize the base layout\n\n    Args:\n        video_width (int): Width of the video\n        video_height (int): Height of the video\n    \"\"\"\n\n    self.video_width = video_width\n    self.video_height = video_height\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout._create_base_frame","title":"<code>_create_base_frame(background_color=(20, 20, 20))</code>","text":"<p>Create a base frame with the specified background color (mostly for internal use)</p> <p>Parameters:</p> Name Type Description Default <code>background_color</code> <code>tuple</code> <p>Background color in RGB format</p> <code>(20, 20, 20)</code> Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def _create_base_frame(self, background_color=(20, 20, 20)):\n    \"\"\"\n    Create a base frame with the specified background color\n    (mostly for internal use)\n\n    Args:\n        background_color (tuple): Background color in RGB format\n    \"\"\"\n\n    frame = Image.new(\n        \"RGBA\", (self.video_width, self.video_height), background_color + (255,)\n    )\n    draw = ImageDraw.Draw(frame)\n    return frame, draw\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.add_speaker","title":"<code>add_speaker(name, image_path)</code>  <code>abstractmethod</code>","text":"<p>Add a speaker to the layout</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the speaker</p> required <code>image_path</code> <code>str</code> <p>Path to the speaker's image</p> required Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>@abstractmethod\ndef add_speaker(self, name, image_path):\n    \"\"\"\n    Add a speaker to the layout\n\n    Args:\n        name (str): Name of the speaker\n        image_path (str): Path to the speaker's image\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.create_frame","title":"<code>create_frame(current_sub=None, opacity=255)</code>  <code>abstractmethod</code>","text":"<p>Create a frame with the current subtitle</p> <p>Parameters:</p> Name Type Description Default <code>current_sub</code> <code>str</code> <p>Current subtitle</p> <code>None</code> <code>opacity</code> <code>int</code> <p>Opacity of the subtitle</p> <code>255</code> Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>@abstractmethod\ndef create_frame(self, current_sub=None, opacity=255):\n    \"\"\"\n    Create a frame with the current subtitle\n\n    Args:\n        current_sub (str): Current subtitle\n        opacity (int): Opacity of the subtitle\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/","title":"Podcast Layout","text":"<p>The podcast layout is designed to create dynamic scenes for the podcast.</p> <p>Typically, it features:</p> <ul> <li>profile pictures of the speakers (which are highlighted when they are speaking)</li> <li>text dialogues of the speakers (which is displayed in real-time as they speak)</li> </ul> <p>Below is the API documentation for the podcast layout:</p>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout","title":"<code>PodcastLayout</code>","text":"<p>               Bases: <code>BaseLayout</code></p> <p>Standard podcast layout with profile pictures and subtitles</p> <p>This layout is designed for standard podcast videos with a header section, profile pictures, and subtitles. It provides a flexible structure for adding speakers and creating frames with customizable parameters.</p> Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>class PodcastLayout(BaseLayout):\n    \"\"\"\n    Standard podcast layout with profile pictures and subtitles\n\n    This layout is designed for standard podcast videos with a header section,\n    profile pictures, and subtitles. It provides a flexible structure for adding\n    speakers and creating frames with customizable parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        video_width=1920,\n        video_height=1080,\n        header_height=150,\n        dp_size=(120, 120),\n        show_speaker_names=True,\n    ):\n        \"\"\"\n        Initialize podcast layout\n\n        Args:\n            video_width (int): Width of the video\n            video_height (int): Height of the video\n            header_height (int): Height of the header section\n            dp_size (tuple): Size of profile pictures\n            show_speaker_names (bool): Whether to show speaker names\n        \"\"\"\n\n        super().__init__(video_width, video_height)\n\n        # Layout parameters\n        self.header_height = header_height\n        self.dp_size = dp_size\n        self.show_speaker_names = show_speaker_names\n        self.dp_margin_left = 40\n        self.text_margin = 50\n        self.name_margin = 30\n\n        # Initialize components\n        self.header = Header(height=header_height)\n        self.text_renderer = TextRenderer()\n\n        # Store profile pictures and positions\n        self.speakers = {}\n        self.dp_positions = {}\n        self.logo_path = None\n        self.title = \"My Podcast\"\n\n    def _calculate_positions(self):\n        \"\"\"\n        Calculate positions for all speakers based on the number of speakers and\n        the size of the profile pictures. (mostly for internal use)\n\n        This method calculates the positions of all speakers based on the number of\n        speakers and the size of the profile pictures. It also takes into account\n        the spacing between the speakers and the header height.\n        \"\"\"\n\n        num_speakers = len(self.speakers)\n        spacing, start_y = self._calculate_layout(num_speakers)\n\n        for i, speaker in enumerate(self.speakers.keys()):\n            y_pos = start_y + (i * (self.dp_size[1] + spacing))\n            self.dp_positions[speaker] = (self.dp_margin_left, y_pos)\n\n    def _calculate_layout(self, num_speakers, min_spacing=40):\n        \"\"\"\n        Calculate dynamic spacing for speaker rows\n        (mostly for internal use)\n\n        This method calculates the spacing between the speakers based on the number of\n        speakers and the size of the profile pictures. It also takes into account the\n        spacing between the speakers and the header height.\n\n        Args:\n            num_speakers (int): Number of speakers\n            min_spacing (int): Minimum spacing between the speakers, defaults to 40\n        \"\"\"\n\n        available_height = self.video_height - self.header_height\n        total_dp_height = num_speakers * self.dp_size[1]\n\n        # Calculate spacing between DPs\n        num_spaces = num_speakers + 1\n        spacing = (available_height - total_dp_height) // num_spaces\n        spacing = max(spacing, min_spacing)\n\n        # Calculate starting Y position\n        start_y = self.header_height + spacing\n\n        return spacing, start_y\n\n    def _draw_subtitle(self, frame, draw, subtitle, opacity):\n        \"\"\"\n        Draw the current subtitle with speaker highlighting\n        (mostly for internal use)\n\n        This method draws the current subtitle with speaker highlighting.\n        It highlights the active speaker and draws the subtitle text.\n\n        Args:\n            frame (Image): Frame to draw on\n            draw (ImageDraw): Draw object to draw on the frame\n            subtitle (Subtitle): Current subtitle\n            opacity (int): Opacity of the subtitle\n        \"\"\"\n\n        speaker, text = subtitle.text.split(\"] \")\n        speaker = speaker.replace(\"[\", \"\").strip()\n\n        # Highlight active speaker\n        if speaker in self.speakers:\n            highlight_color = (255, 200, 0)\n            speaker_pos = self.dp_positions[speaker]\n            self.speakers[speaker].highlight(\n                draw, speaker_pos, color=highlight_color, opacity=opacity\n            )\n\n            # Calculate text position\n            text_x = self.dp_margin_left + self.dp_size[0] + self.text_margin\n            text_y = speaker_pos[1] + (self.dp_size[1] // 2)\n            text_width = self.video_width - text_x - self.text_margin\n\n            # Draw the subtitle text\n            self.text_renderer.draw_wrapped_text(\n                draw,\n                text,\n                (text_x, text_y),\n                max_width=text_width,\n                font_size=40,\n                color=(255, 255, 255, opacity),\n                anchor=\"lm\",\n            )\n\n    def add_speaker(self, name, image_path, shape=\"circle\"):\n        \"\"\"\n        Add a speaker to the layout\n\n        Args:\n            name (str): Name of the speaker\n            image_path (str): Path to the speaker's image\n            shape (str): Shape of the profile picture, defaults to \"circle\"\n        \"\"\"\n\n        self.speakers[name] = ProfilePicture(image_path, self.dp_size, shape)\n\n        # Recalculate positions when speakers are added\n        self._calculate_positions()\n\n        return self\n\n    def create_frame(\n        self, current_sub=None, opacity=255, background_color=(20, 20, 20)\n    ):\n        \"\"\"\n        Create a frame with the podcast layout\n\n        Args:\n            current_sub (str): Current subtitle\n            opacity (int): Opacity of the subtitle\n            background_color (tuple): Background color in RGB format,\n                                      defaults to (20, 20, 20)\n        \"\"\"\n\n        # Create base frame\n        frame, draw = self._create_base_frame(background_color)\n\n        # Draw header\n        if self.logo_path:\n            self.header.set_logo(self.logo_path)\n        self.header.draw(frame, draw, self.video_width, self.title, opacity)\n\n        # Add all speaker DPs and names\n        for speaker, profile in self.speakers.items():\n            pos = self.dp_positions[speaker]\n            frame.paste(profile.image, pos, profile.image)\n\n            # Draw speaker name if enabled\n            if self.show_speaker_names:\n                name_y = pos[1] + self.dp_size[1] + self.name_margin\n                self.text_renderer.draw_text(\n                    draw,\n                    speaker,\n                    (pos[0] + self.dp_size[0] // 2, name_y),\n                    font_size=30,\n                    color=(200, 200, 200, opacity),\n                    anchor=\"mm\",\n                )\n\n        # Add subtitle if there's a current subtitle\n        if current_sub:\n            self._draw_subtitle(frame, draw, current_sub, opacity)\n\n        return np.array(frame)\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.__init__","title":"<code>__init__(video_width=1920, video_height=1080, header_height=150, dp_size=(120, 120), show_speaker_names=True)</code>","text":"<p>Initialize podcast layout</p> <p>Parameters:</p> Name Type Description Default <code>video_width</code> <code>int</code> <p>Width of the video</p> <code>1920</code> <code>video_height</code> <code>int</code> <p>Height of the video</p> <code>1080</code> <code>header_height</code> <code>int</code> <p>Height of the header section</p> <code>150</code> <code>dp_size</code> <code>tuple</code> <p>Size of profile pictures</p> <code>(120, 120)</code> <code>show_speaker_names</code> <code>bool</code> <p>Whether to show speaker names</p> <code>True</code> Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>def __init__(\n    self,\n    video_width=1920,\n    video_height=1080,\n    header_height=150,\n    dp_size=(120, 120),\n    show_speaker_names=True,\n):\n    \"\"\"\n    Initialize podcast layout\n\n    Args:\n        video_width (int): Width of the video\n        video_height (int): Height of the video\n        header_height (int): Height of the header section\n        dp_size (tuple): Size of profile pictures\n        show_speaker_names (bool): Whether to show speaker names\n    \"\"\"\n\n    super().__init__(video_width, video_height)\n\n    # Layout parameters\n    self.header_height = header_height\n    self.dp_size = dp_size\n    self.show_speaker_names = show_speaker_names\n    self.dp_margin_left = 40\n    self.text_margin = 50\n    self.name_margin = 30\n\n    # Initialize components\n    self.header = Header(height=header_height)\n    self.text_renderer = TextRenderer()\n\n    # Store profile pictures and positions\n    self.speakers = {}\n    self.dp_positions = {}\n    self.logo_path = None\n    self.title = \"My Podcast\"\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout._calculate_layout","title":"<code>_calculate_layout(num_speakers, min_spacing=40)</code>","text":"<p>Calculate dynamic spacing for speaker rows (mostly for internal use)</p> <p>This method calculates the spacing between the speakers based on the number of speakers and the size of the profile pictures. It also takes into account the spacing between the speakers and the header height.</p> <p>Parameters:</p> Name Type Description Default <code>num_speakers</code> <code>int</code> <p>Number of speakers</p> required <code>min_spacing</code> <code>int</code> <p>Minimum spacing between the speakers, defaults to 40</p> <code>40</code> Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>def _calculate_layout(self, num_speakers, min_spacing=40):\n    \"\"\"\n    Calculate dynamic spacing for speaker rows\n    (mostly for internal use)\n\n    This method calculates the spacing between the speakers based on the number of\n    speakers and the size of the profile pictures. It also takes into account the\n    spacing between the speakers and the header height.\n\n    Args:\n        num_speakers (int): Number of speakers\n        min_spacing (int): Minimum spacing between the speakers, defaults to 40\n    \"\"\"\n\n    available_height = self.video_height - self.header_height\n    total_dp_height = num_speakers * self.dp_size[1]\n\n    # Calculate spacing between DPs\n    num_spaces = num_speakers + 1\n    spacing = (available_height - total_dp_height) // num_spaces\n    spacing = max(spacing, min_spacing)\n\n    # Calculate starting Y position\n    start_y = self.header_height + spacing\n\n    return spacing, start_y\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout._calculate_positions","title":"<code>_calculate_positions()</code>","text":"<p>Calculate positions for all speakers based on the number of speakers and the size of the profile pictures. (mostly for internal use)</p> <p>This method calculates the positions of all speakers based on the number of speakers and the size of the profile pictures. It also takes into account the spacing between the speakers and the header height.</p> Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>def _calculate_positions(self):\n    \"\"\"\n    Calculate positions for all speakers based on the number of speakers and\n    the size of the profile pictures. (mostly for internal use)\n\n    This method calculates the positions of all speakers based on the number of\n    speakers and the size of the profile pictures. It also takes into account\n    the spacing between the speakers and the header height.\n    \"\"\"\n\n    num_speakers = len(self.speakers)\n    spacing, start_y = self._calculate_layout(num_speakers)\n\n    for i, speaker in enumerate(self.speakers.keys()):\n        y_pos = start_y + (i * (self.dp_size[1] + spacing))\n        self.dp_positions[speaker] = (self.dp_margin_left, y_pos)\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout._draw_subtitle","title":"<code>_draw_subtitle(frame, draw, subtitle, opacity)</code>","text":"<p>Draw the current subtitle with speaker highlighting (mostly for internal use)</p> <p>This method draws the current subtitle with speaker highlighting. It highlights the active speaker and draws the subtitle text.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>Image</code> <p>Frame to draw on</p> required <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>subtitle</code> <code>Subtitle</code> <p>Current subtitle</p> required <code>opacity</code> <code>int</code> <p>Opacity of the subtitle</p> required Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>def _draw_subtitle(self, frame, draw, subtitle, opacity):\n    \"\"\"\n    Draw the current subtitle with speaker highlighting\n    (mostly for internal use)\n\n    This method draws the current subtitle with speaker highlighting.\n    It highlights the active speaker and draws the subtitle text.\n\n    Args:\n        frame (Image): Frame to draw on\n        draw (ImageDraw): Draw object to draw on the frame\n        subtitle (Subtitle): Current subtitle\n        opacity (int): Opacity of the subtitle\n    \"\"\"\n\n    speaker, text = subtitle.text.split(\"] \")\n    speaker = speaker.replace(\"[\", \"\").strip()\n\n    # Highlight active speaker\n    if speaker in self.speakers:\n        highlight_color = (255, 200, 0)\n        speaker_pos = self.dp_positions[speaker]\n        self.speakers[speaker].highlight(\n            draw, speaker_pos, color=highlight_color, opacity=opacity\n        )\n\n        # Calculate text position\n        text_x = self.dp_margin_left + self.dp_size[0] + self.text_margin\n        text_y = speaker_pos[1] + (self.dp_size[1] // 2)\n        text_width = self.video_width - text_x - self.text_margin\n\n        # Draw the subtitle text\n        self.text_renderer.draw_wrapped_text(\n            draw,\n            text,\n            (text_x, text_y),\n            max_width=text_width,\n            font_size=40,\n            color=(255, 255, 255, opacity),\n            anchor=\"lm\",\n        )\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.add_speaker","title":"<code>add_speaker(name, image_path, shape='circle')</code>","text":"<p>Add a speaker to the layout</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the speaker</p> required <code>image_path</code> <code>str</code> <p>Path to the speaker's image</p> required <code>shape</code> <code>str</code> <p>Shape of the profile picture, defaults to \"circle\"</p> <code>'circle'</code> Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>def add_speaker(self, name, image_path, shape=\"circle\"):\n    \"\"\"\n    Add a speaker to the layout\n\n    Args:\n        name (str): Name of the speaker\n        image_path (str): Path to the speaker's image\n        shape (str): Shape of the profile picture, defaults to \"circle\"\n    \"\"\"\n\n    self.speakers[name] = ProfilePicture(image_path, self.dp_size, shape)\n\n    # Recalculate positions when speakers are added\n    self._calculate_positions()\n\n    return self\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.create_frame","title":"<code>create_frame(current_sub=None, opacity=255, background_color=(20, 20, 20))</code>","text":"<p>Create a frame with the podcast layout</p> <p>Parameters:</p> Name Type Description Default <code>current_sub</code> <code>str</code> <p>Current subtitle</p> <code>None</code> <code>opacity</code> <code>int</code> <p>Opacity of the subtitle</p> <code>255</code> <code>background_color</code> <code>tuple</code> <p>Background color in RGB format,                       defaults to (20, 20, 20)</p> <code>(20, 20, 20)</code> Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>def create_frame(\n    self, current_sub=None, opacity=255, background_color=(20, 20, 20)\n):\n    \"\"\"\n    Create a frame with the podcast layout\n\n    Args:\n        current_sub (str): Current subtitle\n        opacity (int): Opacity of the subtitle\n        background_color (tuple): Background color in RGB format,\n                                  defaults to (20, 20, 20)\n    \"\"\"\n\n    # Create base frame\n    frame, draw = self._create_base_frame(background_color)\n\n    # Draw header\n    if self.logo_path:\n        self.header.set_logo(self.logo_path)\n    self.header.draw(frame, draw, self.video_width, self.title, opacity)\n\n    # Add all speaker DPs and names\n    for speaker, profile in self.speakers.items():\n        pos = self.dp_positions[speaker]\n        frame.paste(profile.image, pos, profile.image)\n\n        # Draw speaker name if enabled\n        if self.show_speaker_names:\n            name_y = pos[1] + self.dp_size[1] + self.name_margin\n            self.text_renderer.draw_text(\n                draw,\n                speaker,\n                (pos[0] + self.dp_size[0] // 2, name_y),\n                font_size=30,\n                color=(200, 200, 200, opacity),\n                anchor=\"mm\",\n            )\n\n    # Add subtitle if there's a current subtitle\n    if current_sub:\n        self._draw_subtitle(frame, draw, current_sub, opacity)\n\n    return np.array(frame)\n</code></pre>"},{"location":"examples/podcast_01/","title":"Basic Podcast Video Generation","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: March 05, 2025</li> <li>Last Updated: March 05, 2025</li> <li>Compatible with: Audim v0.0.1</li> </ul> <p>This example demonstrates how to generate a simple podcast video from a subtitle file and audio file using Audim's Sub2Pod module.</p>"},{"location":"examples/podcast_01/#prerequisites","title":"Prerequisites","text":"<p>Before running this example, make sure you have:</p> <ol> <li>Installed Audim following the installation instructions</li> <li> <p>Created the required input files:</p> <ul> <li><code>input/podcast.srt</code> - Subtitle file with speaker tags</li> <li><code>input/podcast.mp3</code> - Audio file of the podcast</li> <li><code>input/host_dp.png</code> - Host profile picture</li> <li><code>input/guest_dp.png</code> - Guest profile picture</li> <li><code>input/logo.png</code> - Brand logo (optional)</li> </ul> </li> </ol>"},{"location":"examples/podcast_01/#example-srt-file","title":"Example SRT File","text":"<p>Your SRT file should follow the standard SubRip Subtitle format with added speaker tags as expected by Audim.</p> <p>Below is an example of an SRT file with speaker tags:</p> <pre><code>1\n00:00:00,000 --&gt; 00:00:04,500\n[Host] Welcome to our podcast!\n\n2\n00:00:04,600 --&gt; 00:00:08,200\n[Guest] Thank you! Glad to be here.\n</code></pre>"},{"location":"examples/podcast_01/#example-code-implementation","title":"Example Code Implementation","text":"<p>The following code demonstrates how to generate a podcast video from an SRT file and audio file using Audim's Sub2Pod module.</p> <pre><code>from datetime import datetime\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Add speakers\nprint(\"Adding speakers...\")\nlayout.add_speaker(\"Host\", \"input/host_dp.png\")\nlayout.add_speaker(\"Guest\", \"input/guest_dp.png\")\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=\"input/podcast.srt\",\n    audio_path=\"input/podcast.mp3\",\n    logo_path=\"input/logo.png\",\n    title=\"My Awesome Podcast\"\n)\n\n# Export the final video\nprint(\"Exporting video...\")\ndatetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\ngenerator.export_video(f\"output/podcast_{datetime}.mp4\")\n</code></pre>"},{"location":"examples/podcast_01/#output","title":"Output","text":"<p>The output podcast video will be saved in the <code>output</code> directory with a timestamp in the filename.</p>"},{"location":"examples/podcast_01/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with this example: - Verify you're using the compatible version of Audim - Check that all input files exist and are in the correct format - Ensure your SRT file has proper speaker tags in the format <code>[SpeakerName]</code></p>"},{"location":"examples/podcast_01/#see-also","title":"See Also","text":"<ul> <li>API Documentation for PodcastLayout</li> <li>API Documentation for VideoGenerator</li> </ul>"},{"location":"examples/podcast_02/","title":"Creating a Professional Podcast Video with Audim","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: March 13, 2025</li> <li>Last Updated: March 14, 2025</li> <li>Compatible with: Audim v0.0.2</li> </ul> <p>This example demonstrates how to create a professional-looking podcast video using Audim's <code>Sub2Pod</code> module, featuring real speakers with profile pictures, custom branding, and high-quality output.</p>"},{"location":"examples/podcast_02/#overview","title":"Overview","text":"<p>In this tutorial, we'll transform a conversation between Grant Sanderson (from 3Blue1Brown) and Sal Khan (from Khan Academy) into a visually engaging podcast video. We'll walk through:</p> <ol> <li>Preparing the input files</li> <li>Setting up the podcast layout</li> <li>Generating the video with Audim</li> <li>Reviewing the final output</li> </ol> <p>Note: The conversation between Grant and Sal is taken from this podcast.</p>"},{"location":"examples/podcast_02/#input-files","title":"Input Files","text":""},{"location":"examples/podcast_02/#1-podcast-audio-file","title":"1. Podcast Audio File","text":"<p>We need to have the audio recording of the podcast that we want to convert to a video.</p> <p>Below is a sample of the podcast audio we'll be using:</p>      Your browser does not support the audio element.    <p>Audio snippet from \"Sal Khan: Beyond Khan Academy | 3b1b Podcast #2\"</p>"},{"location":"examples/podcast_02/#2-podcast-subtitles-file-srt","title":"2. Podcast Subtitles File (.SRT)","text":"<p>The SRT file should contain the transcription with speaker tags for the package to understand the speaker for each text. The SRT file should follow the standard SubRip Subtitle format with added speaker tags as expected by Audim.</p> <p>Below is the SRT file used for this example:</p>"},{"location":"examples/podcast_02/#3-other-files","title":"3. Other Files","text":"<p>Along with the audio and subtitles files, we also need the following files:</p> <ul> <li>Profile Picture of Grant Sanderson</li> <li>Profile Picture of Sal Khan</li> <li>Brand Logo of 3Blue1Brown</li> </ul>"},{"location":"examples/podcast_02/#code-implementation","title":"Code Implementation","text":"<p>After gathering all the files, we can now generate the podcast video using Audim.</p> <p>Here's the complete code to generate our podcast video:</p> <pre><code>from datetime import datetime\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Add speakers\nprint(\"Adding speakers...\")\nlayout.add_speaker(\"Grant Sanderson\", \"input/grant.png\")\nlayout.add_speaker(\"Sal Khan\", \"input/sal.png\")\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=\"input/podcast.srt\",\n    audio_path=\"input/podcast.mp3\",\n    logo_path=\"input/logo.png\",\n    title=\"3b1b Podcast: Sal Khan: Beyond Khan Academy\",\n    cpu_core_utilization=\"max\"\n)\n\n# Export the final video\nprint(\"Exporting video...\")\ndatetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\ngenerator.export_video(f\"output/podcast_{datetime}.mp4\")\n</code></pre> <p>Here is the terminal logs upon running the code:</p> <pre><code>(audim) (base) atanu@atanu-LOQ-15APH8:~/Workspace/GitHub/audim$ python test.py \nCreating layout...\nAdding speakers...\nGenerating video...\n[2025-03-13 23:25:10] VideoGenerator (INFO) - Loading subtitles from input/podcast.srt\n[2025-03-13 23:25:10] VideoGenerator (INFO) - Using 16 CPU cores for parallel processing\n[2025-03-13 23:25:10] VideoGenerator (INFO) - Processing subtitle to generate frames in 23 batches\nProcessing batch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:37&lt;00:00,  1.64s/batch, frames processed=4727]\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Frame generation completed: Total 4727 frames created\nExporting video...\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Starting video generation process with 4727 frames\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Video duration: 156.06s (adjusted to match audio)\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Attempting video export with FFmpeg encoding\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Preparing frame list for FFmpeg\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Using NVIDIA GPU acceleration for video encoding\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Starting FFmpeg encoding process\nEncoding video:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 99/100 [01:09&lt;00:00,  1.42%/s]\n[2025-03-13 23:26:58] VideoGenerator (INFO) - Video successfully encoded to output/podcast_20250313232548.mp4\n[2025-03-13 23:26:58] VideoGenerator (INFO) - Cleaned up temporary files in /tmp/tmpoqv_t5x6\n[2025-03-13 23:26:58] VideoGenerator (INFO) - Video generation completed! Exported to: output/podcast_20250313232548.mp4\n</code></pre>"},{"location":"examples/podcast_02/#output-video","title":"Output Video","text":"<p>Here's how the generated video looks like upon completion of the rendering process:</p>      Your browser does not support the video element."},{"location":"examples/podcast_02/#code-breakdown","title":"Code Breakdown","text":"<ol> <li>Layout Creation: We create a <code>PodcastLayout</code> with Full HD resolution (1920\u00d71080) and enable speaker name display.</li> <li>Speaker Configuration: We add two speakers with their respective profile pictures.</li> <li>Video Generation: We initialize a <code>VideoGenerator</code> with our layout and set the frame rate to 30 FPS.</li> <li>Content Processing: The generator processes our SRT and audio files, incorporating the logo and title.</li> <li>Performance Optimization: We use <code>cpu_core_utilization=\"max\"</code> to leverage all available CPU cores for faster frame generation + native system FFmpeg with NVIDIA GPU acceleration for faster video encoding and rendering.</li> <li>Export: The final video is saved with a timestamp in the filename for easy versioning.</li> </ol>"},{"location":"examples/podcast_02/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ul> <li>Ensure your SRT file has proper speaker tags in the format <code>[SpeakerName]</code></li> <li>Verify that speaker names in the SRT match exactly with those added via <code>add_speaker()</code></li> <li>Check that all input files exist and are in the correct format</li> <li>For performance issues, try adjusting the <code>cpu_core_utilization</code> parameter</li> </ul>"},{"location":"examples/podcast_02/#see-also","title":"See Also","text":"<ul> <li>Basic Podcast Example</li> <li>API Documentation for VideoGenerator</li> <li>API Documentation for PodcastLayout</li> </ul>"},{"location":"setup/development/","title":"Development","text":""},{"location":"setup/development/#code-quality","title":"Code Quality","text":"<p>Before committing, please ensure that the code is formatted and styled correctly. Run the following commands to check and fix code style issues:</p> <pre><code># Check and fix code style issues\nruff format .\nruff check --fix .\n</code></pre>"},{"location":"setup/development/#run-the-project","title":"Run the project","text":"<p>feel free to create a <code>run.py</code> or <code>test.py</code> file to test the project. They will be untracked by git.</p> <p>implement your usage logic in the <code>run.py</code> file.</p> <p>Run with:</p> <pre><code>python run.py\n</code></pre>"},{"location":"setup/development/#build-and-serve-the-documentation","title":"Build and serve the documentation","text":"<p>You can build and serve the documentation by running:</p> <pre><code>uv pip install -e .[docs]\nmkdocs serve\n</code></pre>"},{"location":"setup/installation/","title":"Installation","text":""},{"location":"setup/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python \u2265 3.10</li> <li>Conda</li> </ul>"},{"location":"setup/installation/#setup","title":"Setup","text":""},{"location":"setup/installation/#1-clone-the-repository","title":"1. Clone the repository:","text":"<pre><code>git clone https://github.com/mratanusarkar/audim.git\ncd audim\n</code></pre>"},{"location":"setup/installation/#2-install-ffmpeg-locally-optional","title":"2. Install FFmpeg locally (optional)","text":"<p>Using local FFmpeg is optional. It is recommended to install FFmpeg locally as it will speed up the video encoding process.</p> <p>On Ubuntu, install FFmpeg using:</p> <pre><code>sudo apt install ffmpeg libx264-dev\n</code></pre> <p>On Windows and other platforms, download and install FFmpeg from the official website:</p> <ul> <li>Download FFmpeg</li> <li>Make sure FFmpeg is in your system PATH</li> </ul>"},{"location":"setup/installation/#3-install-uv-and-setup-project-environment","title":"3. Install <code>uv</code> and setup project environment:","text":"<p>Note: If you are using conda base environment as the default base environment for your python projects, run the below command to activate the base environment. If not, skip this step and continue with the next step.</p> <pre><code>conda activate base\n</code></pre> <pre><code># Install uv\npip install uv\n\n# Setup project environment\nuv venv\n\nsource .venv/bin/activate   # on Linux\n# .venv\\Scripts\\activate    # on Windows\n\nuv pip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"setup/installation/#4-create-input-and-output-directories","title":"4. Create input and output directories:","text":"<pre><code>mkdir ./input ./output\n</code></pre> <p>ideally, if done correctly, the setup should be like this:</p> <pre><code>audim/\n\u251c\u2500\u2500 audim/\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 input/\n\u251c\u2500\u2500 output/\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Note: you would dump your input files in the <code>input</code> directory and the output files will be dumped in the <code>output</code> directory.</p>"}]}