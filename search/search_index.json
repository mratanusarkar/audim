{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Audim","text":"<p>Audio Podcast Animation Engine</p> <p>An animation and video rendering engine for audio-based and voice-based podcast videos.</p> <p>Audim is an engine for precise programmatic animation and rendering of podcast videos from audio-based and voice-based file recordings.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Precise programmatic animations.</li> <li>Rendering of videos.</li> <li>Layout based scenes.</li> <li>Support for audio to subtitle generation.</li> <li>Support for video to subtitle and scene elements generation.</li> <li>Support for subtitle and scene elements to video generation.</li> </ul>"},{"location":"#example-usage","title":"Example Usage","text":"<ul> <li>See \"Examples\" section in the documentation for usage examples.</li> <li>Ensure you have setup correctly before usage. See \"Setup\" section in the documentation for more details.</li> </ul>"},{"location":"audim/aud2sub/core/","title":"Core (Subtitle Generation Module)","text":"<p>The core module is the main Subtitle Generation Engine. It is responsible for the overall structure and flow of the subtitle generation.</p> <p>This module provides the high-level API for generating subtitles from audio files. It uses a transcriber object to define the format of the subtitle.</p> <p>The transcriber object internally uses an ASR model to transcribe the audio, aligns the transcription segment timestamps with the audio, and a diarization model to detect the speakers. Finally it uses a formatter to determine the format of the subtitle generation.</p> <p>Below is the API documentation for the core module:</p>"},{"location":"audim/aud2sub/core/#audim.aud2sub.core.SubtitleGenerator","title":"<code>SubtitleGenerator</code>","text":"<p>High-level generator for creating subtitles from audio.</p> <p>This class provides a simple interface for generating subtitles from audio files using a configured transcriber.</p> <p>Parameters:</p> Name Type Description Default <code>transcriber</code> <code>BaseTranscriber</code> <p>Configured transcriber to use for processing audio</p> required Source code in <code>audim/aud2sub/core.py</code> <pre><code>class SubtitleGenerator:\n    \"\"\"\n    High-level generator for creating subtitles from audio.\n\n    This class provides a simple interface for generating subtitles from audio files\n    using a configured transcriber.\n\n    Args:\n        transcriber: Configured transcriber to use for processing audio\n    \"\"\"\n\n    def __init__(self, transcriber: BaseTranscriber):\n        self.transcriber = transcriber\n        self._processed = False\n\n    def generate_from_mp3(self, mp3_path: str) -&gt; None:\n        \"\"\"\n        Generate subtitles from an MP3 file.\n\n        This method processes the audio file and prepares subtitles for export.\n\n        Args:\n            mp3_path: Path to the MP3 file\n        \"\"\"\n\n        self._process_audio_file(mp3_path)\n\n    def generate_from_wav(self, wav_path: str) -&gt; None:\n        \"\"\"\n        Generate subtitles from a WAV file.\n\n        This method processes the audio file and prepares subtitles for export.\n\n        Args:\n            wav_path: Path to the WAV file\n        \"\"\"\n\n        self._process_audio_file(wav_path)\n\n    def generate_from_audio(self, audio_path: str) -&gt; None:\n        \"\"\"\n        Generate subtitles from any supported audio file.\n\n        This method processes the audio file and prepares subtitles for export.\n\n        Args:\n            audio_path: Path to the audio file\n        \"\"\"\n\n        self._process_audio_file(audio_path)\n\n    def _process_audio_file(self, audio_path: str) -&gt; None:\n        \"\"\"\n        Process an audio file using the configured transcriber.\n\n        Args:\n            audio_path: Path to the audio file\n        \"\"\"\n\n        # Process the audio file\n        self.transcriber.process_audio(audio_path)\n        self._processed = True\n\n    def export_subtitle(self, output_path: str) -&gt; None:\n        \"\"\"\n        Export the generated subtitles to a file.\n\n        Args:\n            output_path: Path to the output subtitle file\n        \"\"\"\n\n        if not self._processed:\n            raise ValueError(\n                \"No processed audio available. Run `generate_from_*()` methods first.\"\n            )\n\n        # Ensure the output directory exists\n        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n        # Export the subtitle file\n        self.transcriber.export_subtitle(output_path)\n</code></pre>"},{"location":"audim/aud2sub/core/#audim.aud2sub.core.SubtitleGenerator.export_subtitle","title":"<code>export_subtitle(output_path)</code>","text":"<p>Export the generated subtitles to a file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to the output subtitle file</p> required Source code in <code>audim/aud2sub/core.py</code> <pre><code>def export_subtitle(self, output_path: str) -&gt; None:\n    \"\"\"\n    Export the generated subtitles to a file.\n\n    Args:\n        output_path: Path to the output subtitle file\n    \"\"\"\n\n    if not self._processed:\n        raise ValueError(\n            \"No processed audio available. Run `generate_from_*()` methods first.\"\n        )\n\n    # Ensure the output directory exists\n    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n    # Export the subtitle file\n    self.transcriber.export_subtitle(output_path)\n</code></pre>"},{"location":"audim/aud2sub/core/#audim.aud2sub.core.SubtitleGenerator.generate_from_audio","title":"<code>generate_from_audio(audio_path)</code>","text":"<p>Generate subtitles from any supported audio file.</p> <p>This method processes the audio file and prepares subtitles for export.</p> <p>Parameters:</p> Name Type Description Default <code>audio_path</code> <code>str</code> <p>Path to the audio file</p> required Source code in <code>audim/aud2sub/core.py</code> <pre><code>def generate_from_audio(self, audio_path: str) -&gt; None:\n    \"\"\"\n    Generate subtitles from any supported audio file.\n\n    This method processes the audio file and prepares subtitles for export.\n\n    Args:\n        audio_path: Path to the audio file\n    \"\"\"\n\n    self._process_audio_file(audio_path)\n</code></pre>"},{"location":"audim/aud2sub/core/#audim.aud2sub.core.SubtitleGenerator.generate_from_mp3","title":"<code>generate_from_mp3(mp3_path)</code>","text":"<p>Generate subtitles from an MP3 file.</p> <p>This method processes the audio file and prepares subtitles for export.</p> <p>Parameters:</p> Name Type Description Default <code>mp3_path</code> <code>str</code> <p>Path to the MP3 file</p> required Source code in <code>audim/aud2sub/core.py</code> <pre><code>def generate_from_mp3(self, mp3_path: str) -&gt; None:\n    \"\"\"\n    Generate subtitles from an MP3 file.\n\n    This method processes the audio file and prepares subtitles for export.\n\n    Args:\n        mp3_path: Path to the MP3 file\n    \"\"\"\n\n    self._process_audio_file(mp3_path)\n</code></pre>"},{"location":"audim/aud2sub/core/#audim.aud2sub.core.SubtitleGenerator.generate_from_wav","title":"<code>generate_from_wav(wav_path)</code>","text":"<p>Generate subtitles from a WAV file.</p> <p>This method processes the audio file and prepares subtitles for export.</p> <p>Parameters:</p> Name Type Description Default <code>wav_path</code> <code>str</code> <p>Path to the WAV file</p> required Source code in <code>audim/aud2sub/core.py</code> <pre><code>def generate_from_wav(self, wav_path: str) -&gt; None:\n    \"\"\"\n    Generate subtitles from a WAV file.\n\n    This method processes the audio file and prepares subtitles for export.\n\n    Args:\n        wav_path: Path to the WAV file\n    \"\"\"\n\n    self._process_audio_file(wav_path)\n</code></pre>"},{"location":"audim/aud2sub/transcribers/base/","title":"Base Transcriber","text":"<p>The base transcriber is the base class for all transcriber classes. It defines the interface for all transcriber classes.</p> <p>It must be overriden to create various transcriber classes with various ASR models, diarization models, alignment models, formatters and their implementations.</p> <p>It determines the subtitle generation pipeline and format of the subtitle generation.</p> <p>Below is the API documentation for the base transcriber:</p>"},{"location":"audim/aud2sub/transcribers/base/#audim.aud2sub.transcribers.base.BaseTranscriber","title":"<code>BaseTranscriber</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all transcriber implementations.</p> <p>This abstract class defines the interface that all transcriber implementations must follow. Each implementation should handle transcription, diarization, and formatting internally.</p> Source code in <code>audim/aud2sub/transcribers/base.py</code> <pre><code>class BaseTranscriber(ABC):\n    \"\"\"\n    Base class for all transcriber implementations.\n\n    This abstract class defines the interface that all transcriber implementations\n    must follow. Each implementation should handle transcription, diarization,\n    and formatting internally.\n    \"\"\"\n\n    @abstractmethod\n    def process_audio(self, audio_path: str) -&gt; None:\n        \"\"\"\n        Process an audio file to generate transcription.\n\n        Args:\n            audio_path: Path to the audio file.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def export_subtitle(self, output_path: str) -&gt; None:\n        \"\"\"\n        Export the processed transcription to a subtitle file.\n\n        Args:\n            output_path: Path to the output subtitle file.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"audim/aud2sub/transcribers/base/#audim.aud2sub.transcribers.base.BaseTranscriber.export_subtitle","title":"<code>export_subtitle(output_path)</code>  <code>abstractmethod</code>","text":"<p>Export the processed transcription to a subtitle file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to the output subtitle file.</p> required Source code in <code>audim/aud2sub/transcribers/base.py</code> <pre><code>@abstractmethod\ndef export_subtitle(self, output_path: str) -&gt; None:\n    \"\"\"\n    Export the processed transcription to a subtitle file.\n\n    Args:\n        output_path: Path to the output subtitle file.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"audim/aud2sub/transcribers/base/#audim.aud2sub.transcribers.base.BaseTranscriber.process_audio","title":"<code>process_audio(audio_path)</code>  <code>abstractmethod</code>","text":"<p>Process an audio file to generate transcription.</p> <p>Parameters:</p> Name Type Description Default <code>audio_path</code> <code>str</code> <p>Path to the audio file.</p> required Source code in <code>audim/aud2sub/transcribers/base.py</code> <pre><code>@abstractmethod\ndef process_audio(self, audio_path: str) -&gt; None:\n    \"\"\"\n    Process an audio file to generate transcription.\n\n    Args:\n        audio_path: Path to the audio file.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/","title":"Podcast Transcriber","text":"<p>The podcast transcriber is a transcriber implementation that uses the WhisperX under the hood. WhisperX provides Automatic Speech Recognition with Word-level Timestamps and Diarization.</p> <p>It uses <code>faster-whisper</code> as the ASR and Transcription model, it's own alignment logic and <code>pyannote-audio</code> for diarization.</p> <p>Warning</p> <p>WhisperX uses a local offline ASR model. So, all the models are downloaded and run locally. You must have a good system specification and NVIDIA GPU with 12GB VRAM to run this.</p> <p>In future, we will support to work with online model vendors like <code>OpenAI</code> and <code>HuggingFace</code>.</p> <p>Below is the API documentation for the podcast transcriber:</p>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber","title":"<code>PodcastTranscriber</code>","text":"<p>               Bases: <code>BaseTranscriber</code></p> <p>Podcast transcriber implementation using WhisperX.</p> <p>This class provides a complete implementation for podcast transcription, using WhisperX for ASR, diarization, and subtitle formatting.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>WhisperX model name (tiny, base, small, medium, large, large-v2, large-v3)</p> <code>'large-v2'</code> <code>language</code> <code>Optional[str]</code> <p>Language code (e.g., 'en', 'hi', 'bn') or None for auto-detection</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to run inference on (cpu, cuda, mps)</p> <code>None</code> <code>compute_type</code> <code>str</code> <p>Compute type (float16, float32, int8)</p> <code>'float16'</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>16</code> <code>min_speakers</code> <code>Optional[int]</code> <p>Minimum number of speakers</p> <code>1</code> <code>max_speakers</code> <code>Optional[int]</code> <p>Maximum number of speakers</p> <code>5</code> <code>hf_token</code> <code>Optional[str]</code> <p>HuggingFace token for accessing diarization models If not provided, will try to use HF_TOKEN environment variable or the token stored by huggingface-cli login</p> <code>None</code> <code>max_line_length</code> <code>int</code> <p>Maximum length of subtitle lines</p> <code>70</code> <code>min_char_length_splitter</code> <code>int</code> <p>Minimum characters before line splitting</p> <code>50</code> <code>show_speaker_names</code> <code>bool</code> <p>Whether to show speaker names in subtitles</p> <code>True</code> <code>speaker_name_pattern</code> <code>str</code> <p>Pattern for formatting speaker names</p> <code>'[{speaker}]'</code> <code>clear_gpu_memory</code> <code>bool</code> <p>Whether to clear GPU memory after completing major processing steps</p> <code>False</code> Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>class PodcastTranscriber(BaseTranscriber):\n    \"\"\"\n    Podcast transcriber implementation using WhisperX.\n\n    This class provides a complete implementation for podcast transcription,\n    using WhisperX for ASR, diarization, and subtitle formatting.\n\n    Args:\n        model_name: WhisperX model name\n            (tiny, base, small, medium, large, large-v2, large-v3)\n        language: Language code (e.g., 'en', 'hi', 'bn') or None for auto-detection\n        device: Device to run inference on (cpu, cuda, mps)\n        compute_type: Compute type (float16, float32, int8)\n        batch_size: Batch size for processing\n        min_speakers: Minimum number of speakers\n        max_speakers: Maximum number of speakers\n        hf_token: HuggingFace token for accessing diarization models\n            If not provided, will try to use HF_TOKEN environment variable\n            or the token stored by huggingface-cli login\n        max_line_length: Maximum length of subtitle lines\n        min_char_length_splitter: Minimum characters before line splitting\n        show_speaker_names: Whether to show speaker names in subtitles\n        speaker_name_pattern: Pattern for formatting speaker names\n        clear_gpu_memory: Whether to clear GPU memory after completing\n            major processing steps\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"large-v2\",\n        language: Optional[str] = None,\n        device: Optional[str] = None,\n        compute_type: str = \"float16\",\n        batch_size: int = 16,\n        min_speakers: Optional[int] = 1,\n        max_speakers: Optional[int] = 5,\n        hf_token: Optional[str] = None,\n        max_line_length: int = 70,\n        min_char_length_splitter: int = 50,\n        show_speaker_names: bool = True,\n        speaker_name_pattern: str = \"[{speaker}]\",\n        clear_gpu_memory: bool = False,\n    ):\n        # ASR Model parameters\n        self.model_name = model_name\n        self.language = language\n        self.compute_type = compute_type\n        self.batch_size = batch_size\n\n        # Auto-detect device and correctly set to CPU if CUDA is not available\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        elif device == \"cuda\" and not torch.cuda.is_available():\n            print(\"CUDA not available, using CPU instead.\")\n            device = \"cpu\"\n\n        self.device = device\n\n        # Adjust compute type based on device\n        if self.device == \"cpu\" and self.compute_type == \"float16\":\n            print(\"float16 not supported on CPU, using float32 instead.\")\n            self.compute_type = \"float32\"\n\n        # Diarization parameters\n        self.min_speakers = min_speakers\n        self.max_speakers = max_speakers\n\n        # HuggingFace token handling (following best practices)\n        self.hf_token = self._resolve_huggingface_token(hf_token)\n\n        # Formatting parameters\n        self.max_line_length = max_line_length\n        self.min_char_length_splitter = min_char_length_splitter\n        self.show_speaker_names = show_speaker_names\n        self.speaker_name_pattern = speaker_name_pattern\n\n        # Memory management\n        self.clear_gpu_memory = clear_gpu_memory\n\n        # Results storage\n        self._transcript_result = None\n        self._diarize_segments = None\n        self._segments_with_speakers = None\n        self._processed_segments = None\n        self._detected_language = None\n\n    def _resolve_huggingface_token(self, provided_token: Optional[str] = None) -&gt; Optional[str]:\n        \"\"\"\n        Resolve HuggingFace token from various sources with priority:\n        1. Directly provided token\n        2. HF_TOKEN environment variable\n        3. Token stored by huggingface-cli login\n\n        Args:\n            provided_token: Token directly provided to the method\n\n        Returns:\n            Resolved token or None if no token is found\n        \"\"\"\n        # Priority 1: Directly provided token\n        if provided_token:\n            return provided_token\n\n        # Priority 2: Environment variable\n        env_token = os.environ.get(\"HF_TOKEN\")\n        if env_token:\n            return env_token\n\n        # Priority 3: Token stored by huggingface-cli login\n        try:\n            from huggingface_hub.constants import HF_TOKEN_PATH\n            token_path = Path(HF_TOKEN_PATH)\n            if token_path.exists():\n                return token_path.read_text().strip()\n        except (ImportError, Exception):\n            # If huggingface_hub is not installed or any other error occurs\n            pass\n\n        return None\n\n    def _clear_gpu_memory(self, message: str = None) -&gt; None:\n        \"\"\"\n        Clear GPU memory by collecting garbage and emptying CUDA cache.\n\n        Args:\n            message: Optional message to display when clearing memory\n        \"\"\"\n\n        if not self.clear_gpu_memory or self.device != \"cuda\":\n            return\n\n        if message:\n            print(f\"Clearing GPU memory: {message}\")\n\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def process_audio(self, audio_path: str) -&gt; None:\n        \"\"\"\n        Process audio file to generate transcription with diarization.\n\n        Args:\n            audio_path: Path to the audio file.\n        \"\"\"\n\n        print(f\"Processing {audio_path}...\")\n\n        # 1. Load the audio file\n        audio = whisperx.load_audio(audio_path)\n\n        # 2. Load the ASR model (Whisper)\n        print(f\"Loading Whisper {self.model_name} model...\")\n        model = whisperx.load_model(\n            self.model_name,\n            device=self.device,\n            compute_type=self.compute_type,\n            language=self.language,\n            download_root=None,\n            local_files_only=False,\n            asr_options={\"beam_size\": 5},\n        )\n\n        # 3. Transcribe audio with whisperX\n        print(\"Transcribing audio...\")\n        result = model.transcribe(audio, batch_size=self.batch_size)\n\n        self._detected_language = result[\"language\"]\n        print(f\"Detected language: {self._detected_language}\")\n\n        # Clear GPU memory after transcription if requested\n        self._clear_gpu_memory(\"after transcription\")\n        if self.clear_gpu_memory and self.device == \"cuda\":\n            del model\n\n        # 4. Align whisper output\n        print(\"Aligning whisper output...\")\n        align_model, align_metadata = whisperx.load_align_model(\n            language_code=self._detected_language, device=self.device\n        )\n\n        result = whisperx.align(\n            result[\"segments\"],\n            align_model,\n            align_metadata,\n            audio,\n            self.device,\n            return_char_alignments=False,\n        )\n\n        self._transcript_result = result\n\n        # Clear GPU memory after alignment if requested\n        self._clear_gpu_memory(\"after alignment\")\n        if self.clear_gpu_memory and self.device == \"cuda\":\n            del align_model\n\n        # 5. Speaker diarization\n        if self.hf_token:\n            print(\"Running speaker diarization...\")\n            diarize_model = whisperx.DiarizationPipeline(\n                model_name=\"pyannote/speaker-diarization-3.1\",\n                use_auth_token=self.hf_token,\n                device=self.device,\n            )\n\n            self._diarize_segments = diarize_model(\n                audio, min_speakers=self.min_speakers, max_speakers=self.max_speakers\n            )\n\n            # Assign speaker labels to segments\n            print(\"Assigning speaker labels to segments...\")\n            result = whisperx.assign_word_speakers(\n                self._diarize_segments, self._transcript_result\n            )\n            self._segments_with_speakers = result[\"segments\"]\n\n            # Clear GPU memory after diarization if requested\n            self._clear_gpu_memory(\"after diarization\")\n            if self.clear_gpu_memory and self.device == \"cuda\":\n                del diarize_model\n        else:\n            print(\"Warning: No HuggingFace token found. Skipping diarization.\")\n            print(\"To use diarization, provide a huggingface token by one of these methods:\")\n            print(\"  1. Pass directly: transcriber = PodcastTranscriber(hf_token='your_token')\")\n            print(\"  2. Set environment variable: export HF_TOKEN='your_token'\")\n            print(\"  3. Login with CLI using: huggingface-cli login\")\n            self._segments_with_speakers = self._transcript_result[\"segments\"]\n            for segment in self._segments_with_speakers:\n                segment[\"speaker\"] = \"Speaker\"\n\n        # 6. Process subtitles with SubtitlesProcessor for line length control\n        print(\"Processing subtitles for optimal line length...\")\n        subtitles_processor = SubtitlesProcessor(\n            segments=self._segments_with_speakers,\n            lang=self._detected_language,\n            max_line_length=self.max_line_length,\n            min_char_length_splitter=self.min_char_length_splitter,\n        )\n\n        self._processed_segments = subtitles_processor.process_segments(\n            advanced_splitting=True\n        )\n\n        # Final memory cleanup\n        self._clear_gpu_memory(\"final cleanup\")\n\n        print(\"Audio processing completed successfully.\")\n\n    def export_subtitle(self, output_path: str) -&gt; None:\n        \"\"\"\n        Export the processed transcription to an SRT subtitle file.\n\n        Args:\n            output_path: Path to the output SRT file.\n        \"\"\"\n\n        if self._processed_segments is None:\n            raise ValueError(\n                \"No processed audio available. Run `process_audio()` first.\"\n            )\n\n        # Ensure the output directory exists\n        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n        print(f\"Saving SRT to {output_path}...\")\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            for i, segment in enumerate(self._processed_segments, 1):\n                # Find the original segment that this processed segment came from\n                # by finding which original segment contains this timestamp\n                original_segment = next(\n                    (\n                        s\n                        for s in self._segments_with_speakers\n                        if s[\"start\"] &lt;= segment[\"start\"]\n                        and s[\"end\"] &gt;= segment[\"start\"]\n                    ),\n                    {\"speaker\": \"Speaker\"},\n                )\n\n                speaker = original_segment.get(\"speaker\", \"Speaker\")\n                # Replace SPEAKER_0, SPEAKER_1, etc. with simple Speaker labels\n                speaker_label = re.sub(\n                    r\"SPEAKER_\\d+\",\n                    lambda m: f\"Speaker {int(m.group(0).split('_')[1]) + 1}\",\n                    speaker,\n                )\n\n                start_time = format_timestamp(segment[\"start\"])\n                end_time = format_timestamp(segment[\"end\"])\n                text = segment[\"text\"].strip()\n\n                if self.show_speaker_names:\n                    prefix = self.speaker_name_pattern.format(speaker=speaker_label)\n                    text = f\"{prefix} {text}\"\n\n                f.write(f\"{i}\\n\")\n                f.write(f\"{start_time} --&gt; {end_time}\\n\")\n                f.write(f\"{text}\\n\\n\")\n\n        print(f\"Successfully created SRT file: {output_path}\")\n\n    def get_language(self) -&gt; str:\n        \"\"\"\n        Get the detected language.\n\n        Returns:\n            str: Detected language code\n        \"\"\"\n\n        return self._detected_language\n\n    def set_model(self, model_name: str) -&gt; None:\n        \"\"\"\n        Set the Whisper model name.\n\n        Args:\n            model_name: Whisper model name\n        \"\"\"\n\n        self.model_name = model_name\n\n    def set_language(self, language: str) -&gt; None:\n        \"\"\"\n        Set the language code.\n\n        Args:\n            language: Language code (e.g., 'en', 'hi', 'bn')\n        \"\"\"\n\n        self.language = language\n\n    def set_device(self, device: str) -&gt; None:\n        \"\"\"\n        Set the device for computation.\n\n        Args:\n            device: Device to run on (cpu, cuda, mps)\n        \"\"\"\n\n        self.device = device\n        if self.device == \"cpu\" and self.compute_type == \"float16\":\n            print(\"float16 not supported on CPU, using float32 instead.\")\n            self.compute_type = \"float32\"\n\n    def set_speakers(\n        self, min_speakers: Optional[int] = None, max_speakers: Optional[int] = None\n    ) -&gt; None:\n        \"\"\"\n        Set the number of speakers for diarization.\n\n        Args:\n            min_speakers: Minimum number of speakers\n            max_speakers: Maximum number of speakers\n        \"\"\"\n\n        self.min_speakers = min_speakers\n        self.max_speakers = max_speakers\n\n    def set_huggingface_token(self, hf_token: str) -&gt; None:\n        \"\"\"\n        Set the HuggingFace token for diarization.\n\n        Note: It's recommended to use environment variables or the HuggingFace CLI\n        login for better security rather than hardcoding tokens in your code.\n\n        Args:\n            hf_token: HuggingFace token\n        \"\"\"\n\n        self.hf_token = hf_token\n\n    def set_speaker_names_display(\n        self, show_speaker_names: bool, pattern: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"\n        Configure how speaker names are displayed.\n\n        Args:\n            show_speaker_names: Whether to show speaker names\n            pattern: Pattern for formatting speaker names (e.g., \"[{speaker}]\")\n        \"\"\"\n\n        self.show_speaker_names = show_speaker_names\n        if pattern is not None:\n            self.speaker_name_pattern = pattern\n\n    def set_line_properties(\n        self, max_length: int = 70, min_split_length: int = 50\n    ) -&gt; None:\n        \"\"\"\n        Configure subtitle line properties.\n\n        Args:\n            max_length: Maximum length of subtitle lines\n            min_split_length: Minimum characters before considering a line split\n        \"\"\"\n\n        self.max_line_length = max_length\n        self.min_char_length_splitter = min_split_length\n\n    def set_memory_management(self, clear_gpu_memory: bool) -&gt; None:\n        \"\"\"\n        Configure memory management.\n\n        Args:\n            clear_gpu_memory: Whether to clear GPU memory after major processing steps\n        \"\"\"\n\n        self.clear_gpu_memory = clear_gpu_memory\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.export_subtitle","title":"<code>export_subtitle(output_path)</code>","text":"<p>Export the processed transcription to an SRT subtitle file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to the output SRT file.</p> required Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def export_subtitle(self, output_path: str) -&gt; None:\n    \"\"\"\n    Export the processed transcription to an SRT subtitle file.\n\n    Args:\n        output_path: Path to the output SRT file.\n    \"\"\"\n\n    if self._processed_segments is None:\n        raise ValueError(\n            \"No processed audio available. Run `process_audio()` first.\"\n        )\n\n    # Ensure the output directory exists\n    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n    print(f\"Saving SRT to {output_path}...\")\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        for i, segment in enumerate(self._processed_segments, 1):\n            # Find the original segment that this processed segment came from\n            # by finding which original segment contains this timestamp\n            original_segment = next(\n                (\n                    s\n                    for s in self._segments_with_speakers\n                    if s[\"start\"] &lt;= segment[\"start\"]\n                    and s[\"end\"] &gt;= segment[\"start\"]\n                ),\n                {\"speaker\": \"Speaker\"},\n            )\n\n            speaker = original_segment.get(\"speaker\", \"Speaker\")\n            # Replace SPEAKER_0, SPEAKER_1, etc. with simple Speaker labels\n            speaker_label = re.sub(\n                r\"SPEAKER_\\d+\",\n                lambda m: f\"Speaker {int(m.group(0).split('_')[1]) + 1}\",\n                speaker,\n            )\n\n            start_time = format_timestamp(segment[\"start\"])\n            end_time = format_timestamp(segment[\"end\"])\n            text = segment[\"text\"].strip()\n\n            if self.show_speaker_names:\n                prefix = self.speaker_name_pattern.format(speaker=speaker_label)\n                text = f\"{prefix} {text}\"\n\n            f.write(f\"{i}\\n\")\n            f.write(f\"{start_time} --&gt; {end_time}\\n\")\n            f.write(f\"{text}\\n\\n\")\n\n    print(f\"Successfully created SRT file: {output_path}\")\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.get_language","title":"<code>get_language()</code>","text":"<p>Get the detected language.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Detected language code</p> Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def get_language(self) -&gt; str:\n    \"\"\"\n    Get the detected language.\n\n    Returns:\n        str: Detected language code\n    \"\"\"\n\n    return self._detected_language\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.process_audio","title":"<code>process_audio(audio_path)</code>","text":"<p>Process audio file to generate transcription with diarization.</p> <p>Parameters:</p> Name Type Description Default <code>audio_path</code> <code>str</code> <p>Path to the audio file.</p> required Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def process_audio(self, audio_path: str) -&gt; None:\n    \"\"\"\n    Process audio file to generate transcription with diarization.\n\n    Args:\n        audio_path: Path to the audio file.\n    \"\"\"\n\n    print(f\"Processing {audio_path}...\")\n\n    # 1. Load the audio file\n    audio = whisperx.load_audio(audio_path)\n\n    # 2. Load the ASR model (Whisper)\n    print(f\"Loading Whisper {self.model_name} model...\")\n    model = whisperx.load_model(\n        self.model_name,\n        device=self.device,\n        compute_type=self.compute_type,\n        language=self.language,\n        download_root=None,\n        local_files_only=False,\n        asr_options={\"beam_size\": 5},\n    )\n\n    # 3. Transcribe audio with whisperX\n    print(\"Transcribing audio...\")\n    result = model.transcribe(audio, batch_size=self.batch_size)\n\n    self._detected_language = result[\"language\"]\n    print(f\"Detected language: {self._detected_language}\")\n\n    # Clear GPU memory after transcription if requested\n    self._clear_gpu_memory(\"after transcription\")\n    if self.clear_gpu_memory and self.device == \"cuda\":\n        del model\n\n    # 4. Align whisper output\n    print(\"Aligning whisper output...\")\n    align_model, align_metadata = whisperx.load_align_model(\n        language_code=self._detected_language, device=self.device\n    )\n\n    result = whisperx.align(\n        result[\"segments\"],\n        align_model,\n        align_metadata,\n        audio,\n        self.device,\n        return_char_alignments=False,\n    )\n\n    self._transcript_result = result\n\n    # Clear GPU memory after alignment if requested\n    self._clear_gpu_memory(\"after alignment\")\n    if self.clear_gpu_memory and self.device == \"cuda\":\n        del align_model\n\n    # 5. Speaker diarization\n    if self.hf_token:\n        print(\"Running speaker diarization...\")\n        diarize_model = whisperx.DiarizationPipeline(\n            model_name=\"pyannote/speaker-diarization-3.1\",\n            use_auth_token=self.hf_token,\n            device=self.device,\n        )\n\n        self._diarize_segments = diarize_model(\n            audio, min_speakers=self.min_speakers, max_speakers=self.max_speakers\n        )\n\n        # Assign speaker labels to segments\n        print(\"Assigning speaker labels to segments...\")\n        result = whisperx.assign_word_speakers(\n            self._diarize_segments, self._transcript_result\n        )\n        self._segments_with_speakers = result[\"segments\"]\n\n        # Clear GPU memory after diarization if requested\n        self._clear_gpu_memory(\"after diarization\")\n        if self.clear_gpu_memory and self.device == \"cuda\":\n            del diarize_model\n    else:\n        print(\"Warning: No HuggingFace token found. Skipping diarization.\")\n        print(\"To use diarization, provide a huggingface token by one of these methods:\")\n        print(\"  1. Pass directly: transcriber = PodcastTranscriber(hf_token='your_token')\")\n        print(\"  2. Set environment variable: export HF_TOKEN='your_token'\")\n        print(\"  3. Login with CLI using: huggingface-cli login\")\n        self._segments_with_speakers = self._transcript_result[\"segments\"]\n        for segment in self._segments_with_speakers:\n            segment[\"speaker\"] = \"Speaker\"\n\n    # 6. Process subtitles with SubtitlesProcessor for line length control\n    print(\"Processing subtitles for optimal line length...\")\n    subtitles_processor = SubtitlesProcessor(\n        segments=self._segments_with_speakers,\n        lang=self._detected_language,\n        max_line_length=self.max_line_length,\n        min_char_length_splitter=self.min_char_length_splitter,\n    )\n\n    self._processed_segments = subtitles_processor.process_segments(\n        advanced_splitting=True\n    )\n\n    # Final memory cleanup\n    self._clear_gpu_memory(\"final cleanup\")\n\n    print(\"Audio processing completed successfully.\")\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_device","title":"<code>set_device(device)</code>","text":"<p>Set the device for computation.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>Device to run on (cpu, cuda, mps)</p> required Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def set_device(self, device: str) -&gt; None:\n    \"\"\"\n    Set the device for computation.\n\n    Args:\n        device: Device to run on (cpu, cuda, mps)\n    \"\"\"\n\n    self.device = device\n    if self.device == \"cpu\" and self.compute_type == \"float16\":\n        print(\"float16 not supported on CPU, using float32 instead.\")\n        self.compute_type = \"float32\"\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_huggingface_token","title":"<code>set_huggingface_token(hf_token)</code>","text":"<p>Set the HuggingFace token for diarization.</p> <p>Note: It's recommended to use environment variables or the HuggingFace CLI login for better security rather than hardcoding tokens in your code.</p> <p>Parameters:</p> Name Type Description Default <code>hf_token</code> <code>str</code> <p>HuggingFace token</p> required Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def set_huggingface_token(self, hf_token: str) -&gt; None:\n    \"\"\"\n    Set the HuggingFace token for diarization.\n\n    Note: It's recommended to use environment variables or the HuggingFace CLI\n    login for better security rather than hardcoding tokens in your code.\n\n    Args:\n        hf_token: HuggingFace token\n    \"\"\"\n\n    self.hf_token = hf_token\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_language","title":"<code>set_language(language)</code>","text":"<p>Set the language code.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>Language code (e.g., 'en', 'hi', 'bn')</p> required Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def set_language(self, language: str) -&gt; None:\n    \"\"\"\n    Set the language code.\n\n    Args:\n        language: Language code (e.g., 'en', 'hi', 'bn')\n    \"\"\"\n\n    self.language = language\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_line_properties","title":"<code>set_line_properties(max_length=70, min_split_length=50)</code>","text":"<p>Configure subtitle line properties.</p> <p>Parameters:</p> Name Type Description Default <code>max_length</code> <code>int</code> <p>Maximum length of subtitle lines</p> <code>70</code> <code>min_split_length</code> <code>int</code> <p>Minimum characters before considering a line split</p> <code>50</code> Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def set_line_properties(\n    self, max_length: int = 70, min_split_length: int = 50\n) -&gt; None:\n    \"\"\"\n    Configure subtitle line properties.\n\n    Args:\n        max_length: Maximum length of subtitle lines\n        min_split_length: Minimum characters before considering a line split\n    \"\"\"\n\n    self.max_line_length = max_length\n    self.min_char_length_splitter = min_split_length\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_memory_management","title":"<code>set_memory_management(clear_gpu_memory)</code>","text":"<p>Configure memory management.</p> <p>Parameters:</p> Name Type Description Default <code>clear_gpu_memory</code> <code>bool</code> <p>Whether to clear GPU memory after major processing steps</p> required Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def set_memory_management(self, clear_gpu_memory: bool) -&gt; None:\n    \"\"\"\n    Configure memory management.\n\n    Args:\n        clear_gpu_memory: Whether to clear GPU memory after major processing steps\n    \"\"\"\n\n    self.clear_gpu_memory = clear_gpu_memory\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_model","title":"<code>set_model(model_name)</code>","text":"<p>Set the Whisper model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Whisper model name</p> required Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def set_model(self, model_name: str) -&gt; None:\n    \"\"\"\n    Set the Whisper model name.\n\n    Args:\n        model_name: Whisper model name\n    \"\"\"\n\n    self.model_name = model_name\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_speaker_names_display","title":"<code>set_speaker_names_display(show_speaker_names, pattern=None)</code>","text":"<p>Configure how speaker names are displayed.</p> <p>Parameters:</p> Name Type Description Default <code>show_speaker_names</code> <code>bool</code> <p>Whether to show speaker names</p> required <code>pattern</code> <code>Optional[str]</code> <p>Pattern for formatting speaker names (e.g., \"[{speaker}]\")</p> <code>None</code> Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def set_speaker_names_display(\n    self, show_speaker_names: bool, pattern: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Configure how speaker names are displayed.\n\n    Args:\n        show_speaker_names: Whether to show speaker names\n        pattern: Pattern for formatting speaker names (e.g., \"[{speaker}]\")\n    \"\"\"\n\n    self.show_speaker_names = show_speaker_names\n    if pattern is not None:\n        self.speaker_name_pattern = pattern\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.PodcastTranscriber.set_speakers","title":"<code>set_speakers(min_speakers=None, max_speakers=None)</code>","text":"<p>Set the number of speakers for diarization.</p> <p>Parameters:</p> Name Type Description Default <code>min_speakers</code> <code>Optional[int]</code> <p>Minimum number of speakers</p> <code>None</code> <code>max_speakers</code> <code>Optional[int]</code> <p>Maximum number of speakers</p> <code>None</code> Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def set_speakers(\n    self, min_speakers: Optional[int] = None, max_speakers: Optional[int] = None\n) -&gt; None:\n    \"\"\"\n    Set the number of speakers for diarization.\n\n    Args:\n        min_speakers: Minimum number of speakers\n        max_speakers: Maximum number of speakers\n    \"\"\"\n\n    self.min_speakers = min_speakers\n    self.max_speakers = max_speakers\n</code></pre>"},{"location":"audim/aud2sub/transcribers/podcast/#audim.aud2sub.transcribers.podcast.format_timestamp","title":"<code>format_timestamp(seconds)</code>","text":"<p>Convert seconds to SRT timestamp format (HH:MM:SS,mmm)</p> Source code in <code>audim/aud2sub/transcribers/podcast.py</code> <pre><code>def format_timestamp(seconds: float) -&gt; str:\n    \"\"\"\n    Convert seconds to SRT timestamp format (HH:MM:SS,mmm)\n    \"\"\"\n\n    td = datetime.timedelta(seconds=seconds)\n    ms = int((seconds - int(seconds)) * 1000)\n    hours, remainder = divmod(td.seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{ms:03d}\"\n</code></pre>"},{"location":"audim/sub2pod/core/","title":"Core (Video Generation Module)","text":"<p>The core module is the main Video Generation Engine. It is responsible for the overall structure and flow of the podcast video.</p> <p>It uses a layout object to define the visual arrangement of the video, which internally uses a collection of elements and their effects to define the components of each frame in the video and their animations and transitions.</p> <p>Below is the API documentation for the core module:</p>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator","title":"<code>VideoGenerator</code>","text":"<p>Core engine for generating videos from SRT files</p> <p>This class is responsible for generating video frames from an SRT or subtitle file. The subtitle file must follow our extended SRT format, which adds speaker identification:</p> <ul> <li>Standard SRT format with sequential numbering, timestamps, and text content</li> <li>Speaker identification in square brackets at the beginning of each subtitle text   Example: \"[Host] Welcome to our podcast!\"</li> </ul> <p>Example of expected SRT format: <pre><code>1\n00:00:00,000 --&gt; 00:00:04,500\n[Host] Welcome to our podcast!\n\n2\n00:00:04,600 --&gt; 00:00:08,200\n[Guest] Thank you! Glad to be here.\n</code></pre></p> <p>The speaker tag is used to visually distinguish different speakers in the generated video, and is mandatory for the core engine to work.</p> <p>It uses a layout object to define the visual arrangement of the video.</p> Source code in <code>audim/sub2pod/core.py</code> <pre><code>class VideoGenerator:\n    \"\"\"\n    Core engine for generating videos from SRT files\n\n    This class is responsible for generating video frames from an SRT or subtitle file.\n    The subtitle file must follow our extended SRT format,\n    which adds speaker identification:\n\n    - Standard SRT format with sequential numbering, timestamps, and text content\n    - Speaker identification in square brackets at the beginning of each subtitle text\n      Example: \"[Host] Welcome to our podcast!\"\n\n    Example of expected SRT format:\n    ```srt\n    1\n    00:00:00,000 --&gt; 00:00:04,500\n    [Host] Welcome to our podcast!\n\n    2\n    00:00:04,600 --&gt; 00:00:08,200\n    [Guest] Thank you! Glad to be here.\n    ```\n\n    The speaker tag is used to visually distinguish different speakers in the\n    generated video, and is mandatory for the core engine to work.\n\n    It uses a layout object to define the visual arrangement of the video.\n    \"\"\"\n\n    def __init__(self, layout, fps=30, batch_size=300):\n        \"\"\"\n        Initialize the video generator\n\n        Args:\n            layout: Layout object that defines the visual arrangement\n            fps (int): Frames per second for the output video\n            batch_size (int): Number of frames to process in a batch\n                              before writing to disk\n        \"\"\"\n\n        self.layout = layout\n        self.fps = fps\n        self.batch_size = batch_size\n        self.audio_path = None\n        self.logo_path = None\n        self.title = None\n        self.temp_dir = None\n        self.frame_files = []\n        self.total_frames = 0\n\n    def generate_from_srt(\n        self,\n        srt_path,\n        audio_path=None,\n        logo_path=None,\n        title=None,\n        cpu_core_utilization=\"most\",\n    ):\n        \"\"\"\n        Generate video frames from an SRT file\n\n        Args:\n            srt_path (str): Path to the SRT file\n            audio_path (str, optional): Path to the audio file\n            logo_path (str, optional): Path to the logo image\n            title (str, optional): Title for the video\n            cpu_core_utilization (str, optional): `'single'`, `'half'`, `'most'`,\n                `'max'`\n\n                - `single`: Uses 1 CPU core\n                - `half`: Uses half of available CPU cores\n                - `most`: (default) Uses all available CPU cores except one\n                - `max`: Uses all available CPU cores for maximum performance\n        \"\"\"\n\n        # Store paths for later use\n        self.audio_path = audio_path\n        self.logo_path = logo_path\n        self.title = title\n\n        # Update layout with logo and title\n        if hasattr(self.layout, \"logo_path\"):\n            self.layout.logo_path = logo_path\n        if hasattr(self.layout, \"title\"):\n            self.layout.title = title\n\n        # Load SRT file\n        logger.info(f\"Loading subtitles from {srt_path}\")\n        subs = pysrt.open(srt_path)\n\n        # Determine if we need to normalize the timestamps\n        # Find the minimum start time (ordinal) from all subtitles\n        min_start_ordinal = min(sub.start.ordinal for sub in subs) if subs else 0\n        logger.info(f\"SRT starts at {min_start_ordinal} milliseconds\")\n\n        # Create temporary directory for frame storage\n        self.temp_dir = tempfile.mkdtemp()\n        self.frame_files = []\n        self.total_frames = 0\n\n        # Determine optimal number of workers\n        if cpu_core_utilization == \"single\":\n            num_workers = 1\n        elif cpu_core_utilization == \"half\":\n            num_workers = max(1, multiprocessing.cpu_count() // 2)\n        elif cpu_core_utilization == \"most\":\n            num_workers = max(1, multiprocessing.cpu_count() - 1)\n        elif cpu_core_utilization == \"max\":\n            num_workers = max(1, multiprocessing.cpu_count())\n        else:\n            raise ValueError(f\"Invalid CPU core utilities: {cpu_core_utilization}\")\n\n        logger.info(f\"Using {num_workers} CPU cores for parallel processing\")\n\n        # Process subtitles in parallel batches\n        with concurrent.futures.ProcessPoolExecutor(\n            max_workers=num_workers\n        ) as executor:\n            # Prepare subtitle batches for parallel processing\n            sub_batches = []\n            current_batch = []\n            current_batch_frames = 0\n\n            for sub in subs:\n                # Calculate the frame numbers normalized to start from frame 0\n                # This ensures compatibility with SRTs that start at any timestamp\n                start_frame = (sub.start.ordinal - min_start_ordinal) // (1000 // self.fps)\n                end_frame = (sub.end.ordinal - min_start_ordinal) // (1000 // self.fps)\n\n                num_frames = (end_frame - start_frame) + min(\n                    15, end_frame - start_frame\n                )  # Including fade frames\n\n                if (\n                    current_batch_frames + num_frames &gt; self.batch_size\n                    and current_batch\n                ):\n                    sub_batches.append((current_batch, min_start_ordinal))\n                    current_batch = []\n                    current_batch_frames = 0\n\n                current_batch.append(sub)\n                current_batch_frames += num_frames\n\n            # Add the last batch if not empty\n            if current_batch:\n                sub_batches.append((current_batch, min_start_ordinal))\n\n            logger.info(\n                f\"Processing subtitle to generate frames in {len(sub_batches)} batches\"\n            )\n\n            # Process each batch in parallel\n            batch_results = []\n            for batch_idx, (batch, offset) in enumerate(sub_batches):\n                batch_results.append(\n                    executor.submit(\n                        self._process_subtitle_batch,\n                        batch,\n                        batch_idx,\n                        self.layout,\n                        self.fps,\n                        self.temp_dir,\n                        offset,\n                    )\n                )\n\n            # Collect results with progress bar\n            with tqdm(\n                total=len(batch_results), desc=\"Processing batch\", unit=\"batch\"\n            ) as pbar:\n                for future in concurrent.futures.as_completed(batch_results):\n                    batch_frame_files, batch_frame_count = future.result()\n                    self.frame_files.extend(batch_frame_files)\n                    self.total_frames += batch_frame_count\n                    pbar.update(1)\n                    pbar.set_postfix({\"frames processed\": self.total_frames})\n\n        # Sort frame files by frame number to ensure correct sequence\n        self.frame_files.sort(\n            key=lambda x: int(os.path.basename(x).split(\"_\")[1].split(\".\")[0])\n        )\n\n        logger.info(\n            f\"Frame generation completed: Total {self.total_frames} frames created\"\n        )\n        return self\n\n    def _process_subtitle_batch(self, subs_batch, batch_index, layout, fps, temp_dir, time_offset=0):\n        \"\"\"\n        Process a batch of subtitles in parallel\n\n        Args:\n            subs_batch (list): List of subtitles to process\n            batch_index (int): Index of the current batch\n            layout: Layout object to use for frame creation\n            fps (int): Frames per second\n            temp_dir (str): Directory to store temporary files\n            time_offset (int): Time offset in milliseconds to normalize timestamps\n\n        Returns:\n            tuple: (list of frame files, number of frames processed)\n        \"\"\"\n\n        # Create a batch directory\n        batch_dir = os.path.join(temp_dir, f\"batch_{batch_index}\")\n        os.makedirs(batch_dir, exist_ok=True)\n\n        frame_files = []\n        frame_count = 0\n\n        # Process each subtitle in the batch\n        for sub in subs_batch:\n            # Normalize timestamps to start from time zero\n            start_frame = (sub.start.ordinal - time_offset) // (1000 // fps)\n            end_frame = (sub.end.ordinal - time_offset) // (1000 // fps)\n\n            # Calculate subtitle duration (in seconds)\n            subtitle_duration = (sub.end.ordinal - sub.start.ordinal) / 1000.0\n\n            # Get transition frames count from layout's transition effect\n            transition_frames = 15  # Default\n            if hasattr(layout, \"transition_effect\") and layout.transition_effect:\n                transition_frames = layout.transition_effect.frames\n\n            # Add transition frames\n            fade_frames = min(transition_frames, end_frame - start_frame)\n            for i in range(fade_frames):\n                # Calculate progress for transition effect\n                progress = i / fade_frames\n                # Convert progress to opacity for backward compatibility\n                opacity = int(progress * 255)\n\n                # Calculate subtitle position (in seconds)\n                subtitle_position = i / fps\n\n                # Create frame with transition effect passing position info as kwargs\n                frame = layout.create_frame(\n                    current_sub=sub,\n                    opacity=opacity,\n                    subtitle_position=subtitle_position,\n                    subtitle_duration=subtitle_duration,\n                )\n\n                frame_path = os.path.join(batch_dir, f\"frame_{start_frame + i:08d}.png\")\n\n                # Convert numpy array to PIL Image and save\n                if isinstance(frame, np.ndarray):\n                    Image.fromarray(frame).save(frame_path)\n                else:\n                    frame.save(frame_path)\n\n                frame_files.append(frame_path)\n                frame_count += 1\n\n            # Add main frames\n            for frame_idx in range(start_frame + fade_frames, end_frame):\n                # Calculate subtitle position for current frame\n                subtitle_position = (frame_idx - start_frame) / fps\n\n                # Create frame passing position info as kwargs\n                frame = layout.create_frame(\n                    current_sub=sub,\n                    subtitle_position=subtitle_position,\n                    subtitle_duration=subtitle_duration,\n                )\n\n                frame_path = os.path.join(batch_dir, f\"frame_{frame_idx:08d}.png\")\n\n                # Convert numpy array to PIL Image and save\n                if isinstance(frame, np.ndarray):\n                    Image.fromarray(frame).save(frame_path)\n                else:\n                    frame.save(frame_path)\n\n                frame_files.append(frame_path)\n                frame_count += 1\n\n        return frame_files, frame_count\n\n    def export_video(\n        self,\n        output_path,\n        encoder=\"auto\",\n        video_codec=None,\n        audio_codec=None,\n        video_bitrate=\"8M\",\n        audio_bitrate=\"192k\",\n        preset=\"medium\",\n        crf=23,\n        threads=None,\n        gpu_acceleration=True,\n        extra_ffmpeg_args=None,\n    ):\n        \"\"\"\n        Export the generated frames as a video\n\n        Args:\n            output_path (str): Path for the output video file\n            encoder (str): Encoding method to use:\n                `'ffmpeg'`, `'moviepy'`, or `'auto'` (default)\n            video_codec (str, optional): Video codec to use\n                (default: `'h264_nvenc'` for GPU, `'libx264'` for CPU)\n\n                - See [FFmpeg H.264 Guide](https://trac.ffmpeg.org/wiki/Encode/H.264)\n                  for CPU options\n                - See [NVIDIA FFmpeg Guide](https://developer.nvidia.com/blog/nvidia-ffmpeg-transcoding-guide/)\n                  for GPU options\n\n            audio_codec (str, optional): Audio codec to use (default: `'aac'`)\n\n                - See [FFmpeg AAC Guide](https://trac.ffmpeg.org/wiki/Encode/AAC)\n                  for audio codec options\n\n            video_bitrate (str, optional): Video bitrate (default: `'8M'`)\n            audio_bitrate (str, optional): Audio bitrate (default: `'192k'`)\n            preset (str, optional): Encoding preset (default: `'medium'`)\n\n                For CPU encoding (libx264):\n                    Options: `'ultrafast'`, `'superfast'`, `'veryfast'`, `'faster'`,\n                             `'fast'`, `'medium'`, `'slow'`, `'slower'`, `'veryslow'`\n                    Slower presets give better compression/quality at the cost of\n                    encoding time.\n\n                - See [FFmpeg Preset Guide](https://trac.ffmpeg.org/wiki/Encode/H.264#a2.Chooseapresetandtune)\n\n                For GPU encoding (NVENC):\n                    Will be automatically converted to NVENC presets:\n                    `'slow'`/`'slower'`/`'veryslow'` \u2192 `'p1'` (highest quality)\n                    `'medium'` \u2192 `'p3'` (balanced)\n                    `'fast'`/`'faster'` \u2192 `'p5'` (faster encoding)\n                    `'veryfast'`/`'superfast'`/`'ultrafast'` \u2192 `'p7'` (fastest encoding)\n\n                - See [NVIDIA FFmpeg Integration](https://docs.nvidia.com/video-technologies/video-codec-sdk/12.0/ffmpeg-with-nvidia-gpu/index.html)\n\n            crf (int, optional): Constant Rate Factor for quality\n                (default: `23`, lower is better quality)\n\n                - Range: `0-51`, where lower values mean better quality and larger\n                  file size\n                - Recommended range: `18-28`.\n                - See [CRF Guide](https://trac.ffmpeg.org/wiki/Encode/H.264#crf)\n\n            threads (int, optional): Number of encoding threads (default: CPU count - 1)\n            gpu_acceleration (bool, optional): Whether to use GPU acceleration\n                if available (default: `True`)\n            extra_ffmpeg_args (list, optional): Additional FFmpeg arguments as a list\n\n                - See [FFmpeg Documentation](https://ffmpeg.org/ffmpeg.html) for all\n                  available options\n        \"\"\"\n\n        logger.info(\n            f\"Starting video generation process with {self.total_frames} frames\"\n        )\n\n        # Calculate video duration\n        video_duration = self.total_frames / self.fps\n\n        # Determine audio duration if provided\n        audio_duration = None\n        if self.audio_path:\n            try:\n                # Import locally since this is a heavier dependency\n                from moviepy.editor import AudioFileClip\n\n                audio = AudioFileClip(self.audio_path)\n                audio_duration = audio.duration\n                audio.close()\n            except Exception as e:\n                logger.warning(f\"Could not determine audio duration: {e}\")\n\n        # Use the shorter duration to ensure sync\n        final_duration = video_duration\n        if audio_duration:\n            final_duration = min(video_duration, audio_duration)\n            logger.info(\n                f\"Video duration: {final_duration:.2f}s (adjusted to match audio)\"\n            )\n        else:\n            logger.info(f\"Video duration: {final_duration:.2f}s\")\n\n        # Sort frames by number to ensure correct sequence\n        self.frame_files.sort(\n            key=lambda x: int(os.path.basename(x).split(\"_\")[1].split(\".\")[0])\n        )\n\n        # Set default threads if not specified\n        if threads is None:\n            threads = max(4, os.cpu_count() - 1)\n\n        # Determine which encoder to use\n        if encoder == \"auto\":\n            try:\n                logger.info(\"Attempting video export with FFmpeg encoding\")\n                self._export_video_with_ffmpeg(\n                    output_path,\n                    final_duration,\n                    video_codec,\n                    audio_codec,\n                    video_bitrate,\n                    audio_bitrate,\n                    preset,\n                    crf,\n                    threads,\n                    gpu_acceleration,\n                    extra_ffmpeg_args,\n                )\n            except Exception as e:\n                logger.warning(f\"FFmpeg export failed: {e}\")\n                logger.info(\"Falling back to MoviePy for video encoding\")\n                self._export_video_with_moviepy(\n                    output_path,\n                    final_duration,\n                    video_codec,\n                    audio_codec,\n                    video_bitrate,\n                    audio_bitrate,\n                    preset,\n                    threads,\n                )\n        elif encoder == \"ffmpeg\":\n            logger.info(\"Starting video export using native FFmpeg\")\n            self._export_video_with_ffmpeg(\n                output_path,\n                final_duration,\n                video_codec,\n                audio_codec,\n                video_bitrate,\n                audio_bitrate,\n                preset,\n                crf,\n                threads,\n                gpu_acceleration,\n                extra_ffmpeg_args,\n            )\n        elif encoder == \"moviepy\":\n            logger.info(\"Starting video export using module MoviePy\")\n            self._export_video_with_moviepy(\n                output_path,\n                final_duration,\n                video_codec,\n                audio_codec,\n                video_bitrate,\n                audio_bitrate,\n                preset,\n                threads,\n            )\n        else:\n            logger.error(\n                f\"Invalid encoder: {encoder}. Choose 'ffmpeg', 'moviepy', or 'auto'\"\n            )\n            raise ValueError(\n                f\"Invalid encoder: {encoder}. Choose 'ffmpeg', 'moviepy', or 'auto'\"\n            )\n\n        # Clean up temporary files\n        try:\n            shutil.rmtree(self.temp_dir)\n            logger.info(f\"Cleaned up temporary files in {self.temp_dir}\")\n        except Exception as e:\n            logger.warning(f\"Could not clean up temporary files: {e}\")\n\n        logger.info(f\"Video generation completed! Exported to: {output_path}\")\n        return output_path\n\n    def _export_video_with_ffmpeg(\n        self,\n        output_path,\n        duration,\n        video_codec=None,\n        audio_codec=None,\n        video_bitrate=\"8M\",\n        audio_bitrate=\"192k\",\n        preset=\"medium\",\n        crf=23,\n        threads=None,\n        gpu_acceleration=True,\n        extra_args=None,\n    ):\n        \"\"\"\n        Export video using FFmpeg directly with potential GPU acceleration\n\n        Args:\n            output_path (str): Path for the output video file\n            duration (float): Duration of the video in seconds\n            video_codec (str, optional): Video codec to use\n            audio_codec (str, optional): Audio codec to use\n            video_bitrate (str, optional): Video bitrate\n            audio_bitrate (str, optional): Audio bitrate\n            preset (str, optional): Encoding preset\n            crf (int, optional): Constant Rate Factor for quality\n            threads (int, optional): Number of encoding threads\n            gpu_acceleration (bool): Whether to use GPU acceleration\n            extra_args (list, optional): Additional FFmpeg arguments\n        \"\"\"\n\n        # Prepare output directory\n        output_dir = os.path.dirname(output_path)\n        if output_dir and not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        # Create a temporary file listing all frames with precise timing\n        frames_list_file = os.path.join(self.temp_dir, \"frames_list.txt\")\n        frame_duration = 1.0 / self.fps\n\n        logger.info(\"Preparing frame list for FFmpeg\")\n        with open(frames_list_file, \"w\") as f:\n            for i, frame_file in enumerate(self.frame_files):\n                f.write(f\"file '{frame_file}'\\n\")\n                # Use exact frame duration to prevent drift\n                f.write(f\"duration {frame_duration}\\n\")\n\n        # Check for NVIDIA GPU with NVENC support if GPU acceleration is requested\n        has_nvidia = False\n        if gpu_acceleration and (video_codec is None or video_codec == \"h264_nvenc\"):\n            try:\n                nvidia_check = subprocess.run(\n                    [\"nvidia-smi\"],\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                    text=True,\n                )\n                has_nvidia = nvidia_check.returncode == 0\n            except FileNotFoundError:\n                pass\n\n        # Set default threads if not specified\n        if threads is None:\n            threads = max(4, os.cpu_count() - 1)\n\n        # Base FFmpeg command with improved sync options\n        ffmpeg_cmd = [\n            \"ffmpeg\",\n            \"-y\",\n            \"-f\",\n            \"concat\",\n            \"-safe\",\n            \"0\",\n            \"-i\",\n            frames_list_file,\n            \"-vsync\",\n            \"cfr\",  # Constant frame rate for better sync\n            \"-t\",\n            str(duration),\n        ]\n\n        # Add audio if provided\n        if self.audio_path:\n            ffmpeg_cmd.extend(\n                [\n                    \"-i\",\n                    self.audio_path,\n                    \"-t\",\n                    str(duration),\n                    \"-map\",\n                    \"0:v\",\n                    \"-map\",\n                    \"1:a\",\n                    \"-async\",\n                    \"1\",  # Better audio sync\n                ]\n            )\n\n        # Determine if we should use GPU encoding\n        use_gpu = (\n            has_nvidia\n            and gpu_acceleration\n            and (video_codec is None or video_codec == \"h264_nvenc\")\n        )\n\n        # Determine video codec and encoding settings\n        if use_gpu:\n            logger.info(\"Using NVIDIA GPU acceleration for video encoding\")\n            # Set default video codec for GPU\n            video_codec = \"h264_nvenc\"\n\n            # Convert x264 preset to NVENC preset\n            nvenc_preset = \"p3\"  # Default balanced preset\n            if preset in [\"veryslow\", \"slower\", \"slow\"]:\n                nvenc_preset = \"p1\"  # Highest quality\n            elif preset == \"medium\":\n                nvenc_preset = \"p3\"  # Balanced\n            elif preset in [\"fast\", \"faster\"]:\n                nvenc_preset = \"p5\"  # Faster encoding\n            elif preset in [\"veryfast\", \"superfast\", \"ultrafast\"]:\n                nvenc_preset = \"p7\"  # Fastest encoding\n\n            ffmpeg_cmd.extend(\n                [\n                    \"-c:v\",\n                    video_codec,\n                    \"-preset\",\n                    nvenc_preset,\n                    \"-tune\",\n                    \"hq\",\n                    \"-rc\",\n                    \"vbr\",\n                    \"-b:v\",\n                    video_bitrate,\n                    \"-maxrate\",\n                    str(float(video_bitrate.rstrip(\"M\")) * 1.25) + \"M\",\n                ]\n            )\n        else:\n            logger.info(f\"Using CPU encoding with {threads} threads\")\n            # Set default video codec for CPU if not specified\n            if video_codec is None:\n                video_codec = \"libx264\"\n\n            # For CPU encoding, use the x264 preset directly\n            ffmpeg_cmd.extend(\n                [\n                    \"-c:v\",\n                    video_codec,\n                    \"-preset\",\n                    preset,\n                    \"-crf\",\n                    str(crf),\n                    \"-threads\",\n                    str(threads),\n                ]\n            )\n\n            # Add tune parameter only for libx264\n            if video_codec == \"libx264\":\n                ffmpeg_cmd.extend([\"-tune\", \"film\"])\n\n        # Add audio encoding settings if audio is provided\n        if self.audio_path:\n            # Set default audio codec if not specified\n            if audio_codec is None:\n                audio_codec = \"aac\"\n\n            ffmpeg_cmd.extend([\"-c:a\", audio_codec, \"-b:a\", audio_bitrate])\n\n        # Add output format settings with improved sync options\n        ffmpeg_cmd.extend([\"-pix_fmt\", \"yuv420p\", \"-movflags\", \"+faststart\"])\n\n        # Add any extra arguments\n        if extra_args:\n            ffmpeg_cmd.extend(extra_args)\n\n        # Add output path\n        ffmpeg_cmd.append(output_path)\n\n        # Run FFmpeg\n        logger.info(\"Starting FFmpeg encoding process\")\n        logger.debug(f\"FFmpeg command: {' '.join(ffmpeg_cmd)}\")\n\n        # Run FFmpeg with progress indication\n        process = subprocess.Popen(\n            ffmpeg_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n        )\n\n        # Simple progress indicator since FFmpeg output is complex\n        with tqdm(total=100, desc=\"Encoding video\", unit=\"%\") as pbar:\n            last_progress = 0\n            for line in process.stdout:\n                # Try to extract progress information from FFmpeg output\n                if \"time=\" in line:\n                    try:\n                        time_str = line.split(\"time=\")[1].split()[0]\n                        h, m, s = time_str.split(\":\")\n                        current_time = float(h) * 3600 + float(m) * 60 + float(s)\n                        progress = min(int(current_time / duration * 100), 100)\n                        if progress &gt; last_progress:\n                            pbar.update(progress - last_progress)\n                            last_progress = progress\n                    except Exception:\n                        pass\n\n        process.wait()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, ffmpeg_cmd)\n\n        logger.info(f\"Video successfully encoded to {output_path}\")\n\n    def _export_video_with_moviepy(\n        self,\n        output_path,\n        duration,\n        video_codec=None,\n        audio_codec=None,\n        video_bitrate=\"8M\",\n        audio_bitrate=\"192k\",\n        preset=\"medium\",\n        threads=None,\n    ):\n        \"\"\"\n        Fallback method to export video using MoviePy\n\n        Args:\n            output_path (str): Path for the output video file\n            duration (float): Duration of the video in seconds\n            video_codec (str, optional): Video codec to use\n            audio_codec (str, optional): Audio codec to use\n            video_bitrate (str, optional): Video bitrate\n            audio_bitrate (str, optional): Audio bitrate\n            preset (str, optional): Encoding preset (x264 preset names)\n            threads (int, optional): Number of encoding threads\n        \"\"\"\n\n        # Import locally since this is a heavier dependency\n        from moviepy.editor import ImageSequenceClip\n\n        # Set default codecs if not specified\n        if video_codec is None:\n            video_codec = \"libx264\"\n        if audio_codec is None:\n            audio_codec = \"aac\"\n\n        # Set default threads if not specified\n        if threads is None:\n            threads = max(4, os.cpu_count() - 1)\n\n        logger.info(\"Loading frames for MoviePy\")\n        # Convert frames to video using the saved frame files\n        video = ImageSequenceClip(self.frame_files, fps=self.fps)\n\n        # Trim video to match duration\n        video = video.subclip(0, duration)\n\n        # Add audio if provided\n        if self.audio_path:\n            # Import locally since this is a heavier dependency\n            from moviepy.editor import AudioFileClip\n\n            logger.info(f\"Adding audio from {self.audio_path}\")\n            audio = AudioFileClip(self.audio_path)\n            audio = audio.subclip(0, duration)\n            video = video.set_audio(audio)\n\n        # Prepare ffmpeg parameters for MoviePy\n        ffmpeg_params = [\"-preset\", preset]\n\n        # Add bitrate parameter if specified\n        if video_bitrate:\n            ffmpeg_params.extend([\"-b:v\", video_bitrate])\n\n        # Export video\n        logger.info(f\"Starting MoviePy encoding with {video_codec} codec\")\n        video.write_videofile(\n            output_path,\n            codec=video_codec,\n            fps=self.fps,\n            threads=threads,\n            audio_codec=audio_codec,\n            bitrate=video_bitrate,\n            ffmpeg_params=ffmpeg_params,\n            logger=\"bar\",\n        )\n</code></pre>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator.__init__","title":"<code>__init__(layout, fps=30, batch_size=300)</code>","text":"<p>Initialize the video generator</p> <p>Parameters:</p> Name Type Description Default <code>layout</code> <p>Layout object that defines the visual arrangement</p> required <code>fps</code> <code>int</code> <p>Frames per second for the output video</p> <code>30</code> <code>batch_size</code> <code>int</code> <p>Number of frames to process in a batch               before writing to disk</p> <code>300</code> Source code in <code>audim/sub2pod/core.py</code> <pre><code>def __init__(self, layout, fps=30, batch_size=300):\n    \"\"\"\n    Initialize the video generator\n\n    Args:\n        layout: Layout object that defines the visual arrangement\n        fps (int): Frames per second for the output video\n        batch_size (int): Number of frames to process in a batch\n                          before writing to disk\n    \"\"\"\n\n    self.layout = layout\n    self.fps = fps\n    self.batch_size = batch_size\n    self.audio_path = None\n    self.logo_path = None\n    self.title = None\n    self.temp_dir = None\n    self.frame_files = []\n    self.total_frames = 0\n</code></pre>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator.export_video","title":"<code>export_video(output_path, encoder='auto', video_codec=None, audio_codec=None, video_bitrate='8M', audio_bitrate='192k', preset='medium', crf=23, threads=None, gpu_acceleration=True, extra_ffmpeg_args=None)</code>","text":"<p>Export the generated frames as a video</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path for the output video file</p> required <code>encoder</code> <code>str</code> <p>Encoding method to use: <code>'ffmpeg'</code>, <code>'moviepy'</code>, or <code>'auto'</code> (default)</p> <code>'auto'</code> <code>video_codec</code> <code>str</code> <p>Video codec to use (default: <code>'h264_nvenc'</code> for GPU, <code>'libx264'</code> for CPU)</p> <ul> <li>See FFmpeg H.264 Guide   for CPU options</li> <li>See NVIDIA FFmpeg Guide   for GPU options</li> </ul> <code>None</code> <code>audio_codec</code> <code>str</code> <p>Audio codec to use (default: <code>'aac'</code>)</p> <ul> <li>See FFmpeg AAC Guide   for audio codec options</li> </ul> <code>None</code> <code>video_bitrate</code> <code>str</code> <p>Video bitrate (default: <code>'8M'</code>)</p> <code>'8M'</code> <code>audio_bitrate</code> <code>str</code> <p>Audio bitrate (default: <code>'192k'</code>)</p> <code>'192k'</code> <code>preset</code> <code>str</code> <p>Encoding preset (default: <code>'medium'</code>)</p> <p>For CPU encoding (libx264):     Options: <code>'ultrafast'</code>, <code>'superfast'</code>, <code>'veryfast'</code>, <code>'faster'</code>,              <code>'fast'</code>, <code>'medium'</code>, <code>'slow'</code>, <code>'slower'</code>, <code>'veryslow'</code>     Slower presets give better compression/quality at the cost of     encoding time.</p> <ul> <li>See FFmpeg Preset Guide</li> </ul> <p>For GPU encoding (NVENC):     Will be automatically converted to NVENC presets:     <code>'slow'</code>/<code>'slower'</code>/<code>'veryslow'</code> \u2192 <code>'p1'</code> (highest quality)     <code>'medium'</code> \u2192 <code>'p3'</code> (balanced)     <code>'fast'</code>/<code>'faster'</code> \u2192 <code>'p5'</code> (faster encoding)     <code>'veryfast'</code>/<code>'superfast'</code>/<code>'ultrafast'</code> \u2192 <code>'p7'</code> (fastest encoding)</p> <ul> <li>See NVIDIA FFmpeg Integration</li> </ul> <code>'medium'</code> <code>crf</code> <code>int</code> <p>Constant Rate Factor for quality (default: <code>23</code>, lower is better quality)</p> <ul> <li>Range: <code>0-51</code>, where lower values mean better quality and larger   file size</li> <li>Recommended range: <code>18-28</code>.</li> <li>See CRF Guide</li> </ul> <code>23</code> <code>threads</code> <code>int</code> <p>Number of encoding threads (default: CPU count - 1)</p> <code>None</code> <code>gpu_acceleration</code> <code>bool</code> <p>Whether to use GPU acceleration if available (default: <code>True</code>)</p> <code>True</code> <code>extra_ffmpeg_args</code> <code>list</code> <p>Additional FFmpeg arguments as a list</p> <ul> <li>See FFmpeg Documentation for all   available options</li> </ul> <code>None</code> Source code in <code>audim/sub2pod/core.py</code> <pre><code>def export_video(\n    self,\n    output_path,\n    encoder=\"auto\",\n    video_codec=None,\n    audio_codec=None,\n    video_bitrate=\"8M\",\n    audio_bitrate=\"192k\",\n    preset=\"medium\",\n    crf=23,\n    threads=None,\n    gpu_acceleration=True,\n    extra_ffmpeg_args=None,\n):\n    \"\"\"\n    Export the generated frames as a video\n\n    Args:\n        output_path (str): Path for the output video file\n        encoder (str): Encoding method to use:\n            `'ffmpeg'`, `'moviepy'`, or `'auto'` (default)\n        video_codec (str, optional): Video codec to use\n            (default: `'h264_nvenc'` for GPU, `'libx264'` for CPU)\n\n            - See [FFmpeg H.264 Guide](https://trac.ffmpeg.org/wiki/Encode/H.264)\n              for CPU options\n            - See [NVIDIA FFmpeg Guide](https://developer.nvidia.com/blog/nvidia-ffmpeg-transcoding-guide/)\n              for GPU options\n\n        audio_codec (str, optional): Audio codec to use (default: `'aac'`)\n\n            - See [FFmpeg AAC Guide](https://trac.ffmpeg.org/wiki/Encode/AAC)\n              for audio codec options\n\n        video_bitrate (str, optional): Video bitrate (default: `'8M'`)\n        audio_bitrate (str, optional): Audio bitrate (default: `'192k'`)\n        preset (str, optional): Encoding preset (default: `'medium'`)\n\n            For CPU encoding (libx264):\n                Options: `'ultrafast'`, `'superfast'`, `'veryfast'`, `'faster'`,\n                         `'fast'`, `'medium'`, `'slow'`, `'slower'`, `'veryslow'`\n                Slower presets give better compression/quality at the cost of\n                encoding time.\n\n            - See [FFmpeg Preset Guide](https://trac.ffmpeg.org/wiki/Encode/H.264#a2.Chooseapresetandtune)\n\n            For GPU encoding (NVENC):\n                Will be automatically converted to NVENC presets:\n                `'slow'`/`'slower'`/`'veryslow'` \u2192 `'p1'` (highest quality)\n                `'medium'` \u2192 `'p3'` (balanced)\n                `'fast'`/`'faster'` \u2192 `'p5'` (faster encoding)\n                `'veryfast'`/`'superfast'`/`'ultrafast'` \u2192 `'p7'` (fastest encoding)\n\n            - See [NVIDIA FFmpeg Integration](https://docs.nvidia.com/video-technologies/video-codec-sdk/12.0/ffmpeg-with-nvidia-gpu/index.html)\n\n        crf (int, optional): Constant Rate Factor for quality\n            (default: `23`, lower is better quality)\n\n            - Range: `0-51`, where lower values mean better quality and larger\n              file size\n            - Recommended range: `18-28`.\n            - See [CRF Guide](https://trac.ffmpeg.org/wiki/Encode/H.264#crf)\n\n        threads (int, optional): Number of encoding threads (default: CPU count - 1)\n        gpu_acceleration (bool, optional): Whether to use GPU acceleration\n            if available (default: `True`)\n        extra_ffmpeg_args (list, optional): Additional FFmpeg arguments as a list\n\n            - See [FFmpeg Documentation](https://ffmpeg.org/ffmpeg.html) for all\n              available options\n    \"\"\"\n\n    logger.info(\n        f\"Starting video generation process with {self.total_frames} frames\"\n    )\n\n    # Calculate video duration\n    video_duration = self.total_frames / self.fps\n\n    # Determine audio duration if provided\n    audio_duration = None\n    if self.audio_path:\n        try:\n            # Import locally since this is a heavier dependency\n            from moviepy.editor import AudioFileClip\n\n            audio = AudioFileClip(self.audio_path)\n            audio_duration = audio.duration\n            audio.close()\n        except Exception as e:\n            logger.warning(f\"Could not determine audio duration: {e}\")\n\n    # Use the shorter duration to ensure sync\n    final_duration = video_duration\n    if audio_duration:\n        final_duration = min(video_duration, audio_duration)\n        logger.info(\n            f\"Video duration: {final_duration:.2f}s (adjusted to match audio)\"\n        )\n    else:\n        logger.info(f\"Video duration: {final_duration:.2f}s\")\n\n    # Sort frames by number to ensure correct sequence\n    self.frame_files.sort(\n        key=lambda x: int(os.path.basename(x).split(\"_\")[1].split(\".\")[0])\n    )\n\n    # Set default threads if not specified\n    if threads is None:\n        threads = max(4, os.cpu_count() - 1)\n\n    # Determine which encoder to use\n    if encoder == \"auto\":\n        try:\n            logger.info(\"Attempting video export with FFmpeg encoding\")\n            self._export_video_with_ffmpeg(\n                output_path,\n                final_duration,\n                video_codec,\n                audio_codec,\n                video_bitrate,\n                audio_bitrate,\n                preset,\n                crf,\n                threads,\n                gpu_acceleration,\n                extra_ffmpeg_args,\n            )\n        except Exception as e:\n            logger.warning(f\"FFmpeg export failed: {e}\")\n            logger.info(\"Falling back to MoviePy for video encoding\")\n            self._export_video_with_moviepy(\n                output_path,\n                final_duration,\n                video_codec,\n                audio_codec,\n                video_bitrate,\n                audio_bitrate,\n                preset,\n                threads,\n            )\n    elif encoder == \"ffmpeg\":\n        logger.info(\"Starting video export using native FFmpeg\")\n        self._export_video_with_ffmpeg(\n            output_path,\n            final_duration,\n            video_codec,\n            audio_codec,\n            video_bitrate,\n            audio_bitrate,\n            preset,\n            crf,\n            threads,\n            gpu_acceleration,\n            extra_ffmpeg_args,\n        )\n    elif encoder == \"moviepy\":\n        logger.info(\"Starting video export using module MoviePy\")\n        self._export_video_with_moviepy(\n            output_path,\n            final_duration,\n            video_codec,\n            audio_codec,\n            video_bitrate,\n            audio_bitrate,\n            preset,\n            threads,\n        )\n    else:\n        logger.error(\n            f\"Invalid encoder: {encoder}. Choose 'ffmpeg', 'moviepy', or 'auto'\"\n        )\n        raise ValueError(\n            f\"Invalid encoder: {encoder}. Choose 'ffmpeg', 'moviepy', or 'auto'\"\n        )\n\n    # Clean up temporary files\n    try:\n        shutil.rmtree(self.temp_dir)\n        logger.info(f\"Cleaned up temporary files in {self.temp_dir}\")\n    except Exception as e:\n        logger.warning(f\"Could not clean up temporary files: {e}\")\n\n    logger.info(f\"Video generation completed! Exported to: {output_path}\")\n    return output_path\n</code></pre>"},{"location":"audim/sub2pod/core/#audim.sub2pod.core.VideoGenerator.generate_from_srt","title":"<code>generate_from_srt(srt_path, audio_path=None, logo_path=None, title=None, cpu_core_utilization='most')</code>","text":"<p>Generate video frames from an SRT file</p> <p>Parameters:</p> Name Type Description Default <code>srt_path</code> <code>str</code> <p>Path to the SRT file</p> required <code>audio_path</code> <code>str</code> <p>Path to the audio file</p> <code>None</code> <code>logo_path</code> <code>str</code> <p>Path to the logo image</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the video</p> <code>None</code> <code>cpu_core_utilization</code> <code>str</code> <p><code>'single'</code>, <code>'half'</code>, <code>'most'</code>, <code>'max'</code></p> <ul> <li><code>single</code>: Uses 1 CPU core</li> <li><code>half</code>: Uses half of available CPU cores</li> <li><code>most</code>: (default) Uses all available CPU cores except one</li> <li><code>max</code>: Uses all available CPU cores for maximum performance</li> </ul> <code>'most'</code> Source code in <code>audim/sub2pod/core.py</code> <pre><code>def generate_from_srt(\n    self,\n    srt_path,\n    audio_path=None,\n    logo_path=None,\n    title=None,\n    cpu_core_utilization=\"most\",\n):\n    \"\"\"\n    Generate video frames from an SRT file\n\n    Args:\n        srt_path (str): Path to the SRT file\n        audio_path (str, optional): Path to the audio file\n        logo_path (str, optional): Path to the logo image\n        title (str, optional): Title for the video\n        cpu_core_utilization (str, optional): `'single'`, `'half'`, `'most'`,\n            `'max'`\n\n            - `single`: Uses 1 CPU core\n            - `half`: Uses half of available CPU cores\n            - `most`: (default) Uses all available CPU cores except one\n            - `max`: Uses all available CPU cores for maximum performance\n    \"\"\"\n\n    # Store paths for later use\n    self.audio_path = audio_path\n    self.logo_path = logo_path\n    self.title = title\n\n    # Update layout with logo and title\n    if hasattr(self.layout, \"logo_path\"):\n        self.layout.logo_path = logo_path\n    if hasattr(self.layout, \"title\"):\n        self.layout.title = title\n\n    # Load SRT file\n    logger.info(f\"Loading subtitles from {srt_path}\")\n    subs = pysrt.open(srt_path)\n\n    # Determine if we need to normalize the timestamps\n    # Find the minimum start time (ordinal) from all subtitles\n    min_start_ordinal = min(sub.start.ordinal for sub in subs) if subs else 0\n    logger.info(f\"SRT starts at {min_start_ordinal} milliseconds\")\n\n    # Create temporary directory for frame storage\n    self.temp_dir = tempfile.mkdtemp()\n    self.frame_files = []\n    self.total_frames = 0\n\n    # Determine optimal number of workers\n    if cpu_core_utilization == \"single\":\n        num_workers = 1\n    elif cpu_core_utilization == \"half\":\n        num_workers = max(1, multiprocessing.cpu_count() // 2)\n    elif cpu_core_utilization == \"most\":\n        num_workers = max(1, multiprocessing.cpu_count() - 1)\n    elif cpu_core_utilization == \"max\":\n        num_workers = max(1, multiprocessing.cpu_count())\n    else:\n        raise ValueError(f\"Invalid CPU core utilities: {cpu_core_utilization}\")\n\n    logger.info(f\"Using {num_workers} CPU cores for parallel processing\")\n\n    # Process subtitles in parallel batches\n    with concurrent.futures.ProcessPoolExecutor(\n        max_workers=num_workers\n    ) as executor:\n        # Prepare subtitle batches for parallel processing\n        sub_batches = []\n        current_batch = []\n        current_batch_frames = 0\n\n        for sub in subs:\n            # Calculate the frame numbers normalized to start from frame 0\n            # This ensures compatibility with SRTs that start at any timestamp\n            start_frame = (sub.start.ordinal - min_start_ordinal) // (1000 // self.fps)\n            end_frame = (sub.end.ordinal - min_start_ordinal) // (1000 // self.fps)\n\n            num_frames = (end_frame - start_frame) + min(\n                15, end_frame - start_frame\n            )  # Including fade frames\n\n            if (\n                current_batch_frames + num_frames &gt; self.batch_size\n                and current_batch\n            ):\n                sub_batches.append((current_batch, min_start_ordinal))\n                current_batch = []\n                current_batch_frames = 0\n\n            current_batch.append(sub)\n            current_batch_frames += num_frames\n\n        # Add the last batch if not empty\n        if current_batch:\n            sub_batches.append((current_batch, min_start_ordinal))\n\n        logger.info(\n            f\"Processing subtitle to generate frames in {len(sub_batches)} batches\"\n        )\n\n        # Process each batch in parallel\n        batch_results = []\n        for batch_idx, (batch, offset) in enumerate(sub_batches):\n            batch_results.append(\n                executor.submit(\n                    self._process_subtitle_batch,\n                    batch,\n                    batch_idx,\n                    self.layout,\n                    self.fps,\n                    self.temp_dir,\n                    offset,\n                )\n            )\n\n        # Collect results with progress bar\n        with tqdm(\n            total=len(batch_results), desc=\"Processing batch\", unit=\"batch\"\n        ) as pbar:\n            for future in concurrent.futures.as_completed(batch_results):\n                batch_frame_files, batch_frame_count = future.result()\n                self.frame_files.extend(batch_frame_files)\n                self.total_frames += batch_frame_count\n                pbar.update(1)\n                pbar.set_postfix({\"frames processed\": self.total_frames})\n\n    # Sort frame files by frame number to ensure correct sequence\n    self.frame_files.sort(\n        key=lambda x: int(os.path.basename(x).split(\"_\")[1].split(\".\")[0])\n    )\n\n    logger.info(\n        f\"Frame generation completed: Total {self.total_frames} frames created\"\n    )\n    return self\n</code></pre>"},{"location":"audim/sub2pod/effects/highlights/","title":"Highlights","text":"<p>Highlight effects allow you to create highlight effects to elements within the video frames. It is useful when you want to create a highlight effect to an element, or a group of elements or sections of area within the frame.</p> <p>As of now, the following highlight effects are available:</p> <ul> <li><code>none</code>: No highlight (default)</li> <li><code>pulse</code>: Pulsing highlight</li> <li><code>glow</code>: Glowing highlight</li> <li><code>box</code>: Box highlight</li> <li><code>underline</code>: Underline highlight</li> </ul> <p>Below is the API documentation for the highlight effects:</p> <p>Highlight effects for videos</p> <p>This module provides highlight effects that can be applied to text or other elements during video generation. Highlights are used to emphasize important parts of the content.</p>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.BaseHighlight","title":"<code>BaseHighlight</code>","text":"<p>Base class for all highlight effects (internal use only)</p> Source code in <code>audim/sub2pod/effects/highlights.py</code> <pre><code>class BaseHighlight:\n    \"\"\"Base class for all highlight effects (internal use only)\"\"\"\n\n    def __init__(self, color=(255, 255, 0, 128), padding=5):\n        \"\"\"\n        Initialize the highlight effect\n\n        Args:\n            color (tuple): RGBA color for the highlight\n                (default: semi-transparent yellow)\n            padding (int): Padding around the highlighted area in pixels\n        \"\"\"\n        self.color = color\n        self.padding = padding\n\n    def apply(self, frame, area, **kwargs):\n        \"\"\"\n        Apply the highlight effect to a specific area of a frame\n\n        Args:\n            frame: The frame to apply the effect to (PIL Image or numpy array)\n            area (tuple): The area to highlight as (x1, y1, x2, y2)\n            **kwargs: Additional arguments specific to the highlight\n\n        Returns:\n            The modified frame with the highlight effect applied\n        \"\"\"\n\n        return frame\n</code></pre>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.BaseHighlight.__init__","title":"<code>__init__(color=(255, 255, 0, 128), padding=5)</code>","text":"<p>Initialize the highlight effect</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>tuple</code> <p>RGBA color for the highlight (default: semi-transparent yellow)</p> <code>(255, 255, 0, 128)</code> <code>padding</code> <code>int</code> <p>Padding around the highlighted area in pixels</p> <code>5</code> Source code in <code>audim/sub2pod/effects/highlights.py</code> <pre><code>def __init__(self, color=(255, 255, 0, 128), padding=5):\n    \"\"\"\n    Initialize the highlight effect\n\n    Args:\n        color (tuple): RGBA color for the highlight\n            (default: semi-transparent yellow)\n        padding (int): Padding around the highlighted area in pixels\n    \"\"\"\n    self.color = color\n    self.padding = padding\n</code></pre>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.BaseHighlight.apply","title":"<code>apply(frame, area, **kwargs)</code>","text":"<p>Apply the highlight effect to a specific area of a frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>The frame to apply the effect to (PIL Image or numpy array)</p> required <code>area</code> <code>tuple</code> <p>The area to highlight as (x1, y1, x2, y2)</p> required <code>**kwargs</code> <p>Additional arguments specific to the highlight</p> <code>{}</code> <p>Returns:</p> Type Description <p>The modified frame with the highlight effect applied</p> Source code in <code>audim/sub2pod/effects/highlights.py</code> <pre><code>def apply(self, frame, area, **kwargs):\n    \"\"\"\n    Apply the highlight effect to a specific area of a frame\n\n    Args:\n        frame: The frame to apply the effect to (PIL Image or numpy array)\n        area (tuple): The area to highlight as (x1, y1, x2, y2)\n        **kwargs: Additional arguments specific to the highlight\n\n    Returns:\n        The modified frame with the highlight effect applied\n    \"\"\"\n\n    return frame\n</code></pre>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.Highlight","title":"<code>Highlight</code>","text":"<p>Highlight effects for video elements</p> <p>This class provides various highlight effects that can be applied to text or other elements during video generation.</p> <p>Available effects: - \"pulse\": Pulsing highlight that grows and shrinks - \"glow\": Glowing highlight with blur effect - \"underline\": Simple underline highlight - \"box\": Box around the highlighted area - \"none\": No highlight effect</p> Source code in <code>audim/sub2pod/effects/highlights.py</code> <pre><code>class Highlight:\n    \"\"\"\n    Highlight effects for video elements\n\n    This class provides various highlight effects that can be applied to text\n    or other elements during video generation.\n\n    Available effects:\n    - \"pulse\": Pulsing highlight that grows and shrinks\n    - \"glow\": Glowing highlight with blur effect\n    - \"underline\": Simple underline highlight\n    - \"box\": Box around the highlighted area\n    - \"none\": No highlight effect\n    \"\"\"\n\n    def __init__(self, effect_type=\"none\", **kwargs):\n        \"\"\"\n        Initialize a highlight effect\n\n        Args:\n            effect_type (str): Type of highlight effect\n                \"pulse\": Pulsing highlight\n                \"glow\": Glowing highlight\n                \"underline\": Underline highlight\n                \"box\": Box highlight\n                \"none\": No highlight (default)\n            **kwargs: Additional parameters for the specific effect:\n                color (tuple): RGBA color for the highlight\n                padding (int): Padding around the highlighted area\n                min_size (float): For pulse, minimum size factor (e.g., 0.8)\n                max_size (float): For pulse, maximum size factor (e.g., 1.2)\n                blur_radius (int): Blur radius for glow effect\n                thickness (int): Line thickness for underline/box\n        \"\"\"\n        self.effect_type = effect_type.lower()\n\n        # Common parameters\n        self.color = kwargs.get(\"color\", (255, 255, 0, 128))  # Semi-transparent yellow\n        self.padding = kwargs.get(\"padding\", 5)\n\n        # Pulse-specific parameters\n        self.min_size = kwargs.get(\"min_size\", 0.8)\n        self.max_size = kwargs.get(\"max_size\", 1.2)\n\n        # Glow-specific parameters\n        self.blur_radius = kwargs.get(\"blur_radius\", 5)\n\n        # Line-based effect parameters\n        self.thickness = kwargs.get(\"thickness\", 3)\n\n    def apply(self, frame, area, progress=0.0, **kwargs):\n        \"\"\"\n        Apply the selected highlight effect to a specific area\n\n        Args:\n            frame: The frame to apply the effect to (PIL Image or numpy array)\n            area (tuple): Area to highlight as (x1, y1, x2, y2)\n            progress (float): Animation progress from 0.0 to 1.0\n            **kwargs: Additional arguments\n\n        Returns:\n            The modified frame with the highlight effect applied\n        \"\"\"\n        # Convert numpy array to PIL if needed\n        original_type = type(frame)\n        if isinstance(frame, np.ndarray):\n            frame = Image.fromarray(frame)\n\n        # If not a PIL image, return unchanged\n        if not isinstance(frame, Image.Image):\n            return frame\n\n        # Handle different effect types\n        if self.effect_type == \"pulse\":\n            result = self._apply_pulse(frame, area, progress)\n        elif self.effect_type == \"glow\":\n            result = self._apply_glow(frame, area, progress)\n        elif self.effect_type == \"underline\":\n            result = self._apply_underline(frame, area)\n        elif self.effect_type == \"box\":\n            result = self._apply_box(frame, area)\n        elif self.effect_type == \"none\":\n            result = frame\n        else:\n            # Default to pulse if unknown effect type\n            result = self._apply_pulse(frame, area, progress)\n\n        # Convert back to numpy array if input was numpy\n        if original_type == np.ndarray:\n            return np.array(result)\n\n        return result\n\n    def _apply_pulse(self, frame, area, progress):\n        \"\"\"Apply pulse highlight effect\"\"\"\n        # Ensure we're working with an RGBA image\n        if frame.mode != \"RGBA\":\n            frame = frame.convert(\"RGBA\")\n\n        # Create a copy to avoid modifying the original\n        result = frame.copy()\n        overlay = Image.new(\"RGBA\", frame.size, (0, 0, 0, 0))\n        draw = ImageDraw.Draw(overlay)\n\n        # Calculate pulse size\n        # Use sine wave to create smooth oscillation between min and max size\n        pulse_factor = self.min_size + (self.max_size - self.min_size) * (\n            (math.sin(progress * 2 * math.pi) + 1) / 2\n        )\n\n        # Calculate expanded/contracted area\n        x1, y1, x2, y2 = area\n        center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n        width, height = x2 - x1, y2 - y1\n\n        new_width = width * pulse_factor\n        new_height = height * pulse_factor\n\n        new_x1 = center_x - new_width / 2 - self.padding\n        new_y1 = center_y - new_height / 2 - self.padding\n        new_x2 = center_x + new_width / 2 + self.padding\n        new_y2 = center_y + new_height / 2 + self.padding\n\n        # Draw the highlight\n        draw.rectangle((new_x1, new_y1, new_x2, new_y2), fill=self.color, outline=None)\n\n        # Apply blur if requested\n        if self.blur_radius &gt; 0:\n            overlay = overlay.filter(ImageFilter.GaussianBlur(self.blur_radius))\n\n        # Composite the overlay with the original frame\n        result = Image.alpha_composite(result, overlay)\n\n        return result\n\n    def _apply_glow(self, frame, area, progress):\n        \"\"\"Apply glow highlight effect\"\"\"\n        # Ensure we're working with an RGBA image\n        if frame.mode != \"RGBA\":\n            frame = frame.convert(\"RGBA\")\n\n        # Create a copy to avoid modifying the original\n        result = frame.copy()\n        overlay = Image.new(\"RGBA\", frame.size, (0, 0, 0, 0))\n        draw = ImageDraw.Draw(overlay)\n\n        # Animate the glow opacity if needed\n        alpha = self.color[3]\n        if progress is not None:\n            # Make the glow pulse in opacity\n            alpha = int(alpha * (0.6 + 0.4 * math.sin(progress * 2 * math.pi)))\n\n        # Create the glow color with the animated alpha\n        glow_color = (self.color[0], self.color[1], self.color[2], alpha)\n\n        # Apply padding to the area\n        x1, y1, x2, y2 = area\n        x1 -= self.padding\n        y1 -= self.padding\n        x2 += self.padding\n        y2 += self.padding\n\n        # Draw the highlight\n        draw.rectangle((x1, y1, x2, y2), fill=glow_color)\n\n        # Apply blur\n        overlay = overlay.filter(ImageFilter.GaussianBlur(self.blur_radius))\n\n        # Composite the overlay with the original frame\n        result = Image.alpha_composite(result, overlay)\n\n        return result\n\n    def _apply_underline(self, frame, area):\n        \"\"\"Apply underline highlight effect\"\"\"\n        # Ensure we're working with an RGBA image\n        if frame.mode != \"RGBA\":\n            frame = frame.convert(\"RGBA\")\n\n        # Create a copy to avoid modifying the original\n        result = frame.copy()\n        overlay = Image.new(\"RGBA\", frame.size, (0, 0, 0, 0))\n        draw = ImageDraw.Draw(overlay)\n\n        # Get the underline position (bottom of the area)\n        x1, y1, x2, y2 = area\n\n        # Draw the underline with specified thickness\n        for i in range(self.thickness):\n            draw.line((x1, y2 + i, x2, y2 + i), fill=self.color)\n\n        # Composite the overlay with the original frame\n        result = Image.alpha_composite(result, overlay)\n\n        return result\n\n    def _apply_box(self, frame, area):\n        \"\"\"Apply box highlight effect\"\"\"\n        # Ensure we're working with an RGBA image\n        if frame.mode != \"RGBA\":\n            frame = frame.convert(\"RGBA\")\n\n        # Create a copy to avoid modifying the original\n        result = frame.copy()\n        overlay = Image.new(\"RGBA\", frame.size, (0, 0, 0, 0))\n        draw = ImageDraw.Draw(overlay)\n\n        # Apply padding to the area\n        x1, y1, x2, y2 = area\n        x1 -= self.padding\n        y1 -= self.padding\n        x2 += self.padding\n        y2 += self.padding\n\n        # Draw the box\n        draw.rectangle((x1, y1, x2, y2), outline=self.color, width=self.thickness)\n\n        # Composite the overlay with the original frame\n        result = Image.alpha_composite(result, overlay)\n\n        return result\n</code></pre>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.Highlight.__init__","title":"<code>__init__(effect_type='none', **kwargs)</code>","text":"<p>Initialize a highlight effect</p> <p>Parameters:</p> Name Type Description Default <code>effect_type</code> <code>str</code> <p>Type of highlight effect \"pulse\": Pulsing highlight \"glow\": Glowing highlight \"underline\": Underline highlight \"box\": Box highlight \"none\": No highlight (default)</p> <code>'none'</code> <code>**kwargs</code> <p>Additional parameters for the specific effect: color (tuple): RGBA color for the highlight padding (int): Padding around the highlighted area min_size (float): For pulse, minimum size factor (e.g., 0.8) max_size (float): For pulse, maximum size factor (e.g., 1.2) blur_radius (int): Blur radius for glow effect thickness (int): Line thickness for underline/box</p> <code>{}</code> Source code in <code>audim/sub2pod/effects/highlights.py</code> <pre><code>def __init__(self, effect_type=\"none\", **kwargs):\n    \"\"\"\n    Initialize a highlight effect\n\n    Args:\n        effect_type (str): Type of highlight effect\n            \"pulse\": Pulsing highlight\n            \"glow\": Glowing highlight\n            \"underline\": Underline highlight\n            \"box\": Box highlight\n            \"none\": No highlight (default)\n        **kwargs: Additional parameters for the specific effect:\n            color (tuple): RGBA color for the highlight\n            padding (int): Padding around the highlighted area\n            min_size (float): For pulse, minimum size factor (e.g., 0.8)\n            max_size (float): For pulse, maximum size factor (e.g., 1.2)\n            blur_radius (int): Blur radius for glow effect\n            thickness (int): Line thickness for underline/box\n    \"\"\"\n    self.effect_type = effect_type.lower()\n\n    # Common parameters\n    self.color = kwargs.get(\"color\", (255, 255, 0, 128))  # Semi-transparent yellow\n    self.padding = kwargs.get(\"padding\", 5)\n\n    # Pulse-specific parameters\n    self.min_size = kwargs.get(\"min_size\", 0.8)\n    self.max_size = kwargs.get(\"max_size\", 1.2)\n\n    # Glow-specific parameters\n    self.blur_radius = kwargs.get(\"blur_radius\", 5)\n\n    # Line-based effect parameters\n    self.thickness = kwargs.get(\"thickness\", 3)\n</code></pre>"},{"location":"audim/sub2pod/effects/highlights/#audim.sub2pod.effects.highlights.Highlight.apply","title":"<code>apply(frame, area, progress=0.0, **kwargs)</code>","text":"<p>Apply the selected highlight effect to a specific area</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>The frame to apply the effect to (PIL Image or numpy array)</p> required <code>area</code> <code>tuple</code> <p>Area to highlight as (x1, y1, x2, y2)</p> required <code>progress</code> <code>float</code> <p>Animation progress from 0.0 to 1.0</p> <code>0.0</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>The modified frame with the highlight effect applied</p> Source code in <code>audim/sub2pod/effects/highlights.py</code> <pre><code>def apply(self, frame, area, progress=0.0, **kwargs):\n    \"\"\"\n    Apply the selected highlight effect to a specific area\n\n    Args:\n        frame: The frame to apply the effect to (PIL Image or numpy array)\n        area (tuple): Area to highlight as (x1, y1, x2, y2)\n        progress (float): Animation progress from 0.0 to 1.0\n        **kwargs: Additional arguments\n\n    Returns:\n        The modified frame with the highlight effect applied\n    \"\"\"\n    # Convert numpy array to PIL if needed\n    original_type = type(frame)\n    if isinstance(frame, np.ndarray):\n        frame = Image.fromarray(frame)\n\n    # If not a PIL image, return unchanged\n    if not isinstance(frame, Image.Image):\n        return frame\n\n    # Handle different effect types\n    if self.effect_type == \"pulse\":\n        result = self._apply_pulse(frame, area, progress)\n    elif self.effect_type == \"glow\":\n        result = self._apply_glow(frame, area, progress)\n    elif self.effect_type == \"underline\":\n        result = self._apply_underline(frame, area)\n    elif self.effect_type == \"box\":\n        result = self._apply_box(frame, area)\n    elif self.effect_type == \"none\":\n        result = frame\n    else:\n        # Default to pulse if unknown effect type\n        result = self._apply_pulse(frame, area, progress)\n\n    # Convert back to numpy array if input was numpy\n    if original_type == np.ndarray:\n        return np.array(result)\n\n    return result\n</code></pre>"},{"location":"audim/sub2pod/effects/transitions/","title":"Transitions","text":"<p>Transition effects allow you to create transition effects to elements in the video frames or the frames themselves. It is useful when you want to create a smooth transition between two different scenes or frames, or individual elements.</p> <p>As of now, the following transition effects are available:</p> <ul> <li><code>none</code>: No transition (default)</li> <li><code>fade</code>: Fade-in transition</li> <li><code>slide</code>: Slide-in transition</li> </ul> <p>Below is the API documentation for the transition effects:</p> <p>Transition effects for videos</p> <p>This module provides transition effects that can be applied to frames during video generation. Transitions are used for fade-in, fade-out, dissolve and other similar effects between frames.</p>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.BaseTransition","title":"<code>BaseTransition</code>","text":"<p>Base class for all transition effects (internal use only)</p> Source code in <code>audim/sub2pod/effects/transitions.py</code> <pre><code>class BaseTransition:\n    \"\"\"Base class for all transition effects (internal use only)\"\"\"\n\n    def __init__(self, frames=15):\n        \"\"\"\n        Initialize the transition effect\n\n        Args:\n            frames (int): Number of frames for the transition\n        \"\"\"\n        self.frames = frames\n\n    def apply(self, frame, progress, **kwargs):\n        \"\"\"\n        Apply the transition effect to a frame\n\n        Args:\n            frame: The frame to apply the effect to (PIL Image or numpy array)\n            progress (float): Progress of the transition, from 0.0 to 1.0\n            **kwargs: Additional arguments specific to the transition\n\n        Returns:\n            The modified frame with the transition effect applied\n        \"\"\"\n\n        return frame\n</code></pre>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.BaseTransition.__init__","title":"<code>__init__(frames=15)</code>","text":"<p>Initialize the transition effect</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>int</code> <p>Number of frames for the transition</p> <code>15</code> Source code in <code>audim/sub2pod/effects/transitions.py</code> <pre><code>def __init__(self, frames=15):\n    \"\"\"\n    Initialize the transition effect\n\n    Args:\n        frames (int): Number of frames for the transition\n    \"\"\"\n    self.frames = frames\n</code></pre>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.BaseTransition.apply","title":"<code>apply(frame, progress, **kwargs)</code>","text":"<p>Apply the transition effect to a frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>The frame to apply the effect to (PIL Image or numpy array)</p> required <code>progress</code> <code>float</code> <p>Progress of the transition, from 0.0 to 1.0</p> required <code>**kwargs</code> <p>Additional arguments specific to the transition</p> <code>{}</code> <p>Returns:</p> Type Description <p>The modified frame with the transition effect applied</p> Source code in <code>audim/sub2pod/effects/transitions.py</code> <pre><code>def apply(self, frame, progress, **kwargs):\n    \"\"\"\n    Apply the transition effect to a frame\n\n    Args:\n        frame: The frame to apply the effect to (PIL Image or numpy array)\n        progress (float): Progress of the transition, from 0.0 to 1.0\n        **kwargs: Additional arguments specific to the transition\n\n    Returns:\n        The modified frame with the transition effect applied\n    \"\"\"\n\n    return frame\n</code></pre>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.Transition","title":"<code>Transition</code>","text":"<p>Transition effects for video frames</p> <p>This class provides various transition effects that can be applied to frames during video generation.</p> <p>Available effects: - \"fade\": Simple fade-in transition - \"slide\": Slide-in from the specified direction - \"none\": No transition effect</p> Source code in <code>audim/sub2pod/effects/transitions.py</code> <pre><code>class Transition:\n    \"\"\"\n    Transition effects for video frames\n\n    This class provides various transition effects that can be applied to frames\n    during video generation.\n\n    Available effects:\n    - \"fade\": Simple fade-in transition\n    - \"slide\": Slide-in from the specified direction\n    - \"none\": No transition effect\n    \"\"\"\n\n    def __init__(self, effect_type=\"none\", **kwargs):\n        \"\"\"\n        Initialize a transition effect\n\n        Args:\n            effect_type (str): Type of transition effect\n                \"fade\": Fade-in transition\n                \"slide\": Slide-in transition\n                \"none\": No transition (default)\n            **kwargs: Additional parameters for the specific effect:\n                frames (int): Number of frames for the transition\n                direction (str): Direction for slide transition\n                (\"left\", \"right\", \"up\", \"down\")\n        \"\"\"\n        self.effect_type = effect_type.lower()\n        self.frames = kwargs.get(\"frames\", 15)\n        self.direction = kwargs.get(\"direction\", \"left\")\n\n    def apply(self, frame, progress, **kwargs):\n        \"\"\"\n        Apply the selected transition effect to a frame\n\n        Args:\n            frame: The frame to apply the effect to (PIL Image or numpy array)\n            progress (float): Progress of the transition, from 0.0 to 1.0\n            **kwargs: Additional arguments that may include:\n                opacity_only (bool): If True, just return the opacity value\n                (for fade effect)\n\n        Returns:\n            The modified frame with the transition effect applied\n        \"\"\"\n        # Handle different effect types\n        if self.effect_type == \"fade\":\n            return self._apply_fade(frame, progress, **kwargs)\n        elif self.effect_type == \"slide\":\n            return self._apply_slide(frame, progress, **kwargs)\n        elif self.effect_type == \"none\":\n            return frame\n        else:\n            # Default to fade if unknown effect type\n            return self._apply_fade(frame, progress, **kwargs)\n\n    def _apply_fade(self, frame, progress, **kwargs):\n        \"\"\"\n        Apply fade-in effect to a frame\n\n        Args:\n            frame: The frame to apply the effect to (PIL Image or numpy array)\n            progress (float): Progress of the transition, from 0.0 to 1.0\n            **kwargs: Additional arguments\n\n        Returns:\n            The modified frame with fade-in effect applied\n        \"\"\"\n        opacity = int(progress * 255)\n\n        # If caller just wants the opacity value, return it\n        if kwargs.get(\"opacity_only\", False):\n            return opacity\n\n        # Handle different frame types\n        if isinstance(frame, np.ndarray):\n            # For numpy arrays, apply opacity to alpha channel\n            if frame.shape[2] == 4:  # Has alpha channel\n                frame[:, :, 3] = np.minimum(frame[:, :, 3], opacity)\n            return frame\n        elif isinstance(frame, Image.Image):\n            # For PIL images, use putalpha or convert as needed\n            if frame.mode == \"RGBA\":\n                # Get alpha channel, apply opacity, and put it back\n                alpha = frame.split()[3]\n                alpha = Image.eval(alpha, lambda a: min(a, opacity))\n                frame.putalpha(alpha)\n            elif frame.mode == \"RGB\":\n                # Convert to RGBA and add alpha channel\n                frame = frame.convert(\"RGBA\")\n                frame.putalpha(opacity)\n            return frame\n\n        # Unknown frame type, return unchanged\n        return frame\n\n    def _apply_slide(self, frame, progress, **kwargs):\n        \"\"\"\n        Apply slide-in effect to a frame\n\n        Args:\n            frame: The frame to apply the effect to (PIL Image or numpy array)\n            progress (float): Progress of the transition, from 0.0 to 1.0\n            **kwargs: Additional arguments\n\n        Returns:\n            The modified frame with slide-in effect applied\n        \"\"\"\n        # If caller just wants the opacity value (for backwards compatibility),\n        # calculate it based on progress\n        if kwargs.get(\"opacity_only\", False):\n            return int(progress * 255)\n\n        # If no frame provided, just return the opacity\n        if frame is None:\n            return int(progress * 255)\n\n        # Convert numpy array to PIL if needed\n        if isinstance(frame, np.ndarray):\n            frame = Image.fromarray(frame)\n\n        # If not a PIL image, return unchanged\n        if not isinstance(frame, Image.Image):\n            return frame\n\n        # Ensure we're working with an RGBA image\n        if frame.mode != \"RGBA\":\n            frame = frame.convert(\"RGBA\")\n\n        # Create a blank frame\n        width, height = frame.size\n        result = Image.new(\"RGBA\", (width, height), (0, 0, 0, 0))\n\n        # Calculate offset based on direction and progress\n        offset_x, offset_y = 0, 0\n        if self.direction == \"left\":\n            offset_x = int((1.0 - progress) * width)\n        elif self.direction == \"right\":\n            offset_x = int((progress - 1.0) * width)\n        elif self.direction == \"up\":\n            offset_y = int((1.0 - progress) * height)\n        elif self.direction == \"down\":\n            offset_y = int((progress - 1.0) * height)\n\n        # Also apply a fade-in effect with the slide for smoother transition\n        opacity = int(progress * 255)\n        frame_copy = frame.copy()\n\n        # Apply opacity to the frame\n        if frame_copy.mode == \"RGBA\":\n            alpha = frame_copy.split()[3]\n            alpha = Image.eval(alpha, lambda a: min(a, opacity))\n            frame_copy.putalpha(alpha)\n\n        # Paste the frame at the offset position\n        result.paste(frame_copy, (offset_x, offset_y), frame_copy)\n\n        # Convert back to numpy array if input was numpy\n        if isinstance(kwargs.get(\"original_frame\"), np.ndarray):\n            return np.array(result)\n\n        return result\n</code></pre>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.Transition.__init__","title":"<code>__init__(effect_type='none', **kwargs)</code>","text":"<p>Initialize a transition effect</p> <p>Parameters:</p> Name Type Description Default <code>effect_type</code> <code>str</code> <p>Type of transition effect \"fade\": Fade-in transition \"slide\": Slide-in transition \"none\": No transition (default)</p> <code>'none'</code> <code>**kwargs</code> <p>Additional parameters for the specific effect: frames (int): Number of frames for the transition direction (str): Direction for slide transition (\"left\", \"right\", \"up\", \"down\")</p> <code>{}</code> Source code in <code>audim/sub2pod/effects/transitions.py</code> <pre><code>def __init__(self, effect_type=\"none\", **kwargs):\n    \"\"\"\n    Initialize a transition effect\n\n    Args:\n        effect_type (str): Type of transition effect\n            \"fade\": Fade-in transition\n            \"slide\": Slide-in transition\n            \"none\": No transition (default)\n        **kwargs: Additional parameters for the specific effect:\n            frames (int): Number of frames for the transition\n            direction (str): Direction for slide transition\n            (\"left\", \"right\", \"up\", \"down\")\n    \"\"\"\n    self.effect_type = effect_type.lower()\n    self.frames = kwargs.get(\"frames\", 15)\n    self.direction = kwargs.get(\"direction\", \"left\")\n</code></pre>"},{"location":"audim/sub2pod/effects/transitions/#audim.sub2pod.effects.transitions.Transition.apply","title":"<code>apply(frame, progress, **kwargs)</code>","text":"<p>Apply the selected transition effect to a frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>The frame to apply the effect to (PIL Image or numpy array)</p> required <code>progress</code> <code>float</code> <p>Progress of the transition, from 0.0 to 1.0</p> required <code>**kwargs</code> <p>Additional arguments that may include: opacity_only (bool): If True, just return the opacity value (for fade effect)</p> <code>{}</code> <p>Returns:</p> Type Description <p>The modified frame with the transition effect applied</p> Source code in <code>audim/sub2pod/effects/transitions.py</code> <pre><code>def apply(self, frame, progress, **kwargs):\n    \"\"\"\n    Apply the selected transition effect to a frame\n\n    Args:\n        frame: The frame to apply the effect to (PIL Image or numpy array)\n        progress (float): Progress of the transition, from 0.0 to 1.0\n        **kwargs: Additional arguments that may include:\n            opacity_only (bool): If True, just return the opacity value\n            (for fade effect)\n\n    Returns:\n        The modified frame with the transition effect applied\n    \"\"\"\n    # Handle different effect types\n    if self.effect_type == \"fade\":\n        return self._apply_fade(frame, progress, **kwargs)\n    elif self.effect_type == \"slide\":\n        return self._apply_slide(frame, progress, **kwargs)\n    elif self.effect_type == \"none\":\n        return frame\n    else:\n        # Default to fade if unknown effect type\n        return self._apply_fade(frame, progress, **kwargs)\n</code></pre>"},{"location":"audim/sub2pod/elements/header/","title":"Header","text":"<p>The header element is a component that is usually used at the topmost position in the layout. It is responsible for displaying the podcast title and the host's profile picture.</p> <p>Below is the API documentation for the header element:</p>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header","title":"<code>Header</code>","text":"<p>Header component for podcast layouts</p> <p>This component is responsible for displaying the header at the top of the podcast video frame. It may include various elements like logo, title, host profile, guest profile, etc.</p> Source code in <code>audim/sub2pod/elements/header.py</code> <pre><code>class Header:\n    \"\"\"\n    Header component for podcast layouts\n\n    This component is responsible for displaying the header at the top of the\n    podcast video frame. It may include various elements like logo, title,\n    host profile, guest profile, etc.\n    \"\"\"\n\n    def __init__(self, height=150, background_color=(30, 30, 30)):\n        \"\"\"\n        Initialize the layout header\n\n        Args:\n            height (int): Height of the header, defaults to 150\n            background_color (tuple): RGB background color, defaults to RGB (30, 30, 30)\n            text_renderer (TextRenderer): optional text renderer for the header,\n                                          defaults to a new instance\n            logo (Image): optional logo image, defaults to None\n            logo_size (tuple): optional size of the logo, defaults to (100, 100)\n        \"\"\"\n\n        self.height = height\n        self.background_color = background_color\n        self.text_renderer = TextRenderer()\n        self.logo = None\n        self.logo_size = (100, 100)\n\n    def set_logo(self, logo_path, size=(100, 100)):\n        \"\"\"\n        Set the logo for the header\n\n        Args:\n            logo_path (str): Path to the logo image\n            size (tuple): Size of the logo, defaults to (100, 100)\n        \"\"\"\n\n        if logo_path:\n            self.logo = Image.open(logo_path).convert(\"RGBA\")\n            self.logo_size = size\n            self.logo = self.logo.resize(self.logo_size)\n        return self\n\n    def draw(self, frame, draw, width, title=\"My Podcast\", opacity=255):\n        \"\"\"\n        Draws the header on the frame\n\n        Args:\n            frame (Image): Frame to draw the header on\n            draw (ImageDraw): Draw object to draw on the frame\n            width (int): Width of the frame\n            title (str): Title of the podcast\n            opacity (int): Opacity of the header, defaults to 255\n        \"\"\"\n\n        # Draw header background\n        draw.rectangle([0, 0, width, self.height], fill=self.background_color + (255,))\n\n        # Add logo if available\n        if self.logo:\n            frame.paste(\n                self.logo,\n                (\n                    width - self.logo_size[0] - 50,\n                    (self.height - self.logo_size[1]) // 2,\n                ),\n                self.logo,\n            )\n\n        # Add title\n        self.text_renderer.draw_text(\n            draw,\n            title,\n            (width // 2, self.height // 2),\n            font_size=60,\n            color=(255, 255, 255, opacity),\n            anchor=\"mm\",\n        )\n</code></pre>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header.__init__","title":"<code>__init__(height=150, background_color=(30, 30, 30))</code>","text":"<p>Initialize the layout header</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Height of the header, defaults to 150</p> <code>150</code> <code>background_color</code> <code>tuple</code> <p>RGB background color, defaults to RGB (30, 30, 30)</p> <code>(30, 30, 30)</code> <code>text_renderer</code> <code>TextRenderer</code> <p>optional text renderer for the header,                           defaults to a new instance</p> required <code>logo</code> <code>Image</code> <p>optional logo image, defaults to None</p> required <code>logo_size</code> <code>tuple</code> <p>optional size of the logo, defaults to (100, 100)</p> required Source code in <code>audim/sub2pod/elements/header.py</code> <pre><code>def __init__(self, height=150, background_color=(30, 30, 30)):\n    \"\"\"\n    Initialize the layout header\n\n    Args:\n        height (int): Height of the header, defaults to 150\n        background_color (tuple): RGB background color, defaults to RGB (30, 30, 30)\n        text_renderer (TextRenderer): optional text renderer for the header,\n                                      defaults to a new instance\n        logo (Image): optional logo image, defaults to None\n        logo_size (tuple): optional size of the logo, defaults to (100, 100)\n    \"\"\"\n\n    self.height = height\n    self.background_color = background_color\n    self.text_renderer = TextRenderer()\n    self.logo = None\n    self.logo_size = (100, 100)\n</code></pre>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header.draw","title":"<code>draw(frame, draw, width, title='My Podcast', opacity=255)</code>","text":"<p>Draws the header on the frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>Image</code> <p>Frame to draw the header on</p> required <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>width</code> <code>int</code> <p>Width of the frame</p> required <code>title</code> <code>str</code> <p>Title of the podcast</p> <code>'My Podcast'</code> <code>opacity</code> <code>int</code> <p>Opacity of the header, defaults to 255</p> <code>255</code> Source code in <code>audim/sub2pod/elements/header.py</code> <pre><code>def draw(self, frame, draw, width, title=\"My Podcast\", opacity=255):\n    \"\"\"\n    Draws the header on the frame\n\n    Args:\n        frame (Image): Frame to draw the header on\n        draw (ImageDraw): Draw object to draw on the frame\n        width (int): Width of the frame\n        title (str): Title of the podcast\n        opacity (int): Opacity of the header, defaults to 255\n    \"\"\"\n\n    # Draw header background\n    draw.rectangle([0, 0, width, self.height], fill=self.background_color + (255,))\n\n    # Add logo if available\n    if self.logo:\n        frame.paste(\n            self.logo,\n            (\n                width - self.logo_size[0] - 50,\n                (self.height - self.logo_size[1]) // 2,\n            ),\n            self.logo,\n        )\n\n    # Add title\n    self.text_renderer.draw_text(\n        draw,\n        title,\n        (width // 2, self.height // 2),\n        font_size=60,\n        color=(255, 255, 255, opacity),\n        anchor=\"mm\",\n    )\n</code></pre>"},{"location":"audim/sub2pod/elements/header/#audim.sub2pod.elements.header.Header.set_logo","title":"<code>set_logo(logo_path, size=(100, 100))</code>","text":"<p>Set the logo for the header</p> <p>Parameters:</p> Name Type Description Default <code>logo_path</code> <code>str</code> <p>Path to the logo image</p> required <code>size</code> <code>tuple</code> <p>Size of the logo, defaults to (100, 100)</p> <code>(100, 100)</code> Source code in <code>audim/sub2pod/elements/header.py</code> <pre><code>def set_logo(self, logo_path, size=(100, 100)):\n    \"\"\"\n    Set the logo for the header\n\n    Args:\n        logo_path (str): Path to the logo image\n        size (tuple): Size of the logo, defaults to (100, 100)\n    \"\"\"\n\n    if logo_path:\n        self.logo = Image.open(logo_path).convert(\"RGBA\")\n        self.logo_size = size\n        self.logo = self.logo.resize(self.logo_size)\n    return self\n</code></pre>"},{"location":"audim/sub2pod/elements/profile/","title":"Profile","text":"<p>The profile element is a component that is used to display the profile picture (DP) of the host.</p> <p>Below is the API documentation for the profile element:</p>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture","title":"<code>ProfilePicture</code>","text":"<p>Handles user profile pictures or display picture with various shapes and effects</p> <p>This component is responsible for displaying the profile picture of the speaker. It may also include various shapes and effects like circle, square, highlight, etc.</p> Source code in <code>audim/sub2pod/elements/profile.py</code> <pre><code>class ProfilePicture:\n    \"\"\"\n    Handles user profile pictures or display picture with various shapes and effects\n\n    This component is responsible for displaying the profile picture of the speaker.\n    It may also include various shapes and effects like circle, square, highlight, etc.\n    \"\"\"\n\n    def __init__(self, image_path, size=(120, 120), shape=\"circle\"):\n        \"\"\"\n        Initialize a profile picture\n\n        Args:\n            image_path (str): Path to the profile image\n            size (tuple): Width and height of the profile picture\n            shape (str): Shape of the profile picture (\"circle\" or \"square\"),\n            defaults to \"circle\"\n        \"\"\"\n\n        self.image_path = image_path\n        self.size = size\n        self.shape = shape\n        self.image = self._load_and_process_image()\n\n    def _load_and_process_image(self):\n        \"\"\"\n        Load and process the profile image based on shape\n        (mostly for internal use)\n\n        Returns:\n            Image: Processed profile image\n        \"\"\"\n\n        img = Image.open(self.image_path).convert(\"RGBA\")\n        img = img.resize(self.size)\n\n        if self.shape == \"circle\":\n            mask = self._create_circular_mask()\n            img.putalpha(mask)\n        elif self.shape == \"square\":\n            mask = self._create_square_mask()\n            img.putalpha(mask)\n\n        return img\n\n    def _create_circular_mask(self):\n        \"\"\"\n        Create a circular mask for profile pictures\n        (mostly for internal use)\n\n        Returns:\n            Image: Circular mask\n        \"\"\"\n\n        mask = Image.new(\"L\", self.size, 0)\n        draw = ImageDraw.Draw(mask)\n        draw.ellipse((0, 0) + self.size, fill=255)\n        return mask\n\n    def _create_square_mask(self):\n        \"\"\"\n        Create a square mask for profile pictures\n        (mostly for internal use)\n\n        Returns:\n            Image: Square mask\n        \"\"\"\n\n        mask = Image.new(\"L\", self.size, 0)\n        draw = ImageDraw.Draw(mask)\n        draw.rectangle((0, 0) + self.size, fill=255)\n        return mask\n\n    def highlight(self, draw, position, color=(255, 200, 0), width=3, opacity=255):\n        \"\"\"\n        Add highlight around the profile picture\n\n        Args:\n            draw (ImageDraw): Draw object to draw on the frame\n            position (tuple): Position of the profile picture\n        \"\"\"\n\n        highlight_color = color + (opacity,)\n\n        if self.shape == \"circle\":\n            draw.ellipse(\n                [\n                    position[0] - width,\n                    position[1] - width,\n                    position[0] + self.size[0] + width,\n                    position[1] + self.size[1] + width,\n                ],\n                outline=highlight_color,\n                width=width,\n            )\n        else:\n            draw.rectangle(\n                [\n                    position[0] - width,\n                    position[1] - width,\n                    position[0] + self.size[0] + width,\n                    position[1] + self.size[1] + width,\n                ],\n                outline=highlight_color,\n                width=width,\n            )\n</code></pre>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture.__init__","title":"<code>__init__(image_path, size=(120, 120), shape='circle')</code>","text":"<p>Initialize a profile picture</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the profile image</p> required <code>size</code> <code>tuple</code> <p>Width and height of the profile picture</p> <code>(120, 120)</code> <code>shape</code> <code>str</code> <p>Shape of the profile picture (\"circle\" or \"square\"),</p> <code>'circle'</code> Source code in <code>audim/sub2pod/elements/profile.py</code> <pre><code>def __init__(self, image_path, size=(120, 120), shape=\"circle\"):\n    \"\"\"\n    Initialize a profile picture\n\n    Args:\n        image_path (str): Path to the profile image\n        size (tuple): Width and height of the profile picture\n        shape (str): Shape of the profile picture (\"circle\" or \"square\"),\n        defaults to \"circle\"\n    \"\"\"\n\n    self.image_path = image_path\n    self.size = size\n    self.shape = shape\n    self.image = self._load_and_process_image()\n</code></pre>"},{"location":"audim/sub2pod/elements/profile/#audim.sub2pod.elements.profile.ProfilePicture.highlight","title":"<code>highlight(draw, position, color=(255, 200, 0), width=3, opacity=255)</code>","text":"<p>Add highlight around the profile picture</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>position</code> <code>tuple</code> <p>Position of the profile picture</p> required Source code in <code>audim/sub2pod/elements/profile.py</code> <pre><code>def highlight(self, draw, position, color=(255, 200, 0), width=3, opacity=255):\n    \"\"\"\n    Add highlight around the profile picture\n\n    Args:\n        draw (ImageDraw): Draw object to draw on the frame\n        position (tuple): Position of the profile picture\n    \"\"\"\n\n    highlight_color = color + (opacity,)\n\n    if self.shape == \"circle\":\n        draw.ellipse(\n            [\n                position[0] - width,\n                position[1] - width,\n                position[0] + self.size[0] + width,\n                position[1] + self.size[1] + width,\n            ],\n            outline=highlight_color,\n            width=width,\n        )\n    else:\n        draw.rectangle(\n            [\n                position[0] - width,\n                position[1] - width,\n                position[0] + self.size[0] + width,\n                position[1] + self.size[1] + width,\n            ],\n            outline=highlight_color,\n            width=width,\n        )\n</code></pre>"},{"location":"audim/sub2pod/elements/text/","title":"Text","text":"<p>The text element is a component that is used to display the text, dialogue, or any other text content in the podcast.</p> <p>Below is the API documentation for the text element:</p>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer","title":"<code>TextRenderer</code>","text":"<p>Handles text rendering with various styles and wrapping</p> <p>This component is responsible for rendering text on the frame with various styles and wrapping. It can handle different fonts, sizes, colors, and anchor points.</p> Source code in <code>audim/sub2pod/elements/text.py</code> <pre><code>class TextRenderer:\n    \"\"\"\n    Handles text rendering with various styles and wrapping\n\n    This component is responsible for rendering text on the frame with various styles\n    and wrapping. It can handle different fonts, sizes, colors, and anchor points.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the text renderer with default fonts\n        \"\"\"\n\n        self.font_path = font_manager.findfont(\n            font_manager.FontProperties(family=[\"sans\"])\n        )\n        self.fonts = {}\n\n    def get_font(self, size):\n        \"\"\"\n        Get or create a font of the specified size\n\n        Args:\n            size (int): Size of the font\n        \"\"\"\n\n        if size not in self.fonts:\n            self.fonts[size] = ImageFont.truetype(self.font_path, size)\n        return self.fonts[size]\n\n    def draw_text(\n        self,\n        draw,\n        text,\n        position,\n        font_size=40,\n        color=(255, 255, 255, 255),\n        anchor=\"mm\",\n    ):\n        \"\"\"\n        Draw text at the specified position\n\n        Args:\n            draw (ImageDraw): Draw object to draw on the frame\n            text (str): Text to draw\n            position (tuple): Position of the text\n            font_size (int): Size of the font, defaults to 40\n            color (tuple): Color of the text, defaults to RGB (255, 255, 255, 255)\n            anchor (str): Anchor of the text (from PIL library), defaults to \"mm\".\n                          See [pillow docs: text anchors](https://pillow.readthedocs.io/en/latest/handbook/text-anchors.html)\n                          for all possible options.\n        \"\"\"\n\n        font = self.get_font(font_size)\n        color = self._sanitize_color(color)\n\n        draw.text(position, text, fill=color, font=font, anchor=anchor)\n\n    def draw_wrapped_text(\n        self,\n        draw,\n        text,\n        position,\n        max_width,\n        font_size=40,\n        color=(255, 255, 255, 255),\n        anchor=\"lm\",\n    ):\n        \"\"\"\n        Draw text with word wrapping\n\n        Args:\n            draw (ImageDraw): Draw object to draw on the frame\n            text (str): Text to draw\n            position (tuple): Position of the text\n            max_width (int): Maximum width of the text before wrapping\n            font_size (int): Size of the font, defaults to 40\n            color (tuple): Color of the text, defaults to RGB (255, 255, 255, 255)\n            anchor (str): Anchor of the text (from PIL library), defaults to \"lm\".\n                          See [pillow docs: text anchors](https://pillow.readthedocs.io/en/latest/handbook/text-anchors.html)\n                          for all possible options.\n        \"\"\"\n\n        font = self.get_font(font_size)\n        color = self._sanitize_color(color)\n\n        # Get font metrics for dynamic calculations\n        font_ascent, font_descent = font.getmetrics()\n        line_height = font_ascent + font_descent\n        line_spacing = line_height * 0.5  # 50% of line height for spacing\n        total_line_height = line_height + line_spacing\n\n        # Word wrap\n        words = text.split()\n        lines = []\n        current_line = []\n\n        for word in words:\n            current_line.append(word)\n            w = draw.textlength(\" \".join(current_line), font=font)\n            if w &gt; max_width:\n                current_line.pop()\n                lines.append(\" \".join(current_line))\n                current_line = [word]\n        lines.append(\" \".join(current_line))\n\n        # Calculate vertical offset for multiple lines to maintain center alignment\n        text_x, text_y = position\n        total_text_height = len(lines) * total_line_height\n\n        if anchor == \"lm\":\n            text_start_y = text_y - (total_text_height // 2) + (line_height // 2)\n        else:\n            text_start_y = text_y\n\n        # Draw each line\n        for i, line in enumerate(lines):\n            line_y = text_start_y + (i * total_line_height)\n            draw.text((text_x, line_y), line, fill=color, font=font, anchor=anchor)\n\n    def _sanitize_color(self, color):\n        \"\"\"\n        Normalize color input to a valid RGBA or integer value compatible with PIL.\n\n        Args:\n            color (int, tuple, or list): A color value\n\n        Returns:\n            A valid RGBA tuple (r, g, b, a) or int usable with PIL.\n        \"\"\"\n\n        # Handle the case where color is None\n        if color is None:\n            return (0, 0, 0, 255)\n\n        # If color is a single int, return it as is \n        # (used for opacity, grayscale or palette value)\n        if isinstance(color, int):\n            return color\n\n        # If color is a tuple or list, ensure all values are valid integers\n        if isinstance(color, (tuple, list)):\n            # Process and hard clip RGB color\n            if len(color) == 3:\n                return tuple(\n                    max(0, min(255, int(c) if c is not None else 0)) for c in color\n                )\n            # Process and hard clip RGBA color\n            elif len(color) == 4:\n                r, g, b, a = color\n                # Handle case where any component is None\n                r = max(0, min(255, int(r) if r is not None else 0))\n                g = max(0, min(255, int(g) if g is not None else 0))\n                b = max(0, min(255, int(b) if b is not None else 0))\n                a = max(0, min(255, int(a) if a is not None else 255))\n                return (r, g, b, a)\n\n        # For any other case, return black fully opaque\n        return (0, 0, 0, 255)\n</code></pre>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the text renderer with default fonts</p> Source code in <code>audim/sub2pod/elements/text.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the text renderer with default fonts\n    \"\"\"\n\n    self.font_path = font_manager.findfont(\n        font_manager.FontProperties(family=[\"sans\"])\n    )\n    self.fonts = {}\n</code></pre>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.draw_text","title":"<code>draw_text(draw, text, position, font_size=40, color=(255, 255, 255, 255), anchor='mm')</code>","text":"<p>Draw text at the specified position</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>text</code> <code>str</code> <p>Text to draw</p> required <code>position</code> <code>tuple</code> <p>Position of the text</p> required <code>font_size</code> <code>int</code> <p>Size of the font, defaults to 40</p> <code>40</code> <code>color</code> <code>tuple</code> <p>Color of the text, defaults to RGB (255, 255, 255, 255)</p> <code>(255, 255, 255, 255)</code> <code>anchor</code> <code>str</code> <p>Anchor of the text (from PIL library), defaults to \"mm\".           See pillow docs: text anchors           for all possible options.</p> <code>'mm'</code> Source code in <code>audim/sub2pod/elements/text.py</code> <pre><code>def draw_text(\n    self,\n    draw,\n    text,\n    position,\n    font_size=40,\n    color=(255, 255, 255, 255),\n    anchor=\"mm\",\n):\n    \"\"\"\n    Draw text at the specified position\n\n    Args:\n        draw (ImageDraw): Draw object to draw on the frame\n        text (str): Text to draw\n        position (tuple): Position of the text\n        font_size (int): Size of the font, defaults to 40\n        color (tuple): Color of the text, defaults to RGB (255, 255, 255, 255)\n        anchor (str): Anchor of the text (from PIL library), defaults to \"mm\".\n                      See [pillow docs: text anchors](https://pillow.readthedocs.io/en/latest/handbook/text-anchors.html)\n                      for all possible options.\n    \"\"\"\n\n    font = self.get_font(font_size)\n    color = self._sanitize_color(color)\n\n    draw.text(position, text, fill=color, font=font, anchor=anchor)\n</code></pre>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.draw_wrapped_text","title":"<code>draw_wrapped_text(draw, text, position, max_width, font_size=40, color=(255, 255, 255, 255), anchor='lm')</code>","text":"<p>Draw text with word wrapping</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>ImageDraw</code> <p>Draw object to draw on the frame</p> required <code>text</code> <code>str</code> <p>Text to draw</p> required <code>position</code> <code>tuple</code> <p>Position of the text</p> required <code>max_width</code> <code>int</code> <p>Maximum width of the text before wrapping</p> required <code>font_size</code> <code>int</code> <p>Size of the font, defaults to 40</p> <code>40</code> <code>color</code> <code>tuple</code> <p>Color of the text, defaults to RGB (255, 255, 255, 255)</p> <code>(255, 255, 255, 255)</code> <code>anchor</code> <code>str</code> <p>Anchor of the text (from PIL library), defaults to \"lm\".           See pillow docs: text anchors           for all possible options.</p> <code>'lm'</code> Source code in <code>audim/sub2pod/elements/text.py</code> <pre><code>def draw_wrapped_text(\n    self,\n    draw,\n    text,\n    position,\n    max_width,\n    font_size=40,\n    color=(255, 255, 255, 255),\n    anchor=\"lm\",\n):\n    \"\"\"\n    Draw text with word wrapping\n\n    Args:\n        draw (ImageDraw): Draw object to draw on the frame\n        text (str): Text to draw\n        position (tuple): Position of the text\n        max_width (int): Maximum width of the text before wrapping\n        font_size (int): Size of the font, defaults to 40\n        color (tuple): Color of the text, defaults to RGB (255, 255, 255, 255)\n        anchor (str): Anchor of the text (from PIL library), defaults to \"lm\".\n                      See [pillow docs: text anchors](https://pillow.readthedocs.io/en/latest/handbook/text-anchors.html)\n                      for all possible options.\n    \"\"\"\n\n    font = self.get_font(font_size)\n    color = self._sanitize_color(color)\n\n    # Get font metrics for dynamic calculations\n    font_ascent, font_descent = font.getmetrics()\n    line_height = font_ascent + font_descent\n    line_spacing = line_height * 0.5  # 50% of line height for spacing\n    total_line_height = line_height + line_spacing\n\n    # Word wrap\n    words = text.split()\n    lines = []\n    current_line = []\n\n    for word in words:\n        current_line.append(word)\n        w = draw.textlength(\" \".join(current_line), font=font)\n        if w &gt; max_width:\n            current_line.pop()\n            lines.append(\" \".join(current_line))\n            current_line = [word]\n    lines.append(\" \".join(current_line))\n\n    # Calculate vertical offset for multiple lines to maintain center alignment\n    text_x, text_y = position\n    total_text_height = len(lines) * total_line_height\n\n    if anchor == \"lm\":\n        text_start_y = text_y - (total_text_height // 2) + (line_height // 2)\n    else:\n        text_start_y = text_y\n\n    # Draw each line\n    for i, line in enumerate(lines):\n        line_y = text_start_y + (i * total_line_height)\n        draw.text((text_x, line_y), line, fill=color, font=font, anchor=anchor)\n</code></pre>"},{"location":"audim/sub2pod/elements/text/#audim.sub2pod.elements.text.TextRenderer.get_font","title":"<code>get_font(size)</code>","text":"<p>Get or create a font of the specified size</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Size of the font</p> required Source code in <code>audim/sub2pod/elements/text.py</code> <pre><code>def get_font(self, size):\n    \"\"\"\n    Get or create a font of the specified size\n\n    Args:\n        size (int): Size of the font\n    \"\"\"\n\n    if size not in self.fonts:\n        self.fonts[size] = ImageFont.truetype(self.font_path, size)\n    return self.fonts[size]\n</code></pre>"},{"location":"audim/sub2pod/elements/watermark/","title":"Watermark","text":"<p>The watermark element is a component that is used to display the watermark text at the bottom of the video frame.</p> <p>This is useful for branding and attribution. We can have the following texts as a watermark for Audim:</p> <ol> <li>\"made with \u2764\ufe0f by audim\" (default)</li> <li>\"made with \u2764\ufe0f and audim\"</li> <li>\"created with \u2764\ufe0f by audim\"</li> <li>\"made with audim\"</li> <li>\"created with audim\"</li> <li>\"generated with audim\"</li> <li>or any custom text attributing to \"Audim\"</li> </ol> <p>Below is the API documentation for the watermark element:</p>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark","title":"<code>Watermark</code>","text":"<p>Watermark component for video layouts</p> <p>This component is responsible for displaying a watermark text at the bottom of the video frame. It can be customized with different positions, colors, and opacity levels.</p> Source code in <code>audim/sub2pod/elements/watermark.py</code> <pre><code>class Watermark:\n    \"\"\"\n    Watermark component for video layouts\n\n    This component is responsible for displaying a watermark text at the bottom\n    of the video frame. It can be customized with different positions, colors,\n    and opacity levels.\n    \"\"\"\n\n    def __init__(\n        self, \n        text=\"made with \u2764\ufe0f by audim\",\n        position=\"bottom-right\",\n        color=(255, 255, 255),\n        opacity=150,\n        font_size=20,\n        margin=10\n    ):\n        \"\"\"\n        Initialize the watermark\n\n        Args:\n            text (str): Text to display as watermark\n            position (str): Position of the watermark\n                            'bottom-left', 'bottom-center', or 'bottom-right'\n            color (tuple): RGB color of the watermark text\n            opacity (int): Opacity of the watermark (0-255)\n            font_size (int): Font size of the watermark text\n            margin (int): Margin from the edges in pixels\n        \"\"\"\n\n        self.text = text\n        self.position = position\n        self.color = color\n        self.opacity = opacity\n        self.font_size = font_size\n        self.margin = margin\n        self.text_renderer = TextRenderer()\n\n    def set_text(self, text):\n        \"\"\"\n        Set the watermark text\n\n        Args:\n            text (str): Text to display as watermark\n        \"\"\"\n\n        self.text = text\n        return self\n\n    def set_position(self, position):\n        \"\"\"\n        Set the watermark position\n\n        Args:\n            position (str): Position of the watermark\n                            'bottom-left', 'bottom-center', or 'bottom-right'\n        \"\"\"\n\n        if position not in [\"bottom-left\", \"bottom-center\", \"bottom-right\"]:\n            raise ValueError(\n                \"Position must be 'bottom-left', 'bottom-center', or 'bottom-right'\"\n            )\n        self.position = position\n        return self\n\n    def set_color(self, color):\n        \"\"\"\n        Set the watermark color\n\n        Args:\n            color (tuple): RGB color of the watermark text\n        \"\"\"\n\n        self.color = color\n        return self\n\n    def set_opacity(self, opacity):\n        \"\"\"\n        Set the watermark opacity\n\n        Args:\n            opacity (int): Opacity of the watermark (0-255)\n        \"\"\"\n\n        self.opacity = max(0, min(255, opacity))\n        return self\n\n    def set_font_size(self, font_size):\n        \"\"\"\n        Set the watermark font size\n\n        Args:\n            font_size (int): Font size of the watermark text\n        \"\"\"\n\n        self.font_size = font_size\n        return self\n\n    def draw(self, frame, draw, width, height, frame_opacity=255):\n        \"\"\"\n        Draw the watermark on the frame\n\n        Args:\n            frame: Frame to draw the watermark on\n            draw: Draw object to draw on the frame\n            width (int): Width of the frame\n            height (int): Height of the frame\n            frame_opacity (int): Opacity of the entire frame (for transitions)\n        \"\"\"\n\n        # Ensure opacity values are valid integers\n        watermark_opacity = max(0, min(255, self.opacity))\n        frame_opacity = max(0, min(255, frame_opacity))\n\n        # Calculate final opacity (product of watermark opacity and frame opacity)\n        # Use a proportion to maintain proper alpha blending\n        final_opacity = int((watermark_opacity / 255.0) * (frame_opacity / 255.0) * 255)\n\n        # Create the color tuple with the final opacity\n        if isinstance(self.color, tuple) and len(self.color) &gt;= 3:\n            color_with_opacity = self.color[:3] + (final_opacity,)\n        else:\n            color_with_opacity = (255, 255, 255, final_opacity)\n\n        # Calculate position based on the selected position\n        # for anchor keys, see: https://pillow.readthedocs.io/en/latest/handbook/text-anchors.html\n        if self.position == \"bottom-left\":\n            position = (self.margin, height - self.margin)\n            anchor = \"ls\"\n        elif self.position == \"bottom-center\":\n            position = (width // 2, height - self.margin)\n            anchor = \"ms\"\n        else:\n            position = (width - self.margin, height - self.margin)\n            anchor = \"rs\"\n\n        # Draw the watermark text\n        self.text_renderer.draw_text(\n            draw,\n            self.text,\n            position,\n            font_size=self.font_size,\n            color=color_with_opacity,\n            anchor=anchor,\n        )\n</code></pre>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.__init__","title":"<code>__init__(text='made with \u2764\ufe0f by audim', position='bottom-right', color=(255, 255, 255), opacity=150, font_size=20, margin=10)</code>","text":"<p>Initialize the watermark</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to display as watermark</p> <code>'made with \u2764\ufe0f by audim'</code> <code>position</code> <code>str</code> <p>Position of the watermark             'bottom-left', 'bottom-center', or 'bottom-right'</p> <code>'bottom-right'</code> <code>color</code> <code>tuple</code> <p>RGB color of the watermark text</p> <code>(255, 255, 255)</code> <code>opacity</code> <code>int</code> <p>Opacity of the watermark (0-255)</p> <code>150</code> <code>font_size</code> <code>int</code> <p>Font size of the watermark text</p> <code>20</code> <code>margin</code> <code>int</code> <p>Margin from the edges in pixels</p> <code>10</code> Source code in <code>audim/sub2pod/elements/watermark.py</code> <pre><code>def __init__(\n    self, \n    text=\"made with \u2764\ufe0f by audim\",\n    position=\"bottom-right\",\n    color=(255, 255, 255),\n    opacity=150,\n    font_size=20,\n    margin=10\n):\n    \"\"\"\n    Initialize the watermark\n\n    Args:\n        text (str): Text to display as watermark\n        position (str): Position of the watermark\n                        'bottom-left', 'bottom-center', or 'bottom-right'\n        color (tuple): RGB color of the watermark text\n        opacity (int): Opacity of the watermark (0-255)\n        font_size (int): Font size of the watermark text\n        margin (int): Margin from the edges in pixels\n    \"\"\"\n\n    self.text = text\n    self.position = position\n    self.color = color\n    self.opacity = opacity\n    self.font_size = font_size\n    self.margin = margin\n    self.text_renderer = TextRenderer()\n</code></pre>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.draw","title":"<code>draw(frame, draw, width, height, frame_opacity=255)</code>","text":"<p>Draw the watermark on the frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>Frame to draw the watermark on</p> required <code>draw</code> <p>Draw object to draw on the frame</p> required <code>width</code> <code>int</code> <p>Width of the frame</p> required <code>height</code> <code>int</code> <p>Height of the frame</p> required <code>frame_opacity</code> <code>int</code> <p>Opacity of the entire frame (for transitions)</p> <code>255</code> Source code in <code>audim/sub2pod/elements/watermark.py</code> <pre><code>def draw(self, frame, draw, width, height, frame_opacity=255):\n    \"\"\"\n    Draw the watermark on the frame\n\n    Args:\n        frame: Frame to draw the watermark on\n        draw: Draw object to draw on the frame\n        width (int): Width of the frame\n        height (int): Height of the frame\n        frame_opacity (int): Opacity of the entire frame (for transitions)\n    \"\"\"\n\n    # Ensure opacity values are valid integers\n    watermark_opacity = max(0, min(255, self.opacity))\n    frame_opacity = max(0, min(255, frame_opacity))\n\n    # Calculate final opacity (product of watermark opacity and frame opacity)\n    # Use a proportion to maintain proper alpha blending\n    final_opacity = int((watermark_opacity / 255.0) * (frame_opacity / 255.0) * 255)\n\n    # Create the color tuple with the final opacity\n    if isinstance(self.color, tuple) and len(self.color) &gt;= 3:\n        color_with_opacity = self.color[:3] + (final_opacity,)\n    else:\n        color_with_opacity = (255, 255, 255, final_opacity)\n\n    # Calculate position based on the selected position\n    # for anchor keys, see: https://pillow.readthedocs.io/en/latest/handbook/text-anchors.html\n    if self.position == \"bottom-left\":\n        position = (self.margin, height - self.margin)\n        anchor = \"ls\"\n    elif self.position == \"bottom-center\":\n        position = (width // 2, height - self.margin)\n        anchor = \"ms\"\n    else:\n        position = (width - self.margin, height - self.margin)\n        anchor = \"rs\"\n\n    # Draw the watermark text\n    self.text_renderer.draw_text(\n        draw,\n        self.text,\n        position,\n        font_size=self.font_size,\n        color=color_with_opacity,\n        anchor=anchor,\n    )\n</code></pre>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.set_color","title":"<code>set_color(color)</code>","text":"<p>Set the watermark color</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>tuple</code> <p>RGB color of the watermark text</p> required Source code in <code>audim/sub2pod/elements/watermark.py</code> <pre><code>def set_color(self, color):\n    \"\"\"\n    Set the watermark color\n\n    Args:\n        color (tuple): RGB color of the watermark text\n    \"\"\"\n\n    self.color = color\n    return self\n</code></pre>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.set_font_size","title":"<code>set_font_size(font_size)</code>","text":"<p>Set the watermark font size</p> <p>Parameters:</p> Name Type Description Default <code>font_size</code> <code>int</code> <p>Font size of the watermark text</p> required Source code in <code>audim/sub2pod/elements/watermark.py</code> <pre><code>def set_font_size(self, font_size):\n    \"\"\"\n    Set the watermark font size\n\n    Args:\n        font_size (int): Font size of the watermark text\n    \"\"\"\n\n    self.font_size = font_size\n    return self\n</code></pre>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.set_opacity","title":"<code>set_opacity(opacity)</code>","text":"<p>Set the watermark opacity</p> <p>Parameters:</p> Name Type Description Default <code>opacity</code> <code>int</code> <p>Opacity of the watermark (0-255)</p> required Source code in <code>audim/sub2pod/elements/watermark.py</code> <pre><code>def set_opacity(self, opacity):\n    \"\"\"\n    Set the watermark opacity\n\n    Args:\n        opacity (int): Opacity of the watermark (0-255)\n    \"\"\"\n\n    self.opacity = max(0, min(255, opacity))\n    return self\n</code></pre>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.set_position","title":"<code>set_position(position)</code>","text":"<p>Set the watermark position</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>str</code> <p>Position of the watermark             'bottom-left', 'bottom-center', or 'bottom-right'</p> required Source code in <code>audim/sub2pod/elements/watermark.py</code> <pre><code>def set_position(self, position):\n    \"\"\"\n    Set the watermark position\n\n    Args:\n        position (str): Position of the watermark\n                        'bottom-left', 'bottom-center', or 'bottom-right'\n    \"\"\"\n\n    if position not in [\"bottom-left\", \"bottom-center\", \"bottom-right\"]:\n        raise ValueError(\n            \"Position must be 'bottom-left', 'bottom-center', or 'bottom-right'\"\n        )\n    self.position = position\n    return self\n</code></pre>"},{"location":"audim/sub2pod/elements/watermark/#audim.sub2pod.elements.watermark.Watermark.set_text","title":"<code>set_text(text)</code>","text":"<p>Set the watermark text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to display as watermark</p> required Source code in <code>audim/sub2pod/elements/watermark.py</code> <pre><code>def set_text(self, text):\n    \"\"\"\n    Set the watermark text\n\n    Args:\n        text (str): Text to display as watermark\n    \"\"\"\n\n    self.text = text\n    return self\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/","title":"Base Layout","text":"<p>The base layout sets the base layout for the podcast.</p> <p>It must be overriden to create various layout structures placing various elements in different places in the scenes. This would determine how the video frames would look like in the podcast.</p> <p>A Layout is defined by a collection of elements and their positions and placements in a video frame with their effects. It defines the components of each frame in the video and their animations and transitions.</p> <p>Below is the API documentation for the base layout:</p>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout","title":"<code>BaseLayout</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all layouts</p> <p>This class defines the base structure for all layout classes. It provides a common interface for adding speakers and creating frames and scenes.</p> Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>class BaseLayout(ABC):\n    \"\"\"\n    Base class for all layouts\n\n    This class defines the base structure for all layout classes.\n    It provides a common interface for adding speakers and creating frames and scenes.\n    \"\"\"\n\n    def __init__(self, video_width=1920, video_height=1080, content_horizontal_offset=0):\n        \"\"\"\n        Initialize the base layout\n\n        Args:\n            video_width (int): Width of the video\n            video_height (int): Height of the video\n            content_horizontal_offset (int): Horizontal offset for the content\n                (positive values move content right, negative values move content left).\n                This allows shifting the main content (display pictures and subtitles)\n                within the frame while keeping the header fixed.\n        \"\"\"\n\n        self.video_width = video_width\n        self.video_height = video_height\n        self.content_horizontal_offset = content_horizontal_offset\n\n        # Default transition effect\n        self.transition_effect = Transition(\"fade\")\n\n        # No default highlight effect\n        self.highlight_effect = None\n\n        # Default watermark (enabled)\n        self.watermark = Watermark()\n        self.show_watermark = True\n\n    def set_transition_effect(self, effect_type, **kwargs):\n        \"\"\"\n        Set the transition effect for this layout\n\n        Args:\n            effect_type (str): Type of transition effect\n                \"fade\": Fade-in transition (default)\n                \"slide\": Slide-in transition\n                \"none\": No transition\n            **kwargs: Additional parameters for the effect\n                frames (int): Number of frames for the transition\n                direction (str): Direction for slide transition\n                (\"left\", \"right\", \"up\", \"down\")\n        \"\"\"\n\n        self.transition_effect = Transition(effect_type, **kwargs)\n\n    def set_highlight_effect(self, effect_type, **kwargs):\n        \"\"\"\n        Set the highlight effect for this layout\n\n        Args:\n            effect_type (str): Type of highlight effect\n                \"pulse\": Pulsing highlight\n                \"glow\": Glowing highlight\n                \"underline\": Underline highlight\n                \"box\": Box highlight\n                \"none\": No highlight\n            **kwargs: Additional parameters for the effect\n                color (tuple): RGBA color for the highlight\n                padding (int): Padding around the highlighted area\n                min_size (float): For pulse, minimum size factor (e.g., 0.8)\n                max_size (float): For pulse, maximum size factor (e.g., 1.2)\n                blur_radius (int): Blur radius for glow effect\n            thickness (int): Line thickness for underline/box\n        \"\"\"\n\n        self.highlight_effect = Highlight(effect_type, **kwargs)\n\n    def set_content_offset(self, offset):\n        \"\"\"\n        Set horizontal offset for the main content area\n\n        This method allows shifting the main content (display pictures and subtitles)\n        horizontally within the frame while keeping the header fixed.\n        Useful for adjusting the layout based on subtitle length.\n\n        Args:\n            offset (int): Horizontal offset in pixels.\n            Positive values move content right, \n            negative values move content left.\n        \"\"\"\n\n        self.content_horizontal_offset = offset\n        return self\n\n    def enable_watermark(self, show=True):\n        \"\"\"\n        Enable or disable the watermark\n\n        Args:\n            show (bool): Whether to show the watermark\n        \"\"\"\n\n        self.show_watermark = show\n        if show and self.watermark is None:\n            self.watermark = Watermark()\n        return self\n\n    def set_watermark_text(self, text):\n        \"\"\"\n        Set the watermark text\n\n        Args:\n            text (str): Text to display as watermark\n        \"\"\"\n\n        if self.watermark is None:\n            self.watermark = Watermark(text=text)\n        else:\n            self.watermark.set_text(text)\n        return self\n\n    def set_watermark_position(self, position):\n        \"\"\"\n        Set the watermark position\n\n        Args:\n            position (str): Position of the watermark\n                'bottom-left', 'bottom-center', or 'bottom-right'\n        \"\"\"\n\n        if self.watermark is None:\n            self.watermark = Watermark(position=position)\n        else:\n            self.watermark.set_position(position)\n        return self\n\n    def set_watermark_color(self, color):\n        \"\"\"\n        Set the watermark color\n\n        Args:\n            color (tuple): RGB color of the watermark text\n        \"\"\"\n\n        if self.watermark is None:\n            self.watermark = Watermark(color=color)\n        else:\n            self.watermark.set_color(color)\n        return self\n\n    def set_watermark_opacity(self, opacity):\n        \"\"\"\n        Set the watermark opacity\n\n        Args:\n            opacity (int): Opacity of the watermark (0-255)\n        \"\"\"\n\n        if self.watermark is None:\n            self.watermark = Watermark(opacity=opacity)\n        else:\n            self.watermark.set_opacity(opacity)\n        return self\n\n    def set_watermark_properties(self, **kwargs):\n        \"\"\"\n        Set multiple watermark properties at once\n\n        Args:\n            **kwargs: Keyword arguments for watermark properties:\n                text (str): Watermark text\n                position (str): Watermark position\n                color (tuple): RGB color\n                opacity (int): Opacity\n                font_size (int): Font size\n                margin (int): Margin from edges\n        \"\"\"\n\n        if self.watermark is None:\n            self.watermark = Watermark(**kwargs)\n        else:\n            if \"text\" in kwargs:\n                self.watermark.set_text(kwargs[\"text\"])\n            if \"position\" in kwargs:\n                self.watermark.set_position(kwargs[\"position\"])\n            if \"color\" in kwargs:\n                self.watermark.set_color(kwargs[\"color\"])\n            if \"opacity\" in kwargs:\n                self.watermark.set_opacity(kwargs[\"opacity\"])\n            if \"font_size\" in kwargs:\n                self.watermark.set_font_size(kwargs[\"font_size\"])\n            if \"margin\" in kwargs:\n                self.watermark.margin = kwargs[\"margin\"]\n        return self\n\n    @abstractmethod\n    def add_speaker(self, name, image_path):\n        \"\"\"\n        Add a speaker to the layout\n\n        Args:\n            name (str): Name of the speaker\n            image_path (str): Path to the speaker's image\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def create_frame(self, current_sub=None, opacity=255):\n        \"\"\"\n        Create a frame with the current subtitle\n\n        Args:\n            current_sub (str): Current subtitle\n            opacity (int): Opacity of the subtitle\n        \"\"\"\n\n        pass\n\n    def _create_base_frame(self, background_color=(20, 20, 20)):\n        \"\"\"\n        Create a base frame with the specified background color\n        (mostly for internal use)\n\n        Args:\n            background_color (tuple): Background color in RGB format\n        \"\"\"\n\n        frame = Image.new(\n            \"RGBA\", (self.video_width, self.video_height), background_color + (255,)\n        )\n        draw = ImageDraw.Draw(frame)\n        return frame, draw\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.__init__","title":"<code>__init__(video_width=1920, video_height=1080, content_horizontal_offset=0)</code>","text":"<p>Initialize the base layout</p> <p>Parameters:</p> Name Type Description Default <code>video_width</code> <code>int</code> <p>Width of the video</p> <code>1920</code> <code>video_height</code> <code>int</code> <p>Height of the video</p> <code>1080</code> <code>content_horizontal_offset</code> <code>int</code> <p>Horizontal offset for the content (positive values move content right, negative values move content left). This allows shifting the main content (display pictures and subtitles) within the frame while keeping the header fixed.</p> <code>0</code> Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def __init__(self, video_width=1920, video_height=1080, content_horizontal_offset=0):\n    \"\"\"\n    Initialize the base layout\n\n    Args:\n        video_width (int): Width of the video\n        video_height (int): Height of the video\n        content_horizontal_offset (int): Horizontal offset for the content\n            (positive values move content right, negative values move content left).\n            This allows shifting the main content (display pictures and subtitles)\n            within the frame while keeping the header fixed.\n    \"\"\"\n\n    self.video_width = video_width\n    self.video_height = video_height\n    self.content_horizontal_offset = content_horizontal_offset\n\n    # Default transition effect\n    self.transition_effect = Transition(\"fade\")\n\n    # No default highlight effect\n    self.highlight_effect = None\n\n    # Default watermark (enabled)\n    self.watermark = Watermark()\n    self.show_watermark = True\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.add_speaker","title":"<code>add_speaker(name, image_path)</code>  <code>abstractmethod</code>","text":"<p>Add a speaker to the layout</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the speaker</p> required <code>image_path</code> <code>str</code> <p>Path to the speaker's image</p> required Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>@abstractmethod\ndef add_speaker(self, name, image_path):\n    \"\"\"\n    Add a speaker to the layout\n\n    Args:\n        name (str): Name of the speaker\n        image_path (str): Path to the speaker's image\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.create_frame","title":"<code>create_frame(current_sub=None, opacity=255)</code>  <code>abstractmethod</code>","text":"<p>Create a frame with the current subtitle</p> <p>Parameters:</p> Name Type Description Default <code>current_sub</code> <code>str</code> <p>Current subtitle</p> <code>None</code> <code>opacity</code> <code>int</code> <p>Opacity of the subtitle</p> <code>255</code> Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>@abstractmethod\ndef create_frame(self, current_sub=None, opacity=255):\n    \"\"\"\n    Create a frame with the current subtitle\n\n    Args:\n        current_sub (str): Current subtitle\n        opacity (int): Opacity of the subtitle\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.enable_watermark","title":"<code>enable_watermark(show=True)</code>","text":"<p>Enable or disable the watermark</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>Whether to show the watermark</p> <code>True</code> Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def enable_watermark(self, show=True):\n    \"\"\"\n    Enable or disable the watermark\n\n    Args:\n        show (bool): Whether to show the watermark\n    \"\"\"\n\n    self.show_watermark = show\n    if show and self.watermark is None:\n        self.watermark = Watermark()\n    return self\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_content_offset","title":"<code>set_content_offset(offset)</code>","text":"<p>Set horizontal offset for the main content area</p> <p>This method allows shifting the main content (display pictures and subtitles) horizontally within the frame while keeping the header fixed. Useful for adjusting the layout based on subtitle length.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Horizontal offset in pixels.</p> required Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def set_content_offset(self, offset):\n    \"\"\"\n    Set horizontal offset for the main content area\n\n    This method allows shifting the main content (display pictures and subtitles)\n    horizontally within the frame while keeping the header fixed.\n    Useful for adjusting the layout based on subtitle length.\n\n    Args:\n        offset (int): Horizontal offset in pixels.\n        Positive values move content right, \n        negative values move content left.\n    \"\"\"\n\n    self.content_horizontal_offset = offset\n    return self\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_highlight_effect","title":"<code>set_highlight_effect(effect_type, **kwargs)</code>","text":"<p>Set the highlight effect for this layout</p> <p>Parameters:</p> Name Type Description Default <code>effect_type</code> <code>str</code> <p>Type of highlight effect \"pulse\": Pulsing highlight \"glow\": Glowing highlight \"underline\": Underline highlight \"box\": Box highlight \"none\": No highlight</p> required <code>**kwargs</code> <p>Additional parameters for the effect color (tuple): RGBA color for the highlight padding (int): Padding around the highlighted area min_size (float): For pulse, minimum size factor (e.g., 0.8) max_size (float): For pulse, maximum size factor (e.g., 1.2) blur_radius (int): Blur radius for glow effect</p> <code>{}</code> <code>thickness</code> <code>int</code> <p>Line thickness for underline/box</p> required Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def set_highlight_effect(self, effect_type, **kwargs):\n    \"\"\"\n    Set the highlight effect for this layout\n\n    Args:\n        effect_type (str): Type of highlight effect\n            \"pulse\": Pulsing highlight\n            \"glow\": Glowing highlight\n            \"underline\": Underline highlight\n            \"box\": Box highlight\n            \"none\": No highlight\n        **kwargs: Additional parameters for the effect\n            color (tuple): RGBA color for the highlight\n            padding (int): Padding around the highlighted area\n            min_size (float): For pulse, minimum size factor (e.g., 0.8)\n            max_size (float): For pulse, maximum size factor (e.g., 1.2)\n            blur_radius (int): Blur radius for glow effect\n        thickness (int): Line thickness for underline/box\n    \"\"\"\n\n    self.highlight_effect = Highlight(effect_type, **kwargs)\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_transition_effect","title":"<code>set_transition_effect(effect_type, **kwargs)</code>","text":"<p>Set the transition effect for this layout</p> <p>Parameters:</p> Name Type Description Default <code>effect_type</code> <code>str</code> <p>Type of transition effect \"fade\": Fade-in transition (default) \"slide\": Slide-in transition \"none\": No transition</p> required <code>**kwargs</code> <p>Additional parameters for the effect frames (int): Number of frames for the transition direction (str): Direction for slide transition (\"left\", \"right\", \"up\", \"down\")</p> <code>{}</code> Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def set_transition_effect(self, effect_type, **kwargs):\n    \"\"\"\n    Set the transition effect for this layout\n\n    Args:\n        effect_type (str): Type of transition effect\n            \"fade\": Fade-in transition (default)\n            \"slide\": Slide-in transition\n            \"none\": No transition\n        **kwargs: Additional parameters for the effect\n            frames (int): Number of frames for the transition\n            direction (str): Direction for slide transition\n            (\"left\", \"right\", \"up\", \"down\")\n    \"\"\"\n\n    self.transition_effect = Transition(effect_type, **kwargs)\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_watermark_color","title":"<code>set_watermark_color(color)</code>","text":"<p>Set the watermark color</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>tuple</code> <p>RGB color of the watermark text</p> required Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def set_watermark_color(self, color):\n    \"\"\"\n    Set the watermark color\n\n    Args:\n        color (tuple): RGB color of the watermark text\n    \"\"\"\n\n    if self.watermark is None:\n        self.watermark = Watermark(color=color)\n    else:\n        self.watermark.set_color(color)\n    return self\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_watermark_opacity","title":"<code>set_watermark_opacity(opacity)</code>","text":"<p>Set the watermark opacity</p> <p>Parameters:</p> Name Type Description Default <code>opacity</code> <code>int</code> <p>Opacity of the watermark (0-255)</p> required Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def set_watermark_opacity(self, opacity):\n    \"\"\"\n    Set the watermark opacity\n\n    Args:\n        opacity (int): Opacity of the watermark (0-255)\n    \"\"\"\n\n    if self.watermark is None:\n        self.watermark = Watermark(opacity=opacity)\n    else:\n        self.watermark.set_opacity(opacity)\n    return self\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_watermark_position","title":"<code>set_watermark_position(position)</code>","text":"<p>Set the watermark position</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>str</code> <p>Position of the watermark 'bottom-left', 'bottom-center', or 'bottom-right'</p> required Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def set_watermark_position(self, position):\n    \"\"\"\n    Set the watermark position\n\n    Args:\n        position (str): Position of the watermark\n            'bottom-left', 'bottom-center', or 'bottom-right'\n    \"\"\"\n\n    if self.watermark is None:\n        self.watermark = Watermark(position=position)\n    else:\n        self.watermark.set_position(position)\n    return self\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_watermark_properties","title":"<code>set_watermark_properties(**kwargs)</code>","text":"<p>Set multiple watermark properties at once</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for watermark properties: text (str): Watermark text position (str): Watermark position color (tuple): RGB color opacity (int): Opacity font_size (int): Font size margin (int): Margin from edges</p> <code>{}</code> Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def set_watermark_properties(self, **kwargs):\n    \"\"\"\n    Set multiple watermark properties at once\n\n    Args:\n        **kwargs: Keyword arguments for watermark properties:\n            text (str): Watermark text\n            position (str): Watermark position\n            color (tuple): RGB color\n            opacity (int): Opacity\n            font_size (int): Font size\n            margin (int): Margin from edges\n    \"\"\"\n\n    if self.watermark is None:\n        self.watermark = Watermark(**kwargs)\n    else:\n        if \"text\" in kwargs:\n            self.watermark.set_text(kwargs[\"text\"])\n        if \"position\" in kwargs:\n            self.watermark.set_position(kwargs[\"position\"])\n        if \"color\" in kwargs:\n            self.watermark.set_color(kwargs[\"color\"])\n        if \"opacity\" in kwargs:\n            self.watermark.set_opacity(kwargs[\"opacity\"])\n        if \"font_size\" in kwargs:\n            self.watermark.set_font_size(kwargs[\"font_size\"])\n        if \"margin\" in kwargs:\n            self.watermark.margin = kwargs[\"margin\"]\n    return self\n</code></pre>"},{"location":"audim/sub2pod/layouts/base/#audim.sub2pod.layouts.base.BaseLayout.set_watermark_text","title":"<code>set_watermark_text(text)</code>","text":"<p>Set the watermark text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to display as watermark</p> required Source code in <code>audim/sub2pod/layouts/base.py</code> <pre><code>def set_watermark_text(self, text):\n    \"\"\"\n    Set the watermark text\n\n    Args:\n        text (str): Text to display as watermark\n    \"\"\"\n\n    if self.watermark is None:\n        self.watermark = Watermark(text=text)\n    else:\n        self.watermark.set_text(text)\n    return self\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/","title":"Podcast Layout","text":"<p>The podcast layout is designed to create dynamic scenes for the podcast.</p> <p>Typically, it features:</p> <ul> <li>profile pictures of the speakers (which are highlighted when they are speaking)</li> <li>text dialogues of the speakers (which is displayed in real-time as they speak)</li> </ul> <p>Below is the API documentation for the podcast layout:</p>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout","title":"<code>PodcastLayout</code>","text":"<p>               Bases: <code>BaseLayout</code></p> <p>Standard podcast layout with profile pictures and subtitles</p> <p>This layout is designed for standard podcast videos with a header section, profile pictures, and subtitles. It provides a flexible structure for adding speakers and creating frames with customizable parameters.</p> Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>class PodcastLayout(BaseLayout):\n    \"\"\"\n    Standard podcast layout with profile pictures and subtitles\n\n    This layout is designed for standard podcast videos with a header section,\n    profile pictures, and subtitles. It provides a flexible structure for adding\n    speakers and creating frames with customizable parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        video_width=1920,\n        video_height=1080,\n        header_height=150,\n        dp_size=(120, 120),\n        show_speaker_names=True,\n        content_horizontal_offset=0,\n        show_watermark=True,\n    ):\n        \"\"\"\n        Initialize podcast layout\n\n        Args:\n            video_width (int): Width of the video\n            video_height (int): Height of the video\n            header_height (int): Height of the header section\n            dp_size (tuple): Size of profile pictures\n            show_speaker_names (bool): Whether to show speaker names\n            content_horizontal_offset (int): Horizontal offset for the content\n                (positive values move content right,\n                negative values move content left)\n            show_watermark (bool): Whether to show the watermark\n        \"\"\"\n\n        super().__init__(video_width, video_height)\n\n        # Layout parameters\n        self.header_height = header_height\n        self.dp_size = dp_size\n        self.show_speaker_names = show_speaker_names\n        self.dp_margin_left = 40\n        self.text_margin = 50\n        self.name_margin = 30\n        self.content_horizontal_offset = content_horizontal_offset\n\n        # Initialize components\n        self.header = Header(height=header_height)\n        self.text_renderer = TextRenderer()\n\n        # Initialize watermark\n        self.show_watermark = show_watermark\n        if show_watermark:\n            # Opposite of background color (which is typically dark)\n            self.watermark = Watermark(color=(255, 255, 255))\n\n        # Store profile pictures and positions\n        self.speakers = {}\n        self.dp_positions = {}\n        self.logo_path = None\n        self.title = \"My Podcast\"\n\n        # Store the active subtitle area for highlighting\n        self.active_subtitle_area = None\n\n    def set_content_offset(self, offset):\n        \"\"\"\n        Set horizontal offset for the content (display pictures and subtitles)\n\n        Args:\n            offset (int): Horizontal offset in pixels.\n                Positive values move content right, \n                negative values move content left.\n        \"\"\"\n        self.content_horizontal_offset = offset\n        # Recalculate positions with the new offset\n        self._calculate_positions()\n        return self\n\n    def _calculate_positions(self):\n        \"\"\"\n        Calculate positions for all speakers based on the number of speakers and\n        the size of the profile pictures. (mostly for internal use)\n\n        This method calculates the positions of all speakers based on the number of\n        speakers and the size of the profile pictures. It also takes into account\n        the spacing between the speakers and the header height.\n        \"\"\"\n\n        num_speakers = len(self.speakers)\n        spacing, start_y = self._calculate_layout(num_speakers)\n\n        for i, speaker in enumerate(self.speakers.keys()):\n            y_pos = start_y + (i * (self.dp_size[1] + spacing))\n            x_pos = self.dp_margin_left + self.content_horizontal_offset\n            self.dp_positions[speaker] = (x_pos, y_pos)\n\n    def _calculate_layout(self, num_speakers, min_spacing=40):\n        \"\"\"\n        Calculate dynamic spacing for speaker rows\n        (mostly for internal use)\n\n        This method calculates the spacing between the speakers based on the number of\n        speakers and the size of the profile pictures. It also takes into account the\n        spacing between the speakers and the header height.\n\n        Args:\n            num_speakers (int): Number of speakers\n            min_spacing (int): Minimum spacing between the speakers, defaults to 40\n        \"\"\"\n\n        available_height = self.video_height - self.header_height\n        total_dp_height = num_speakers * self.dp_size[1]\n\n        # Calculate spacing between DPs\n        num_spaces = num_speakers + 1\n        spacing = (available_height - total_dp_height) // num_spaces\n        spacing = max(spacing, min_spacing)\n\n        # Calculate starting Y position\n        start_y = self.header_height + spacing\n\n        return spacing, start_y\n\n    def _draw_subtitle(self, frame, draw, subtitle, opacity, subtitle_info=None):\n        \"\"\"\n        Draw the current subtitle with speaker highlighting\n        (mostly for internal use)\n\n        This method draws the current subtitle with speaker highlighting.\n        It highlights the active speaker and draws the subtitle text.\n\n        Args:\n            frame (Image): Frame to draw on\n            draw (ImageDraw): Draw object to draw on the frame\n            subtitle (Subtitle): Current subtitle\n            opacity (int): Opacity of the subtitle (0-255)\n            subtitle_info (dict, optional): Dictionary with subtitle position\n                and duration info\n\n        Returns:\n            Image: The frame with subtitle and highlight effect applied\n        \"\"\"\n\n        speaker, text = subtitle.text.split(\"] \")\n        speaker = speaker.replace(\"[\", \"\").strip()\n\n        # Ensure opacity is a valid integer\n        opacity = max(0, min(255, int(opacity)))\n\n        # Highlight active speaker\n        if speaker in self.speakers:\n            highlight_color = (255, 200, 0)\n            speaker_pos = self.dp_positions[speaker]\n            self.speakers[speaker].highlight(\n                draw, speaker_pos, color=highlight_color, opacity=opacity\n            )\n\n            # Calculate text position\n            text_x = self.dp_margin_left + self.dp_size[0] + self.text_margin + self.content_horizontal_offset\n            text_y = speaker_pos[1] + (self.dp_size[1] // 2)\n\n            # Adjust text width based on horizontal offset to ensure it stays within the frame\n            # If offset is negative (moves content left), we have more space for text\n            # If offset is positive (moves content right), we have less space for text\n            if self.content_horizontal_offset &gt; 0:\n                # Reduce available text width when moved right\n                text_width = self.video_width - text_x - self.text_margin\n            else:\n                # Increase available text width when moved left (but ensure it doesn't go off screen)\n                text_width = min(\n                    self.video_width - text_x - self.text_margin,\n                    self.video_width - self.text_margin * 2\n                )\n\n            # Store text area for possible highlight effects\n            estimated_text_height = 100  # Approximate height for highlighting\n            self.active_subtitle_area = (\n                text_x,\n                text_y - estimated_text_height / 2,\n                text_x + text_width,\n                text_y + estimated_text_height / 2,\n            )\n\n            # Draw the subtitle text\n            self.text_renderer.draw_wrapped_text(\n                draw,\n                text,\n                (text_x, text_y),\n                max_width=text_width,\n                font_size=40,\n                color=(255, 255, 255, opacity),\n                anchor=\"lm\",\n            )\n\n            # Apply highlight effect if configured\n            if self.highlight_effect and self.active_subtitle_area:\n                # Get progress value from subtitle_info or use a default\n                progress = 0.0\n                if (\n                    subtitle_info\n                    and \"position\" in subtitle_info\n                    and \"duration\" in subtitle_info\n                ):\n                    # Calculate progress as a ratio of position to duration\n                    progress = (\n                        subtitle_info[\"position\"] / subtitle_info[\"duration\"]\n                        if subtitle_info[\"duration\"] &gt; 0\n                        else 0.0\n                    )\n\n                # Apply the highlight effect\n                frame = self.highlight_effect.apply(\n                    frame, self.active_subtitle_area, progress=progress\n                )\n\n        return frame\n\n    def add_speaker(self, name, image_path, shape=\"circle\"):\n        \"\"\"\n        Add a speaker to the layout\n\n        Args:\n            name (str): Name of the speaker\n            image_path (str): Path to the speaker's image\n            shape (str): Shape of the profile picture, defaults to \"circle\"\n        \"\"\"\n\n        self.speakers[name] = ProfilePicture(image_path, self.dp_size, shape)\n\n        # Recalculate positions when speakers are added\n        self._calculate_positions()\n\n        return self\n\n    def create_frame(\n        self, current_sub=None, opacity=255, background_color=(20, 20, 20), **kwargs\n    ):\n        \"\"\"\n        Create a frame with the podcast layout\n\n        Args:\n            current_sub (str): Current subtitle\n            opacity (int): Opacity of the subtitle\n            background_color (tuple): Background color in RGB format,\n                                      defaults to (20, 20, 20)\n            **kwargs: Additional keyword arguments:\n                subtitle_position (float): Current position within subtitle in seconds\n                subtitle_duration (float): Total duration of subtitle in seconds\n        \"\"\"\n        # Instead of modifying the subtitle object, we'll add the position and duration\n        # to a local dictionary that we'll use in _draw_subtitle\n        subtitle_info = {}\n        if \"subtitle_position\" in kwargs:\n            subtitle_info[\"position\"] = kwargs[\"subtitle_position\"]\n        if \"subtitle_duration\" in kwargs:\n            subtitle_info[\"duration\"] = kwargs[\"subtitle_duration\"]\n\n        # Ensure the opacity is a valid integer\n        opacity = max(0, min(255, int(opacity)))\n\n        # If we have a transition effect and opacity is specified,\n        # use the transition effect to calculate opacity\n        if hasattr(self, \"transition_effect\") and opacity != 255:\n            # Calculate progress from opacity\n            progress = opacity / 255.0\n            # Use transition effect to apply the transition\n            if self.transition_effect:\n                # We'll handle the opacity ourselves in this method,\n                # so just get the calculated opacity from the effect\n                opacity = self.transition_effect.apply(\n                    None, progress, opacity_only=True\n                )\n\n        # Create base frame\n        frame, draw = self._create_base_frame(background_color)\n\n        # Draw header\n        if self.logo_path:\n            self.header.set_logo(self.logo_path)\n        self.header.draw(frame, draw, self.video_width, self.title, opacity)\n\n        # Add all speaker DPs and names\n        for speaker, profile in self.speakers.items():\n            pos = self.dp_positions[speaker]\n            frame.paste(profile.image, pos, profile.image)\n\n            # Draw speaker name if enabled\n            if self.show_speaker_names:\n                name_y = pos[1] + self.dp_size[1] + self.name_margin\n                self.text_renderer.draw_text(\n                    draw,\n                    speaker,\n                    (pos[0] + self.dp_size[0] // 2, name_y),\n                    font_size=30,\n                    color=(200, 200, 200, opacity),\n                    anchor=\"mm\",\n                )\n\n        # Add subtitle if there's a current subtitle\n        if current_sub:\n            # Pass the subtitle_info dictionary as an additional parameter\n            frame = self._draw_subtitle(\n                frame, draw, current_sub, opacity, subtitle_info\n            )\n\n        # Draw watermark if enabled\n        if self.show_watermark and self.watermark:\n            self.watermark.draw(\n                frame, draw, self.video_width, self.video_height, opacity\n            )\n\n        # If we have a transition effect and opacity is not max,\n        # apply the full transition effect to the frame\n        if (\n            hasattr(self, \"transition_effect\")\n            and self.transition_effect\n            and opacity != 255\n        ):\n            progress = opacity / 255.0\n            frame = self.transition_effect.apply(frame, progress)\n\n        return np.array(frame)\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.__init__","title":"<code>__init__(video_width=1920, video_height=1080, header_height=150, dp_size=(120, 120), show_speaker_names=True, content_horizontal_offset=0, show_watermark=True)</code>","text":"<p>Initialize podcast layout</p> <p>Parameters:</p> Name Type Description Default <code>video_width</code> <code>int</code> <p>Width of the video</p> <code>1920</code> <code>video_height</code> <code>int</code> <p>Height of the video</p> <code>1080</code> <code>header_height</code> <code>int</code> <p>Height of the header section</p> <code>150</code> <code>dp_size</code> <code>tuple</code> <p>Size of profile pictures</p> <code>(120, 120)</code> <code>show_speaker_names</code> <code>bool</code> <p>Whether to show speaker names</p> <code>True</code> <code>content_horizontal_offset</code> <code>int</code> <p>Horizontal offset for the content (positive values move content right, negative values move content left)</p> <code>0</code> <code>show_watermark</code> <code>bool</code> <p>Whether to show the watermark</p> <code>True</code> Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>def __init__(\n    self,\n    video_width=1920,\n    video_height=1080,\n    header_height=150,\n    dp_size=(120, 120),\n    show_speaker_names=True,\n    content_horizontal_offset=0,\n    show_watermark=True,\n):\n    \"\"\"\n    Initialize podcast layout\n\n    Args:\n        video_width (int): Width of the video\n        video_height (int): Height of the video\n        header_height (int): Height of the header section\n        dp_size (tuple): Size of profile pictures\n        show_speaker_names (bool): Whether to show speaker names\n        content_horizontal_offset (int): Horizontal offset for the content\n            (positive values move content right,\n            negative values move content left)\n        show_watermark (bool): Whether to show the watermark\n    \"\"\"\n\n    super().__init__(video_width, video_height)\n\n    # Layout parameters\n    self.header_height = header_height\n    self.dp_size = dp_size\n    self.show_speaker_names = show_speaker_names\n    self.dp_margin_left = 40\n    self.text_margin = 50\n    self.name_margin = 30\n    self.content_horizontal_offset = content_horizontal_offset\n\n    # Initialize components\n    self.header = Header(height=header_height)\n    self.text_renderer = TextRenderer()\n\n    # Initialize watermark\n    self.show_watermark = show_watermark\n    if show_watermark:\n        # Opposite of background color (which is typically dark)\n        self.watermark = Watermark(color=(255, 255, 255))\n\n    # Store profile pictures and positions\n    self.speakers = {}\n    self.dp_positions = {}\n    self.logo_path = None\n    self.title = \"My Podcast\"\n\n    # Store the active subtitle area for highlighting\n    self.active_subtitle_area = None\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.add_speaker","title":"<code>add_speaker(name, image_path, shape='circle')</code>","text":"<p>Add a speaker to the layout</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the speaker</p> required <code>image_path</code> <code>str</code> <p>Path to the speaker's image</p> required <code>shape</code> <code>str</code> <p>Shape of the profile picture, defaults to \"circle\"</p> <code>'circle'</code> Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>def add_speaker(self, name, image_path, shape=\"circle\"):\n    \"\"\"\n    Add a speaker to the layout\n\n    Args:\n        name (str): Name of the speaker\n        image_path (str): Path to the speaker's image\n        shape (str): Shape of the profile picture, defaults to \"circle\"\n    \"\"\"\n\n    self.speakers[name] = ProfilePicture(image_path, self.dp_size, shape)\n\n    # Recalculate positions when speakers are added\n    self._calculate_positions()\n\n    return self\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.create_frame","title":"<code>create_frame(current_sub=None, opacity=255, background_color=(20, 20, 20), **kwargs)</code>","text":"<p>Create a frame with the podcast layout</p> <p>Parameters:</p> Name Type Description Default <code>current_sub</code> <code>str</code> <p>Current subtitle</p> <code>None</code> <code>opacity</code> <code>int</code> <p>Opacity of the subtitle</p> <code>255</code> <code>background_color</code> <code>tuple</code> <p>Background color in RGB format,                       defaults to (20, 20, 20)</p> <code>(20, 20, 20)</code> <code>**kwargs</code> <p>Additional keyword arguments: subtitle_position (float): Current position within subtitle in seconds subtitle_duration (float): Total duration of subtitle in seconds</p> <code>{}</code> Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>def create_frame(\n    self, current_sub=None, opacity=255, background_color=(20, 20, 20), **kwargs\n):\n    \"\"\"\n    Create a frame with the podcast layout\n\n    Args:\n        current_sub (str): Current subtitle\n        opacity (int): Opacity of the subtitle\n        background_color (tuple): Background color in RGB format,\n                                  defaults to (20, 20, 20)\n        **kwargs: Additional keyword arguments:\n            subtitle_position (float): Current position within subtitle in seconds\n            subtitle_duration (float): Total duration of subtitle in seconds\n    \"\"\"\n    # Instead of modifying the subtitle object, we'll add the position and duration\n    # to a local dictionary that we'll use in _draw_subtitle\n    subtitle_info = {}\n    if \"subtitle_position\" in kwargs:\n        subtitle_info[\"position\"] = kwargs[\"subtitle_position\"]\n    if \"subtitle_duration\" in kwargs:\n        subtitle_info[\"duration\"] = kwargs[\"subtitle_duration\"]\n\n    # Ensure the opacity is a valid integer\n    opacity = max(0, min(255, int(opacity)))\n\n    # If we have a transition effect and opacity is specified,\n    # use the transition effect to calculate opacity\n    if hasattr(self, \"transition_effect\") and opacity != 255:\n        # Calculate progress from opacity\n        progress = opacity / 255.0\n        # Use transition effect to apply the transition\n        if self.transition_effect:\n            # We'll handle the opacity ourselves in this method,\n            # so just get the calculated opacity from the effect\n            opacity = self.transition_effect.apply(\n                None, progress, opacity_only=True\n            )\n\n    # Create base frame\n    frame, draw = self._create_base_frame(background_color)\n\n    # Draw header\n    if self.logo_path:\n        self.header.set_logo(self.logo_path)\n    self.header.draw(frame, draw, self.video_width, self.title, opacity)\n\n    # Add all speaker DPs and names\n    for speaker, profile in self.speakers.items():\n        pos = self.dp_positions[speaker]\n        frame.paste(profile.image, pos, profile.image)\n\n        # Draw speaker name if enabled\n        if self.show_speaker_names:\n            name_y = pos[1] + self.dp_size[1] + self.name_margin\n            self.text_renderer.draw_text(\n                draw,\n                speaker,\n                (pos[0] + self.dp_size[0] // 2, name_y),\n                font_size=30,\n                color=(200, 200, 200, opacity),\n                anchor=\"mm\",\n            )\n\n    # Add subtitle if there's a current subtitle\n    if current_sub:\n        # Pass the subtitle_info dictionary as an additional parameter\n        frame = self._draw_subtitle(\n            frame, draw, current_sub, opacity, subtitle_info\n        )\n\n    # Draw watermark if enabled\n    if self.show_watermark and self.watermark:\n        self.watermark.draw(\n            frame, draw, self.video_width, self.video_height, opacity\n        )\n\n    # If we have a transition effect and opacity is not max,\n    # apply the full transition effect to the frame\n    if (\n        hasattr(self, \"transition_effect\")\n        and self.transition_effect\n        and opacity != 255\n    ):\n        progress = opacity / 255.0\n        frame = self.transition_effect.apply(frame, progress)\n\n    return np.array(frame)\n</code></pre>"},{"location":"audim/sub2pod/layouts/podcast/#audim.sub2pod.layouts.podcast.PodcastLayout.set_content_offset","title":"<code>set_content_offset(offset)</code>","text":"<p>Set horizontal offset for the content (display pictures and subtitles)</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Horizontal offset in pixels. Positive values move content right,  negative values move content left.</p> required Source code in <code>audim/sub2pod/layouts/podcast.py</code> <pre><code>def set_content_offset(self, offset):\n    \"\"\"\n    Set horizontal offset for the content (display pictures and subtitles)\n\n    Args:\n        offset (int): Horizontal offset in pixels.\n            Positive values move content right, \n            negative values move content left.\n    \"\"\"\n    self.content_horizontal_offset = offset\n    # Recalculate positions with the new offset\n    self._calculate_positions()\n    return self\n</code></pre>"},{"location":"audim/utils/extract/","title":"Extract","text":"<p>The <code>Extract</code> is an utility class that is used to extract or convert media data from various types of media files. For example, it can be used to extract audio from video files.</p> <p>List of utilities provided by the <code>Extract</code> class:</p> <ul> <li><code>extract_audio</code>: Extract audio from a video file with no loss in quality.</li> <li>more to come...</li> </ul> <p>Below is the API documentation for the <code>Extract</code> utility:</p>"},{"location":"audim/utils/extract/#audim.utils.extract.Extract","title":"<code>Extract</code>","text":"<p>A class for extracting and converting various forms of media data from various types of media files</p> Source code in <code>audim/utils/extract.py</code> <pre><code>class Extract:\n    \"\"\"\n    A class for extracting and converting various forms of media data from various types of media files\n    \"\"\"\n\n    def extract_audio(self, input_path, output_path, output_format='wav', bitrate='192k', sample_rate=44100) -&gt; str | None:\n        \"\"\"\n        Extract audio from a video file with no loss in quality.\n\n        Args:\n            input_path (str): Path to the input video file\n            output_path (str): Path to save the output audio file\n            output_format (str): Format of the output audio file. e.g.: mp3, wav, flac (default: wav)\n            bitrate (str): Bitrate for the output audio. e.g.: 128k, 192k, 320k (default: 192k)\n            sample_rate (int): Sample rate for the output audio. e.g.: 44100, 48000, 96000 (default: 44100)\n\n        Returns:\n            str | None: Path to the output audio file if extraction was successful, None otherwise\n        \"\"\"\n\n        # Check if input file exists\n        if not os.path.isfile(input_path):\n            print(f\"Error: Input file '{input_path}' does not exist.\")\n            return None\n\n        # Create output directory if it doesn't exist\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n        # If output_path doesn't have the correct extension, add it\n        if not output_path.lower().endswith(f'.{output_format.lower()}'):\n            output_path = f\"{output_path}.{output_format.lower()}\"\n\n        # Prepare FFmpeg command\n        cmd = [\n            \"ffmpeg\",\n            \"-i\", input_path,\n            \"-vn\",\n            \"-acodec\", self._get_audio_codec(output_format),\n            \"-ab\", bitrate,\n            \"-ar\", str(sample_rate),\n            \"-y\",\n            output_path\n        ]\n\n        # Run the command\n        try:\n            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            print(f\"Successfully extracted audio to {output_path}\")\n            return output_path\n        except subprocess.CalledProcessError as e:\n            print(f\"Error extracting audio: {e}\")\n            return None\n\n    def _get_audio_codec(self, format):\n        \"\"\"\n        Get the appropriate audio codec based on the output format.\n\n        Args:\n            format (str): Output audio format\n\n        Returns:\n            str: Audio codec to use\n        \"\"\"\n\n        # Audio formats to ffmpeg codecs mappings\n        codecs = {\n            \"mp3\": \"libmp3lame\",\n            \"aac\": \"aac\",\n            \"m4a\": \"aac\",\n            \"ogg\": \"libvorbis\",\n            \"wav\": \"pcm_s16le\",\n            \"flac\": \"flac\",\n            \"opus\": \"libopus\",\n            \"wma\": \"wmav2\",\n        }\n\n        # Get the appropriate ffmpeg codec\n        # Note: \"copy\" tells FFmpeg to stream copy the audio without re-encoding it.\n        format = format.lower()\n        ffmpeg_codec = codecs.get(format, \"copy\")\n\n        return ffmpeg_codec\n</code></pre>"},{"location":"audim/utils/extract/#audim.utils.extract.Extract.extract_audio","title":"<code>extract_audio(input_path, output_path, output_format='wav', bitrate='192k', sample_rate=44100)</code>","text":"<p>Extract audio from a video file with no loss in quality.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input video file</p> required <code>output_path</code> <code>str</code> <p>Path to save the output audio file</p> required <code>output_format</code> <code>str</code> <p>Format of the output audio file. e.g.: mp3, wav, flac (default: wav)</p> <code>'wav'</code> <code>bitrate</code> <code>str</code> <p>Bitrate for the output audio. e.g.: 128k, 192k, 320k (default: 192k)</p> <code>'192k'</code> <code>sample_rate</code> <code>int</code> <p>Sample rate for the output audio. e.g.: 44100, 48000, 96000 (default: 44100)</p> <code>44100</code> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: Path to the output audio file if extraction was successful, None otherwise</p> Source code in <code>audim/utils/extract.py</code> <pre><code>def extract_audio(self, input_path, output_path, output_format='wav', bitrate='192k', sample_rate=44100) -&gt; str | None:\n    \"\"\"\n    Extract audio from a video file with no loss in quality.\n\n    Args:\n        input_path (str): Path to the input video file\n        output_path (str): Path to save the output audio file\n        output_format (str): Format of the output audio file. e.g.: mp3, wav, flac (default: wav)\n        bitrate (str): Bitrate for the output audio. e.g.: 128k, 192k, 320k (default: 192k)\n        sample_rate (int): Sample rate for the output audio. e.g.: 44100, 48000, 96000 (default: 44100)\n\n    Returns:\n        str | None: Path to the output audio file if extraction was successful, None otherwise\n    \"\"\"\n\n    # Check if input file exists\n    if not os.path.isfile(input_path):\n        print(f\"Error: Input file '{input_path}' does not exist.\")\n        return None\n\n    # Create output directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # If output_path doesn't have the correct extension, add it\n    if not output_path.lower().endswith(f'.{output_format.lower()}'):\n        output_path = f\"{output_path}.{output_format.lower()}\"\n\n    # Prepare FFmpeg command\n    cmd = [\n        \"ffmpeg\",\n        \"-i\", input_path,\n        \"-vn\",\n        \"-acodec\", self._get_audio_codec(output_format),\n        \"-ab\", bitrate,\n        \"-ar\", str(sample_rate),\n        \"-y\",\n        output_path\n    ]\n\n    # Run the command\n    try:\n        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        print(f\"Successfully extracted audio to {output_path}\")\n        return output_path\n    except subprocess.CalledProcessError as e:\n        print(f\"Error extracting audio: {e}\")\n        return None\n</code></pre>"},{"location":"audim/utils/playback/","title":"Playback","text":"<p>The <code>Playback</code> is an utility class that is used to playback various outputs and generations from Audim.</p> <p>It is also used to test and validate the results of the transcriber, subtitle generator and video generator (coming soon).</p> <p>List of utilities provided by the <code>Playback</code> class:</p> <ul> <li><code>play_audio_with_srt</code>: Play an audio file with synchronized subtitles in CLI</li> <li>more to come...</li> </ul> <p>Below is the API documentation for the <code>Playback</code> utility:</p>"},{"location":"audim/utils/playback/#audim.utils.playback.Playback","title":"<code>Playback</code>","text":"<p>Class for synchronized audio and video playback with subtitles</p> <p>Use it to playback various outputs and generations from Audim and testing and validating the results</p> Source code in <code>audim/utils/playback.py</code> <pre><code>class Playback:\n    \"\"\"\n    Class for synchronized audio and video playback with subtitles\n\n    Use it to playback various outputs and generations from Audim\n    and testing and validating the results\n    \"\"\"\n\n    def _srt_audio_player(self, subs, start_time):\n        \"\"\"\n        Play audio with subtitles in CLI in realtime and in sync\n\n        Args:\n            subs (list): List of pysrt.SubRipItem objects\n            start_time (datetime): Start time of the playback\n        \"\"\"\n\n        for sub in subs:\n            # Calculate wait time until this subtitle should be shown\n            # Convert subtitle time to seconds from start\n            start_seconds = (\n                sub.start.hours * 3600\n                + sub.start.minutes * 60\n                + sub.start.seconds\n                + sub.start.milliseconds / 1000\n            )\n            current_time = (datetime.now() - start_time).total_seconds()\n\n            # If we need to wait, sleep until it's time to show this subtitle\n            if start_seconds &gt; current_time:\n                time.sleep(start_seconds - current_time)\n\n            # Clear previous subtitle and display current one\n            print(\"\\033[H\\033[J\", end=\"\")\n            print(f\"\\n\\n\\n\\n\\n{sub.text}\\n\")\n\n            # Calculate duration of this subtitle\n            end_seconds = (\n                sub.end.hours * 3600\n                + sub.end.minutes * 60\n                + sub.end.seconds\n                + sub.end.milliseconds / 1000\n            )\n            duration = end_seconds - start_seconds\n\n            # If next subtitle doesn't start immediately, clear screen\n            if duration &gt; 0:\n                time.sleep(duration)\n                print(\"\\033[H\\033[J\", end=\"\")\n\n    def play_audio_with_srt(self, audio_file, srt_file):\n        \"\"\"\n        Play audio file with synchronized subtitles in CLI\n\n        Args:\n            audio_file (str): Path to the audio file\n            srt_file (str): Path to the SRT subtitle file\n        \"\"\"\n\n        try:\n            # Load the audio file\n            audio = AudioSegment.from_file(audio_file)\n\n            # Load the subtitles\n            subs = pysrt.open(srt_file)\n\n            print(\"Playing audio with subtitles. Press Ctrl+C to stop.\")\n            time.sleep(1)\n            print(\"\\033[H\\033[J\", end=\"\")\n\n            # Start the subtitle display thread\n            start_time = datetime.now()\n            subtitle_thread = threading.Thread(\n                target=self._srt_audio_player, args=(subs, start_time)\n            )\n            subtitle_thread.daemon = True\n            subtitle_thread.start()\n\n            # Play the audio\n            play(audio)\n\n        except KeyboardInterrupt:\n            print(\"\\nPlayback stopped.\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n            raise\n</code></pre>"},{"location":"audim/utils/playback/#audim.utils.playback.Playback.play_audio_with_srt","title":"<code>play_audio_with_srt(audio_file, srt_file)</code>","text":"<p>Play audio file with synchronized subtitles in CLI</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>str</code> <p>Path to the audio file</p> required <code>srt_file</code> <code>str</code> <p>Path to the SRT subtitle file</p> required Source code in <code>audim/utils/playback.py</code> <pre><code>def play_audio_with_srt(self, audio_file, srt_file):\n    \"\"\"\n    Play audio file with synchronized subtitles in CLI\n\n    Args:\n        audio_file (str): Path to the audio file\n        srt_file (str): Path to the SRT subtitle file\n    \"\"\"\n\n    try:\n        # Load the audio file\n        audio = AudioSegment.from_file(audio_file)\n\n        # Load the subtitles\n        subs = pysrt.open(srt_file)\n\n        print(\"Playing audio with subtitles. Press Ctrl+C to stop.\")\n        time.sleep(1)\n        print(\"\\033[H\\033[J\", end=\"\")\n\n        # Start the subtitle display thread\n        start_time = datetime.now()\n        subtitle_thread = threading.Thread(\n            target=self._srt_audio_player, args=(subs, start_time)\n        )\n        subtitle_thread.daemon = True\n        subtitle_thread.start()\n\n        # Play the audio\n        play(audio)\n\n    except KeyboardInterrupt:\n        print(\"\\nPlayback stopped.\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n        raise\n</code></pre>"},{"location":"audim/utils/subtitle/","title":"Subtitle","text":"<p>The <code>Subtitle</code> is an utility class that is used to handle and modify subtitle files.</p> <p>It is used to handle the output subtitle files from <code>aud2sub</code> before feeding them to <code>sub2pod</code> for video generation.</p> <p>List of utilities provided by the <code>Subtitle</code> class:</p> <ul> <li><code>replace_speakers</code>: Replace the audim speaker tags (names) in the subtitle file with new ones</li> <li><code>preview_replacement</code>: Preview the changes from <code>replace_speakers</code> in CLI without modifying the file</li> <li>more to come...</li> </ul> <p>Below is the API documentation for the <code>Subtitle</code> utility:</p>"},{"location":"audim/utils/subtitle/#audim.utils.subtitle.Subtitle","title":"<code>Subtitle</code>","text":"<p>Contains utility functions for SRT files</p> Source code in <code>audim/utils/subtitle.py</code> <pre><code>class Subtitle:\n    \"\"\"\n    Contains utility functions for SRT files\n    \"\"\"\n\n    def replace_speakers(self, srt_file, speakers, in_place=True):\n        \"\"\"\n        Replace speaker placeholders with actual names in SRT file\n\n        Example, allows replacing \"[Speaker 1]\", \"[Speaker 2]\", etc.\n        with actual speaker names such as \"[Host]\", \"[Guest]\", etc.\n\n        Args:\n            srt_file (str): Path to the SRT file\n            speakers (list or dict): Either a list of speaker names in order\n                or a dictionary mapping speaker numbers/names to actual names\n            in_place (bool): Whether to modify the file in place (default: True)\n                If False, returns modified subs without saving\n\n        Returns:\n            pysrt.SubRipFile: The modified subtitles object\n        \"\"\"\n\n        # Load the subtitles\n        subs = pysrt.open(srt_file)\n\n        # Determine if speakers is a list or dictionary\n        speaker_map = {}\n        if isinstance(speakers, list):\n            # Create mapping from [Speaker N] to [SpeakerName]\n            for i, name in enumerate(speakers, 1):\n                speaker_map[f\"[Speaker {i}]\"] = f\"[{name.title()}]\"\n        elif isinstance(speakers, dict):\n            # Handle dictionary input\n            for key, name in speakers.items():\n                # If the key is an integer, convert to [Speaker N] format\n                if isinstance(key, int):\n                    speaker_map[f\"[Speaker {key}]\"] = f\"[{name.title()}]\"\n                # If the key already includes 'Speaker', use as is\n                elif \"Speaker\" in str(key):\n                    # Ensure proper formatting with brackets\n                    formatted_key = (\n                        f\"[{key}]\" if not str(key).startswith(\"[\") else str(key)\n                    )\n                    formatted_key = (\n                        formatted_key\n                        if formatted_key.endswith(\"]\")\n                        else f\"{formatted_key}]\"\n                    )\n                    speaker_map[formatted_key] = f\"[{name.title()}]\"\n                else:\n                    # Default case, assume key is the speaker identifier\n                    speaker_map[f\"[{key}]\"] = f\"[{name.title()}]\"\n        else:\n            raise ValueError(\"Speakers must be a list or dictionary\")\n\n        # Process each subtitle\n        for sub in subs:\n            for placeholder, real_name in speaker_map.items():\n                # Replace each speaker placeholder with the real name\n                sub.text = sub.text.replace(placeholder, real_name)\n\n        # Save changes if requested\n        if in_place:\n            subs.save(srt_file, encoding=\"utf-8\")\n\n        return subs\n\n    def preview_replacement(self, srt_file, speakers, limit=5, pretty_print=True):\n        \"\"\"\n        Preview the speaker replacements without modifying the file\n\n        Args:\n            srt_file (str): Path to the SRT file\n            speakers (list or dict): Either a list of speaker names in order\n                or a dictionary mapping speaker numbers/names to actual names\n            limit (int): Maximum number of subtitles to display in preview\n            pretty_print (bool): Whether to print a formatted preview to console\n\n        Returns:\n            list: List of tuples with (original_text, modified_text)\n        \"\"\"\n\n        # Get modified subs without saving\n        modified_subs = self.replace_speakers(srt_file, speakers, in_place=False)\n        original_subs = pysrt.open(srt_file)\n\n        # Create preview of changes\n        preview = []\n        for i, (orig, mod) in enumerate(zip(original_subs, modified_subs)):\n            if i &gt;= limit:\n                break\n            preview.append((orig.text, mod.text))\n\n        if pretty_print:\n            print(\n                f\"\\n{'=' * 50}\\n\"\n                f\"SPEAKER REPLACEMENT PREVIEW ({limit} entries max)\\n\"\n                f\"{'=' * 50}\"\n            )\n\n            for i, (orig, mod) in enumerate(preview, 1):\n                # Highlight the differences\n                print(f\"\\n#{i}: \")\n                print(f\"  BEFORE: {orig}\")\n                print(f\"  AFTER : {mod}\")\n\n                # Only print divider if not the last item\n                if i &lt; len(preview):\n                    print(f\"  {'-' * 48}\")\n\n            print(f\"\\n{'=' * 50}\\n\")\n\n        return preview\n</code></pre>"},{"location":"audim/utils/subtitle/#audim.utils.subtitle.Subtitle.preview_replacement","title":"<code>preview_replacement(srt_file, speakers, limit=5, pretty_print=True)</code>","text":"<p>Preview the speaker replacements without modifying the file</p> <p>Parameters:</p> Name Type Description Default <code>srt_file</code> <code>str</code> <p>Path to the SRT file</p> required <code>speakers</code> <code>list or dict</code> <p>Either a list of speaker names in order or a dictionary mapping speaker numbers/names to actual names</p> required <code>limit</code> <code>int</code> <p>Maximum number of subtitles to display in preview</p> <code>5</code> <code>pretty_print</code> <code>bool</code> <p>Whether to print a formatted preview to console</p> <code>True</code> <p>Returns:</p> Name Type Description <code>list</code> <p>List of tuples with (original_text, modified_text)</p> Source code in <code>audim/utils/subtitle.py</code> <pre><code>def preview_replacement(self, srt_file, speakers, limit=5, pretty_print=True):\n    \"\"\"\n    Preview the speaker replacements without modifying the file\n\n    Args:\n        srt_file (str): Path to the SRT file\n        speakers (list or dict): Either a list of speaker names in order\n            or a dictionary mapping speaker numbers/names to actual names\n        limit (int): Maximum number of subtitles to display in preview\n        pretty_print (bool): Whether to print a formatted preview to console\n\n    Returns:\n        list: List of tuples with (original_text, modified_text)\n    \"\"\"\n\n    # Get modified subs without saving\n    modified_subs = self.replace_speakers(srt_file, speakers, in_place=False)\n    original_subs = pysrt.open(srt_file)\n\n    # Create preview of changes\n    preview = []\n    for i, (orig, mod) in enumerate(zip(original_subs, modified_subs)):\n        if i &gt;= limit:\n            break\n        preview.append((orig.text, mod.text))\n\n    if pretty_print:\n        print(\n            f\"\\n{'=' * 50}\\n\"\n            f\"SPEAKER REPLACEMENT PREVIEW ({limit} entries max)\\n\"\n            f\"{'=' * 50}\"\n        )\n\n        for i, (orig, mod) in enumerate(preview, 1):\n            # Highlight the differences\n            print(f\"\\n#{i}: \")\n            print(f\"  BEFORE: {orig}\")\n            print(f\"  AFTER : {mod}\")\n\n            # Only print divider if not the last item\n            if i &lt; len(preview):\n                print(f\"  {'-' * 48}\")\n\n        print(f\"\\n{'=' * 50}\\n\")\n\n    return preview\n</code></pre>"},{"location":"audim/utils/subtitle/#audim.utils.subtitle.Subtitle.replace_speakers","title":"<code>replace_speakers(srt_file, speakers, in_place=True)</code>","text":"<p>Replace speaker placeholders with actual names in SRT file</p> <p>Example, allows replacing \"[Speaker 1]\", \"[Speaker 2]\", etc. with actual speaker names such as \"[Host]\", \"[Guest]\", etc.</p> <p>Parameters:</p> Name Type Description Default <code>srt_file</code> <code>str</code> <p>Path to the SRT file</p> required <code>speakers</code> <code>list or dict</code> <p>Either a list of speaker names in order or a dictionary mapping speaker numbers/names to actual names</p> required <code>in_place</code> <code>bool</code> <p>Whether to modify the file in place (default: True) If False, returns modified subs without saving</p> <code>True</code> <p>Returns:</p> Type Description <p>pysrt.SubRipFile: The modified subtitles object</p> Source code in <code>audim/utils/subtitle.py</code> <pre><code>def replace_speakers(self, srt_file, speakers, in_place=True):\n    \"\"\"\n    Replace speaker placeholders with actual names in SRT file\n\n    Example, allows replacing \"[Speaker 1]\", \"[Speaker 2]\", etc.\n    with actual speaker names such as \"[Host]\", \"[Guest]\", etc.\n\n    Args:\n        srt_file (str): Path to the SRT file\n        speakers (list or dict): Either a list of speaker names in order\n            or a dictionary mapping speaker numbers/names to actual names\n        in_place (bool): Whether to modify the file in place (default: True)\n            If False, returns modified subs without saving\n\n    Returns:\n        pysrt.SubRipFile: The modified subtitles object\n    \"\"\"\n\n    # Load the subtitles\n    subs = pysrt.open(srt_file)\n\n    # Determine if speakers is a list or dictionary\n    speaker_map = {}\n    if isinstance(speakers, list):\n        # Create mapping from [Speaker N] to [SpeakerName]\n        for i, name in enumerate(speakers, 1):\n            speaker_map[f\"[Speaker {i}]\"] = f\"[{name.title()}]\"\n    elif isinstance(speakers, dict):\n        # Handle dictionary input\n        for key, name in speakers.items():\n            # If the key is an integer, convert to [Speaker N] format\n            if isinstance(key, int):\n                speaker_map[f\"[Speaker {key}]\"] = f\"[{name.title()}]\"\n            # If the key already includes 'Speaker', use as is\n            elif \"Speaker\" in str(key):\n                # Ensure proper formatting with brackets\n                formatted_key = (\n                    f\"[{key}]\" if not str(key).startswith(\"[\") else str(key)\n                )\n                formatted_key = (\n                    formatted_key\n                    if formatted_key.endswith(\"]\")\n                    else f\"{formatted_key}]\"\n                )\n                speaker_map[formatted_key] = f\"[{name.title()}]\"\n            else:\n                # Default case, assume key is the speaker identifier\n                speaker_map[f\"[{key}]\"] = f\"[{name.title()}]\"\n    else:\n        raise ValueError(\"Speakers must be a list or dictionary\")\n\n    # Process each subtitle\n    for sub in subs:\n        for placeholder, real_name in speaker_map.items():\n            # Replace each speaker placeholder with the real name\n            sub.text = sub.text.replace(placeholder, real_name)\n\n    # Save changes if requested\n    if in_place:\n        subs.save(srt_file, encoding=\"utf-8\")\n\n    return subs\n</code></pre>"},{"location":"examples/podcast_01/","title":"Basic Podcast Video Generation","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: March 05, 2025</li> <li>Last Updated: March 05, 2025</li> <li>Compatible with: Audim v0.0.1</li> </ul> <p>This example demonstrates how to generate a simple podcast video from a subtitle file and audio file using Audim's Sub2Pod module.</p>"},{"location":"examples/podcast_01/#prerequisites","title":"Prerequisites","text":"<p>Before running this example, make sure you have:</p> <ol> <li>Installed Audim following the installation instructions</li> <li> <p>Created the required input files:</p> <ul> <li><code>input/podcast.srt</code> - Subtitle file with speaker tags</li> <li><code>input/podcast.mp3</code> - Audio file of the podcast</li> <li><code>input/host_dp.png</code> - Host profile picture</li> <li><code>input/guest_dp.png</code> - Guest profile picture</li> <li><code>input/logo.png</code> - Brand logo (optional)</li> </ul> </li> </ol>"},{"location":"examples/podcast_01/#example-srt-file","title":"Example SRT File","text":"<p>Your SRT file should follow the standard SubRip Subtitle format with added speaker tags as expected by Audim.</p> <p>Below is an example of an SRT file with speaker tags:</p> <pre><code>1\n00:00:00,000 --&gt; 00:00:04,500\n[Host] Welcome to our podcast!\n\n2\n00:00:04,600 --&gt; 00:00:08,200\n[Guest] Thank you! Glad to be here.\n</code></pre>"},{"location":"examples/podcast_01/#example-code-implementation","title":"Example Code Implementation","text":"<p>The following code demonstrates how to generate a podcast video from an SRT file and audio file using Audim's Sub2Pod module.</p> <pre><code>from datetime import datetime\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Add speakers\nprint(\"Adding speakers...\")\nlayout.add_speaker(\"Host\", \"input/host_dp.png\")\nlayout.add_speaker(\"Guest\", \"input/guest_dp.png\")\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=\"input/podcast.srt\",\n    audio_path=\"input/podcast.mp3\",\n    logo_path=\"input/logo.png\",\n    title=\"My Awesome Podcast\"\n)\n\n# Export the final video\nprint(\"Exporting video...\")\ndatetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\ngenerator.export_video(f\"output/podcast_{datetime}.mp4\")\n</code></pre>"},{"location":"examples/podcast_01/#output","title":"Output","text":"<p>The output podcast video will be saved in the <code>output</code> directory with a timestamp in the filename.</p>"},{"location":"examples/podcast_01/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with this example: - Verify you're using the compatible version of Audim - Check that all input files exist and are in the correct format - Ensure your SRT file has proper speaker tags in the format <code>[SpeakerName]</code></p>"},{"location":"examples/podcast_01/#see-also","title":"See Also","text":"<ul> <li>API Documentation for PodcastLayout</li> <li>API Documentation for VideoGenerator</li> </ul>"},{"location":"examples/podcast_02/","title":"Creating a Professional Podcast Video with Audim","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: March 13, 2025</li> <li>Last Updated: March 14, 2025</li> <li>Compatible with: Audim v0.0.2</li> </ul> <p>This example demonstrates how to create a professional-looking podcast video using Audim's <code>Sub2Pod</code> module, featuring real speakers with profile pictures, custom branding, and high-quality output.</p>"},{"location":"examples/podcast_02/#overview","title":"Overview","text":"<p>In this tutorial, we'll transform a conversation between Grant Sanderson (from 3Blue1Brown) and Sal Khan (from Khan Academy) into a visually engaging podcast video. We'll walk through:</p> <ol> <li>Preparing the input files</li> <li>Setting up the podcast layout</li> <li>Generating the video with Audim</li> <li>Reviewing the final output</li> </ol> <p>Note: The conversation between Grant and Sal is taken from this podcast.</p>"},{"location":"examples/podcast_02/#input-files","title":"Input Files","text":""},{"location":"examples/podcast_02/#1-podcast-audio-file","title":"1. Podcast Audio File","text":"<p>We need to have the audio recording of the podcast that we want to convert to a video.</p> <p>Below is a sample of the podcast audio we'll be using:</p>      Your browser does not support the audio element.    <p>Audio snippet from \"Sal Khan: Beyond Khan Academy | 3b1b Podcast #2\"</p>"},{"location":"examples/podcast_02/#2-podcast-subtitles-file-srt","title":"2. Podcast Subtitles File (.SRT)","text":"<p>The SRT file should contain the transcription with speaker tags for the package to understand the speaker for each text. The SRT file should follow the standard SubRip Subtitle format with added speaker tags as expected by Audim.</p> <p>Below is the SRT file used for this example:</p>"},{"location":"examples/podcast_02/#3-other-files","title":"3. Other Files","text":"<p>Along with the audio and subtitles files, we also need the following files:</p> <ul> <li>Profile Picture of Grant Sanderson</li> <li>Profile Picture of Sal Khan</li> <li>Brand Logo of 3Blue1Brown</li> </ul>"},{"location":"examples/podcast_02/#code-implementation","title":"Code Implementation","text":"<p>After gathering all the files, we can now generate the podcast video using Audim.</p> <p>Here's the complete code to generate our podcast video:</p> <pre><code>from datetime import datetime\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Add speakers\nprint(\"Adding speakers...\")\nlayout.add_speaker(\"Grant Sanderson\", \"input/grant.png\")\nlayout.add_speaker(\"Sal Khan\", \"input/sal.png\")\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=\"input/podcast.srt\",\n    audio_path=\"input/podcast.mp3\",\n    logo_path=\"input/logo.png\",\n    title=\"3b1b Podcast: Sal Khan: Beyond Khan Academy\",\n    cpu_core_utilization=\"max\"\n)\n\n# Export the final video\nprint(\"Exporting video...\")\ndatetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\ngenerator.export_video(f\"output/podcast_{datetime}.mp4\")\n</code></pre> <p>Here is the terminal logs upon running the code:</p> <pre><code>(audim) (base) atanu@atanu-LOQ-15APH8:~/Workspace/GitHub/audim$ python test.py \nCreating layout...\nAdding speakers...\nGenerating video...\n[2025-03-13 23:25:10] VideoGenerator (INFO) - Loading subtitles from input/podcast.srt\n[2025-03-13 23:25:10] VideoGenerator (INFO) - Using 16 CPU cores for parallel processing\n[2025-03-13 23:25:10] VideoGenerator (INFO) - Processing subtitle to generate frames in 23 batches\nProcessing batch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:37&lt;00:00,  1.64s/batch, frames processed=4727]\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Frame generation completed: Total 4727 frames created\nExporting video...\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Starting video generation process with 4727 frames\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Video duration: 156.06s (adjusted to match audio)\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Attempting video export with FFmpeg encoding\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Preparing frame list for FFmpeg\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Using NVIDIA GPU acceleration for video encoding\n[2025-03-13 23:25:48] VideoGenerator (INFO) - Starting FFmpeg encoding process\nEncoding video:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 99/100 [01:09&lt;00:00,  1.42%/s]\n[2025-03-13 23:26:58] VideoGenerator (INFO) - Video successfully encoded to output/podcast_20250313232548.mp4\n[2025-03-13 23:26:58] VideoGenerator (INFO) - Cleaned up temporary files in /tmp/tmpoqv_t5x6\n[2025-03-13 23:26:58] VideoGenerator (INFO) - Video generation completed! Exported to: output/podcast_20250313232548.mp4\n</code></pre>"},{"location":"examples/podcast_02/#output-video","title":"Output Video","text":"<p>Here's how the generated video looks like upon completion of the rendering process:</p>      Your browser does not support the video element."},{"location":"examples/podcast_02/#code-breakdown","title":"Code Breakdown","text":"<ol> <li>Layout Creation: We create a <code>PodcastLayout</code> with Full HD resolution (1920\u00d71080) and enable speaker name display.</li> <li>Speaker Configuration: We add two speakers with their respective profile pictures.</li> <li>Video Generation: We initialize a <code>VideoGenerator</code> with our layout and set the frame rate to 30 FPS.</li> <li>Content Processing: The generator processes our SRT and audio files, incorporating the logo and title.</li> <li>Performance Optimization: We use <code>cpu_core_utilization=\"max\"</code> to leverage all available CPU cores for faster frame generation + native system FFmpeg with NVIDIA GPU acceleration for faster video encoding and rendering.</li> <li>Export: The final video is saved with a timestamp in the filename for easy versioning.</li> </ol>"},{"location":"examples/podcast_02/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ul> <li>Ensure your SRT file has proper speaker tags in the format <code>[SpeakerName]</code></li> <li>Verify that speaker names in the SRT match exactly with those added via <code>add_speaker()</code></li> <li>Check that all input files exist and are in the correct format</li> <li>For performance issues, try adjusting the <code>cpu_core_utilization</code> parameter</li> </ul>"},{"location":"examples/podcast_02/#see-also","title":"See Also","text":"<ul> <li>Basic Podcast Example</li> <li>API Documentation for VideoGenerator</li> <li>API Documentation for PodcastLayout</li> </ul>"},{"location":"examples/podcast_03/","title":"Advanced Video Effects with Progressive Disclosure of Complexity","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: April 16, 2025</li> <li>Last Updated: April 17, 2025</li> <li>Compatible with: Audim v0.0.3</li> </ul> <p>This example explores Audim's implementation of the \"progressive disclosure of complexity\" design principle through the <code>effects</code> module in the <code>sub2pod</code> package. We'll demonstrate how Audim provides a smooth learning curve for users of all experience levels.</p>"},{"location":"examples/podcast_03/#what-is-progressive-disclosure-of-complexity","title":"What is Progressive Disclosure of Complexity?","text":"<p>Progressive disclosure of complexity is a design principle that gradually reveals advanced functionality as users become more experienced with a system. This concept originated in UI design but has been adapted for API design in software libraries.</p> <p>As described by design experts:</p> <p>\"Progressive disclosure is an interaction design technique that sequences information and actions across several screens in order to reduce feelings of overwhelm for the user.\"</p> <p>\u2014 Interaction Design Foundation</p> <p>Specially in module or library design, this principle is realized by creating higher level APIs and lower level APIs. While the lower level APIs are granular, fundamental, small and rigid catering to specific functionalities of the module or library. The higher level APIs are complex and formed by combining the lower level APIs catering to specific end user requirements and use cases.</p> <p>This results in end users using the higher level APIs with ease, getting out of the box experience, while also being able to dig deeper into the lower level APIs for specific customizations when needed.</p> <p>The best way to put it is to quote the creator of Keras:</p> <p>\"A key design principle I follow in libraries (e.g. Keras) is 'progressive disclosure of complexity'. Make it easy to get started, yet make it possible to handle arbitrarily flexible use cases, only requiring incremental learning at each step.\"</p> <p>\u2014 Fran\u00e7ois Chollet</p> <p>This approach is used by many popular libraries like Keras, Hugging Face Transformers, Hugging Face Diffusers and many more allowing users to:</p> <ul> <li>Start simple with sensible defaults</li> <li>Incrementally discover more advanced features</li> <li>Access the full power of the library when needed</li> </ul>"},{"location":"examples/podcast_03/#how-audims-sub2pod-submodule-implements-progressive-disclosure","title":"How Audim's <code>sub2pod</code> submodule Implements Progressive Disclosure","text":"<p>Audim's <code>sub2pod</code> submodule implements progressive disclosure of complexity through a carefully designed hierarchy of abstractions:</p> <ol> <li> <p>High-Level API (<code>VideoGenerator</code> &amp; <code>PodcastLayout</code>)</p> <ul> <li><code>VideoGenerator</code> takes care of video generation and rendering</li> <li><code>PodcastLayout</code> takes care of how each frames in the video will look like</li> <li>Provides out-of-the-box functionality for podcast video generation</li> <li>Offers sensible defaults for all parameters</li> <li>Perfect for users who want to get started quickly</li> </ul> </li> <li> <p>Mid-Level API (<code>BaseLayout</code>)</p> <ul> <li>Allows customization of layouts, elements in the layout and it's effects</li> <li>Provides string-based configuration for common use cases</li> <li>Enables users to put together various elements and effects to create their own layouts</li> <li>It's like a lego or a puzzle piece where lower level elements and effects can be used to create new layouts</li> <li>Ideal for users who want to customize their videos</li> </ul> </li> <li> <p>Lower-Level API (<code>Header</code>, <code>Profile</code>, <code>Text</code> Elements and <code>Transition</code>, <code>Highlight</code> Effects)</p> <ul> <li>Offers complete control over each elements and effects</li> <li>These elements are the fundamental building blocks that Audim provides</li> <li>These effects are the fundamental animations that Audim provides</li> <li>These elements and effects can be combined to create new layouts and animations</li> </ul> </li> <li> <p>Lowest-Level API (<code>BaseElement</code> &amp; <code>BaseEffect</code>)</p> <ul> <li>Offers complete control over rendering and effects</li> <li>Allows creation of custom elements and effects</li> <li>Provides access to all parameters and methods</li> <li>Designed for power users and developers</li> </ul> </li> </ol> <p>So, by providing API abstractions from higher levels to the lower levels, Audim's <code>sub2pod</code> submodule allows users to:</p> <ul> <li>Start using and generation podcast videos out of the box with sensible defaults</li> <li>Incrementally discover more advanced features and customizations when needed</li> <li>Access the full power of the library when needed with fine-tuned control over the layouts, elements and effects</li> <li>Power users can also implement their own custom layouts, elements and effects by overriding <code>BaseLayout</code>, <code>BaseElement</code> and <code>BaseEffect</code> classes.</li> </ul> <p>This layered approach creates a natural progression path for users:</p> <pre><code># Level 1: High-Level API (Simple)\nlayout = PodcastLayout(...)\ngenerator = VideoGenerator(layout)\ngenerator.generate_from_srt(...)\n</code></pre> <pre><code># Level 2: Mid-Level API (Customization)\nlayout.set_transition_effect(\"fade\")\nlayout.set_highlight_effect(\"none\")\n</code></pre> <pre><code># Level 3: Low-Level API\nclass CustomEffect(BaseEffect):\n    # Override for full customization and control\n    def apply(self, frame, progress):\n        # Custom effect implementation\n        pass\n</code></pre> <p>So, the abstraction layers and inheritance hierarchy of Audim's <code>sub2pod</code> submodule can be visualized as follows:</p> <pre><code>graph TD\n    A[VideoGenerator] --&gt; B[PodcastLayout]\n    B --&gt; C[BaseLayout]\n    C --&gt; D[Elements]\n    C --&gt; E[Effects]\n    D --&gt; F[Header]\n    D --&gt; G[ProfilePicture]\n    D --&gt; H[TextRenderer]\n    E --&gt; I[Transition]\n    E --&gt; J[Highlight]\n    I --&gt; K[BaseTransition]\n    J --&gt; L[BaseHighlight]</code></pre>"},{"location":"examples/podcast_03/#why-this-is-powerful","title":"Why this is powerful?","text":""},{"location":"examples/podcast_03/#1-progressive-disclosure-of-complexity","title":"1. Progressive Disclosure of Complexity","text":"<p>This approach creates a natural hierarchy of complexity:</p> <ul> <li>Simple level: Users choose a pre-configured layout with default effects</li> <li>Intermediate level: Users customize effects for elements on existing layouts</li> <li>Advanced level: Users create custom layouts with custom elements and effects</li> </ul> <p>This matches how video editors typically work - first selecting templates, then adjusting effects, and finally creating custom compositions when needed.</p>"},{"location":"examples/podcast_03/#2-end-user-experience","title":"2. End User Experience","text":"<p>For video creators and editors, this model is intuitive because:</p> <ul> <li>It follows familiar mental models from tools like OBS, Premiere Pro, and After Effects</li> <li>Effects are naturally tied to how content appears (the layout)</li> <li>The separation keeps the API clean while maintaining flexibility</li> <li>Users can think in terms of \"scenes\" (layouts) that have both positioning and visual effects</li> </ul>"},{"location":"examples/podcast_03/#3-developer-experience","title":"3. Developer Experience","text":"<p>For developers and power users, this architecture provides:</p> <ul> <li>Clear extension points for adding new features</li> <li>Well-defined interfaces between components</li> <li>Easy testing and maintenance of individual components</li> <li>Ability to mix and match different levels of abstraction</li> </ul>"},{"location":"examples/podcast_03/#4-performance-and-maintainability","title":"4. Performance and Maintainability","text":"<p>The layered architecture also benefits the codebase itself:</p> <ul> <li>Each layer can be optimized independently</li> <li>Changes in one layer don't affect others</li> <li>Easier to add new features without breaking existing code</li> <li>Better separation of concerns</li> </ul>"},{"location":"examples/podcast_03/#how-the-new-effects-module-implements-progressive-disclosure","title":"How the new <code>effects</code> module Implements Progressive Disclosure","text":"<p>Audim's <code>effects</code> module provides transition and highlight effects for videos with three distinct levels of complexity:</p> <ol> <li>Level 1: Default Usage - No configuration needed</li> <li>Level 2: Simple Customization - Basic string-based configuration</li> <li>Level 3: Advanced Customization - Detailed parameter configuration</li> </ol> <p>This approach allows beginners to get started quickly while giving advanced users the power and flexibility they need.</p>"},{"location":"examples/podcast_03/#example-three-levels-of-complexity","title":"Example: Three Levels of Complexity","text":"<p>Let's explore how you can use Audim's effects at different complexity levels:</p>"},{"location":"examples/podcast_03/#level-1-default-usage-no-configuration","title":"Level 1: Default Usage (No Configuration)","text":"<p>The simplest way to use Audim is with default settings. The <code>PodcastLayout</code> automatically includes a default fade transition:</p> <pre><code>from audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout with default effects\nlayout = PodcastLayout(\n    video_width=1920, \n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Add speakers\nlayout.add_speaker(\"Host\", \"input/host.png\")\nlayout.add_speaker(\"Guest\", \"input/guest.png\")\n\n# Create generator with this layout\ngenerator = VideoGenerator(layout)\n\n# The layout will automatically use the default effects\n# No explicit configuration needed!\n</code></pre> <p>At this level, users don't need to know anything about effects - they just work.</p>"},{"location":"examples/podcast_03/#level-2-simple-customization","title":"Level 2: Simple Customization","text":"<p>As users become more comfortable, they can easily customize effects by simply specifying the effect type:</p> <pre><code># Create podcast layout\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Simple customization - just specify effect type\nlayout.set_transition_effect(\"fade\")\nlayout.set_highlight_effect(\"glow\")\n\n# Add speakers and generate video as before\n</code></pre> <p>This level introduces a clean, string-based API that's easy to understand and use.</p>"},{"location":"examples/podcast_03/#level-3-advanced-customization","title":"Level 3: Advanced Customization","text":"<p>Power users can access detailed customization options when they need fine-grained control:</p> <pre><code># Create podcast layout\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Advanced customization with detailed parameters\nlayout.set_transition_effect(\n    \"slide\", \n    frames=25,                  # Longer transition (default: 15 frames)\n    direction=\"left\"            # Slide in from left\n)\n\nlayout.set_highlight_effect(\n    \"pulse\",\n    color=(255, 215, 0, 100),   # Custom gold color, semi-transparent\n    min_size=0.9,               # Subtle pulse (90% to 110% size)\n    max_size=1.1,\n    blur_radius=8               # More blur for softer effect\n)\n\n# Add speakers and generate video as before\n</code></pre> <p>At this level, users have complete control over every aspect of the effects.</p>"},{"location":"examples/podcast_03/#full-implementation-example","title":"Full Implementation Example","text":"<p>Here's how you might use Audim's <code>sub2pod</code> submodule in a complete project:</p> <pre><code>from datetime import datetime\nfrom audim.sub2pod.layouts.podcast import PodcastLayout\nfrom audim.sub2pod.core import VideoGenerator\n\n# Create a podcast layout\nprint(\"Creating layout...\")\nlayout = PodcastLayout(\n    video_width=1920,\n    video_height=1080,\n    show_speaker_names=True\n)\n\n# Set custom effects for the layout\nlayout.set_transition_effect(\"fade\", frames=20)\nlayout.set_highlight_effect(\"none\")\n\n# Add speakers\nprint(\"Adding speakers...\")\nlayout.add_speaker(\"Grant Sanderson\", \"input/grant.png\")\nlayout.add_speaker(\"Sal Khan\", \"input/sal.png\")\n\n# Generate video\nprint(\"Generating video...\")\ngenerator = VideoGenerator(layout, fps=30)\ngenerator.generate_from_srt(\n    srt_path=\"input/podcast.srt\",\n    audio_path=\"input/podcast.mp3\",\n    logo_path=\"input/logo.png\",\n    title=\"3b1b Podcast: Sal Khan: Beyond Khan Academy\",\n    cpu_core_utilization=\"max\"\n)\n\n# Export the final video\nprint(\"Exporting video...\")\ndatetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\ngenerator.export_video(f\"output/podcast_underline_{datetime}.mp4\")\n</code></pre> <p>Here's how the generated video looks like upon completion of the rendering process:</p>      Your browser does not support the video element."},{"location":"examples/podcast_03/#available-effects","title":"Available Effects","text":""},{"location":"examples/podcast_03/#transition-effects","title":"Transition Effects","text":"<p>Transitions control how each frame fades in or slides into view:</p> Effect Type Description Parameters <code>\"none\"</code> No transition (default) None <code>\"fade\"</code> Smooth fade-in transition <code>frames</code>: Duration of fade <code>\"slide\"</code> Slide-in animation <code>frames</code>: Duration of slide, <code>direction</code>: \"left\", \"right\", \"up\", or \"down\""},{"location":"examples/podcast_03/#highlight-effects","title":"Highlight Effects","text":"<p>Highlights emphasize the active speaker's text:</p> Effect Type Description Parameters <code>\"none\"</code> No highlight None <code>\"pulse\"</code> Pulsing animation <code>color</code>, <code>min_size</code>, <code>max_size</code>, <code>blur_radius</code> <code>\"glow\"</code> Glowing background <code>color</code>, <code>blur_radius</code> <code>\"underline\"</code> Simple underline <code>color</code>, <code>thickness</code> <code>\"box\"</code> Box around text <code>color</code>, <code>padding</code>, <code>thickness</code>"},{"location":"examples/podcast_03/#benefits-of-progressive-disclosure","title":"Benefits of Progressive Disclosure","text":"<p>This design pattern offers several advantages:</p> <ol> <li>Reduced Learning Curve: New users can be productive immediately without being overwhelmed</li> <li>Smooth Progression: Users can gradually discover more advanced features as they need them</li> <li>Documentation Organization: Documentation can target different user groups based on expertise</li> <li>API Cleanliness: The API remains clean and intuitive at all levels</li> <li>Flexibility: Advanced users can access powerful features without sacrificing simplicity for beginners</li> </ol>"},{"location":"examples/podcast_03/#real-world-impact","title":"Real-World Impact","text":"<p>The progressive disclosure pattern in Audim helps different types of users:</p> <ul> <li>Content Creators: Can quickly generate basic podcast videos without technical knowledge</li> <li>Video Editors: Can customize effects to match their brand and style</li> <li>Developers: Can achieve precise control and integrate Audim into larger workflows</li> </ul>"},{"location":"examples/podcast_03/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with effects:</p> <ul> <li>Verify you're using Audim v0.0.3 or later</li> <li>Check that effect names are spelled correctly (e.g., \"fade\" not \"fading\")</li> <li>For slide transitions with text, ensure your text color includes an opacity value</li> <li>When using highlight effects, ensure the subtitle area is properly defined</li> </ul>"},{"location":"examples/podcast_03/#see-also","title":"See Also","text":"<ul> <li>Basic Podcast Example</li> <li>Professional Podcast Example</li> <li>API Documentation for Transitions</li> <li>API Documentation for Highlights</li> </ul>"},{"location":"examples/podcast_04/","title":"Automated Podcast Subtitling: From Audio to SRT","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: Apr 22, 2025</li> <li>Last Updated: Apr 22, 2025</li> <li>Compatible with: Audim v0.0.5</li> </ul> <p>This guide demonstrates how to use Audim's <code>aud2sub</code> module to automatically generate high-quality subtitles from podcast audio files. We'll also explore the <code>utils</code> module for replacing generic speaker labels and manually validating subtitles. The subtitles generated can finally be fed into <code>sub2pod</code> to generate podcast videos (as covered in previous examples).</p> <p>Together, these tools form a powerful foundation for podcast content creation.</p>"},{"location":"examples/podcast_04/#overview","title":"Overview","text":"<p>The audio-to-subtitle workflow involves these key steps:</p> <ol> <li>Transcription: Converting spoken audio to text</li> <li>Diarization: Identifying who spoke which parts</li> <li>Formatting: Optimizing subtitles for readability</li> <li>Validation: Checking, testing and validating the generated subtitles</li> </ol> <p>Audim handles all these steps with an easy-to-use interface, providing both simple defaults and advanced customization options.</p>"},{"location":"examples/podcast_04/#prerequisites","title":"Prerequisites","text":"<p>Before running this example, ensure you have:</p> <ol> <li>Installed Audim with its dependencies: see installation instructions</li> <li>An audio file to process (MP3, WAV, or other supported formats) with your podcast audio recording</li> <li>A HuggingFace token for speaker diarization (get one at huggingface.co/settings/tokens)</li> </ol> <p>Note: See HuggingFace Token Management for more details.</p>"},{"location":"examples/podcast_04/#basic-usage-high-level-api","title":"Basic Usage (High-level API)","text":"<p>The simplest way to generate subtitles from an audio file is:</p> <pre><code>from audim.aud2sub.transcribers.podcast import PodcastTranscriber\nfrom audim.aud2sub.core import SubtitleGenerator\n\n# Create a transcriber with default settings\ntranscriber = PodcastTranscriber(model_name=\"large-v2\")\n\n# Generate and export subtitles\ngenerator = SubtitleGenerator(transcriber)\ngenerator.generate_from_mp3(\"input/podcast.mp3\")\ngenerator.export_subtitle(\"output/podcast.srt\")\n</code></pre> <p>This basic approach works well for most of the cases, using these default settings: - WhisperX large-v2 model for transcription - Automatic language detection - 1-5 speakers for diarization - Speaker names displayed as \"[Speaker 1]\", \"[Speaker 2]\", etc. - Optimized line length for readability</p>"},{"location":"examples/podcast_04/#customizing-transcription-mid-level-api","title":"Customizing Transcription (Mid-level API)","text":"<p>For more custom use cases, you can configure the transcriber directly during initialization. You can also use the convenient setters for customizing various aspects of the transcription process:</p> <pre><code># Create a transcriber\ntranscriber = PodcastTranscriber(\n    hf_token=\"YOUR_HUGGINGFACE_TOKEN\",\n    model_name=\"large-v3\",              # Use newer/larger model for better accuracy\n    language=\"en\",                      # Force English language detection  \n    device=\"cuda\",                      # Use GPU for faster processing\n    compute_type=\"float16\",             # Use half-precision floating point for faster processing\n    batch_size=16,                      # Process audio in batches of 16 samples\n)\n\n# Customize speaker detection\ntranscriber.set_speakers(min_speakers=2, max_speakers=6)\n\n# Customize subtitle formatting\ntranscriber.set_speaker_names_display(True, pattern=\"[{speaker}]\")\ntranscriber.set_line_properties(max_length=70, min_split_length=50)\n\n# Enable GPU memory management for large files\ntranscriber.set_memory_management(clear_gpu_memory=True)\n</code></pre>"},{"location":"examples/podcast_04/#advanced-configuration-low-level-api","title":"Advanced Configuration (Low-level API)","text":"<p>For more advanced use cases, you can override the base transcriber class and implement your own transcriber:</p> <pre><code>from audim.aud2sub.transcribers.base import BaseTranscriber\nfrom audim.aud2sub.core import SubtitleGenerator\n\nclass MyTranscriber(BaseTranscriber):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Add any custom initialization logic here\n\n    def process_audio(self, audio_path: str) -&gt; None:\n        # Add any custom transcription pipeline and logic here\n\n    def export_subtitle(self, output_path: str) -&gt; None:\n        # Add any custom subtitle export format and logic here\n\ntranscriber = MyTranscriber()\ngenerator = SubtitleGenerator(transcriber)\ngenerator.generate_from_mp3(\"input/podcast.mp3\")\ngenerator.export_subtitle(\"output/podcast.srt\")\n</code></pre>"},{"location":"examples/podcast_04/#complete-subtitling-example","title":"Complete Subtitling Example","text":"<p>Here's a complete example demonstrating the entire process:</p> <pre><code>from audim.aud2sub.transcribers.podcast import PodcastTranscriber\nfrom audim.aud2sub.core import SubtitleGenerator\n\n# Create transcriber\nprint(\"Creating transcriber...\")\ntranscriber = PodcastTranscriber(\n    model_name=\"large-v2\",\n    hf_token=\"YOUR_HUGGINGFACE_TOKEN\"\n)\n\n# Set speaker detection and subtitle formatting parameters\ntranscriber.set_speakers(min_speakers=1, max_speakers=5)\ntranscriber.set_speaker_names_display(True, pattern=\"[{speaker}]\")\ntranscriber.set_line_properties(max_length=70, min_split_length=50)\n\n# Generate subtitles\nprint(\"Generating subtitles...\")\ngenerator = SubtitleGenerator(transcriber)\n\n# Process audio file\nprint(\"Processing audio...\")\ngenerator.generate_from_mp3(\"input/podcast.mp3\")\n\n# Export the final subtitle\nprint(\"Exporting subtitles...\")\ngenerator.export_subtitle(\"output/podcast.srt\")\nprint(\"Done! Check output/podcast.srt for results.\")\n</code></pre>"},{"location":"examples/podcast_04/#validating-and-enhancing-subtitles","title":"Validating and Enhancing Subtitles","text":"<p>After generating subtitles, Audim's <code>utils</code> module provides tools for validating and enhancing them:</p>"},{"location":"examples/podcast_04/#replacing-generic-speaker-labels","title":"Replacing Generic Speaker Labels","text":"<p>Generated subtitles use generic labels like \"[Speaker 1]\" and \"[Speaker 2]\". You can replace these with actual names:</p> <pre><code>from audim.utils.subtitle import Subtitle\n\nsubtitle = Subtitle()\n\n# Preview replacements before applying\nsubtitle.preview_replacement(\n    \"output/podcast.srt\",\n    speakers=[\"Host\", \"Guest\"],\n    pretty_print=True\n)\n\n# Apply replacements\nsubtitle.replace_speakers(\n    \"output/podcast.srt\",\n    speakers=[\"Host\", \"Guest\"],\n    in_place=True\n)\n</code></pre> <p>You can also use a dictionary for more complex mappings:</p> <pre><code>subtitle.replace_speakers(\n    \"output/podcast.srt\",\n    speakers={\n        1: \"Host\",              # Replace [Speaker 1]\n        3: \"Guest 1\",           # Replace [Speaker 3]\n        \"Speaker 2\": \"Guest 2\"  # Replace [Speaker 2]\n    },\n    in_place=True\n)\n</code></pre>"},{"location":"examples/podcast_04/#playing-audio-with-synchronized-subtitles","title":"Playing Audio with Synchronized Subtitles","text":"<p>The <code>playback</code> utility helps validate your subtitles by playing the audio with synchronized subtitles in the terminal:</p> <pre><code>from audim.utils.playback import Playback\n\n# Create a playback instance\nplayback = Playback()\n\n# Play audio with synchronized subtitles\nplayback.play_audio_with_srt(\"input/podcast.mp3\", \"output/podcast.srt\")\n</code></pre> <p>This is a script for checking if subtitles are correctly aligned with the audio and if speaker identification is accurate.</p>"},{"location":"examples/podcast_04/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>This complete example demonstrates the entire workflow from audio to validated subtitles:</p> <pre><code>from audim.aud2sub.transcribers.podcast import PodcastTranscriber\nfrom audim.aud2sub.core import SubtitleGenerator\nfrom audim.utils.subtitle import Subtitle\nfrom audim.utils.playback import Playback\n\n# 1. Generate subtitles\ntranscriber = PodcastTranscriber(hf_token=\"YOUR_HUGGINGFACE_TOKEN\")\ngenerator = SubtitleGenerator(transcriber)\ngenerator.generate_from_mp3(\"input/podcast.mp3\")\ngenerator.export_subtitle(\"output/podcast.srt\")\n\n# 2. Replace generic speaker labels with actual names\nsubtitle = Subtitle()\nsubtitle.replace_speakers(\n    \"output/podcast.srt\",\n    speakers=[\"Grant Sanderson\", \"Sal Khan\"],\n    in_place=True\n)\n\n# 3. Play audio with synchronized subtitles for validation\nplayback = Playback()\nplayback.play_audio_with_srt(\"input/podcast.mp3\", \"output/podcast.srt\")\n</code></pre>"},{"location":"examples/podcast_04/#advanced-memory-management","title":"Advanced Memory Management","text":"<p>When transcribing long audio files or using large models, you may encounter GPU memory limitations. Audim includes memory management features to help:</p> <pre><code>transcriber = PodcastTranscriber(\n    model_name=\"large-v3\",\n    hf_token=\"YOUR_HUGGINGFACE_TOKEN\",\n    clear_gpu_memory=True  # Enable automatic GPU memory cleanup\n)\n</code></pre> <p>With memory management enabled, Audim will: - Clear GPU memory after transcription - Clear GPU memory after alignment - Clear GPU memory after diarization - Perform a final cleanup at the end</p>"},{"location":"examples/podcast_04/#huggingface-token-management","title":"HuggingFace Token Management","text":"<p>Audim needs a HuggingFace token to access the diarization model. You can get one from huggingface.co/settings/tokens.</p> <p>Audim supports multiple ways to provide your HuggingFace token:</p> <ol> <li> <p>Directly passing the token as a parameter when creating a transcriber:</p> <pre><code>from audim.aud2sub.transcribers.podcast import PodcastTranscriber\n\ntranscriber = PodcastTranscriber(hf_token=\"your_huggingface_token\")\n</code></pre> </li> <li> <p>(Recommended) Set the <code>HF_TOKEN</code> environment variable before running your script:</p> <pre><code>export HF_TOKEN=\"your_huggingface_token\"\n</code></pre> </li> <li> <p>Login once using the HuggingFace CLI tool (you need to have <code>huggingface_hub</code> installed):</p> <p>Install:</p> <pre><code>&gt;&gt;&gt; pip install -U \"huggingface_hub[cli]\"\n</code></pre> <p>Login:</p> <pre><code>huggingface-cli login\n# Enter your token when prompted\n</code></pre> </li> </ol>"},{"location":"examples/podcast_04/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ul> <li>Memory Errors: Enable <code>clear_gpu_memory=True</code> or use a smaller model</li> <li>Missing Speakers: Check that you provided a valid HuggingFace token </li> <li>Poor Transcription: Try a larger model like \"large-v3\" or specify the language</li> <li>Speaker Confusion: Adjust min_speakers and max_speakers to match your podcast</li> </ul>"},{"location":"examples/podcast_04/#whats-next","title":"What's Next?","text":"<p>After generating and validating subtitles with <code>aud2sub</code> and <code>utils</code>, you can:</p> <ol> <li>Use the subtitles in <code>sub2pod</code> to generate podcast videos (see Podcast Video Generation)</li> <li>Add advanced effects to your videos (see Advanced Video Effects)</li> <li>Build a complete podcast production pipeline with Audim</li> </ol>"},{"location":"examples/podcast_04/#see-also","title":"See Also","text":"<ul> <li>API Documentation for aud2sub</li> <li>API Documentation for playback</li> <li>API Documentation for subtitle</li> <li>Basic Podcast Example</li> <li>Advanced Video Effects</li> </ul>"},{"location":"examples/podcast_05/","title":"Audio Extraction Utility: From Video to Audio","text":"<ul> <li>Author: @mratanusarkar</li> <li>Created: May 15, 2025</li> <li>Last Updated: May 15, 2025</li> <li>Compatible with: Audim v0.0.6</li> </ul> <p>This guide introduces Audim's new <code>Extract</code> utility for seamlessly extracting high-quality audio from video files. This simple yet powerful feature streamlines the podcast production workflow by eliminating the need for external tools, and can be used in various other use cases and scenarios in the podcast production workflow.</p>"},{"location":"examples/podcast_05/#overview","title":"Overview","text":"<p>The <code>Extract</code> utility provides a clean interface for:</p> <ol> <li>Video-to-Audio Conversion: Extract audio from podcast videos</li> <li>Format Flexibility: Support for WAV, MP3, FLAC, and other formats</li> <li>Quality Control: Configurable bitrate and sample rate</li> </ol>"},{"location":"examples/podcast_05/#basic-usage","title":"Basic Usage","text":"<p>Extracting audio from a video file is straightforward:</p> <pre><code>from audim.utils.extract import Extract\n\n# Set input and output file paths\ninput_file = \"./input/podcast.mp4\"\noutput_file = \"./output/podcast.wav\"\noutput_format = \"wav\"\n\n# Extract audio from video\nextractor = Extract()\nextractor.extract_audio(input_file, output_file, output_format)\n</code></pre>"},{"location":"examples/podcast_05/#advanced-options","title":"Advanced Options","text":"<p>For more control over the extraction process:</p> <pre><code>from audim.utils.extract import Extract\n\nextractor = Extract()\nextractor.extract_audio(\n    input_path=\"./input/podcast.mp4\",\n    output_path=\"./output/podcast.mp3\",\n    output_format=\"mp3\",\n    bitrate=\"320k\",\n    sample_rate=48000\n)\n</code></pre>"},{"location":"examples/podcast_05/#integration-with-audim-workflow","title":"Integration with Audim Workflow","text":"<p>This utility complements the existing Audim modules:</p> <ol> <li>Extract audio from your recorded video using <code>Extract.extract_audio()</code></li> <li>Generate subtitles with <code>aud2sub</code> from the extracted audio (see Podcast Subtitling)</li> <li>Produce podcasts with <code>sub2pod</code> from the extracted audio and generated subtitles (see Podcast Videos)</li> </ol>"},{"location":"examples/podcast_05/#see-also","title":"See Also","text":"<ul> <li>API Documentation for Extract</li> <li>Automated Podcast Subtitling</li> <li>Basic Podcast Example</li> </ul>"},{"location":"setup/development/","title":"Development","text":""},{"location":"setup/development/#code-quality","title":"Code Quality","text":"<p>Before committing, please ensure that the code is formatted and styled correctly. Run the following commands to check and fix code style issues:</p> <pre><code># Check and fix code style issues\nruff format .\nruff check --fix .\n</code></pre>"},{"location":"setup/development/#run-the-project","title":"Run the project","text":"<p>feel free to create a <code>run.py</code> or <code>test.py</code> file to test the project. They will be untracked by git.</p> <p>implement your usage logic in the <code>run.py</code> file.</p> <p>Run with:</p> <pre><code>python run.py\n</code></pre>"},{"location":"setup/development/#build-and-serve-the-documentation","title":"Build and serve the documentation","text":"<p>You can build and serve the documentation by running:</p> <pre><code>uv pip install -e .[docs]\nmkdocs serve\n</code></pre>"},{"location":"setup/installation/","title":"Installation","text":""},{"location":"setup/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python \u2265 3.10</li> <li>Conda</li> </ul>"},{"location":"setup/installation/#setup","title":"Setup","text":""},{"location":"setup/installation/#1-clone-the-repository","title":"1. Clone the repository:","text":"<pre><code>git clone https://github.com/mratanusarkar/audim.git\ncd audim\n</code></pre>"},{"location":"setup/installation/#2-install-ffmpeg-locally-optional","title":"2. Install FFmpeg locally (optional)","text":"<p>Using local FFmpeg is optional. It is recommended to install FFmpeg locally as it will speed up the video encoding process.</p> <p>On Ubuntu, install FFmpeg using:</p> <pre><code>sudo apt install ffmpeg libx264-dev\n</code></pre> <p>On Windows and other platforms, download and install FFmpeg from the official website:</p> <ul> <li>Download FFmpeg</li> <li>Make sure FFmpeg is in your system PATH</li> </ul>"},{"location":"setup/installation/#3-install-uv-and-setup-project-environment","title":"3. Install <code>uv</code> and setup project environment:","text":"<p>Note: If you are using conda base environment as the default base environment for your python projects, run the below command to activate the base environment. If not, skip this step and continue with the next step.</p> <pre><code>conda activate base\n</code></pre> <pre><code># Install uv\npip install uv\n\n# Setup project environment\nuv venv\n\nsource .venv/bin/activate   # on Linux\n# .venv\\Scripts\\activate    # on Windows\n\nuv pip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"setup/installation/#4-create-input-and-output-directories","title":"4. Create input and output directories:","text":"<pre><code>mkdir ./input ./output\n</code></pre> <p>ideally, if done correctly, the setup should be like this:</p> <pre><code>audim/\n\u251c\u2500\u2500 audim/\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 input/\n\u251c\u2500\u2500 output/\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Note: you would dump your input files in the <code>input</code> directory and the output files will be dumped in the <code>output</code> directory.</p>"}]}